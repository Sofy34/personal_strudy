{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## thesis specific import ##\n",
    "import sys\n",
    "import imp\n",
    "import seaborn as sns\n",
    "sys.path.append('./src/')\n",
    "import doc_utils\n",
    "import feature_utils\n",
    "import defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'doc_utils' from './src/doc_utils.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(doc_utils)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_par_to_blocks(par):\n",
    "    block_list = re.split('(&|#)', par)\n",
    "    return block_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " : סתם מילים \n",
      "1\n",
      " : &\n",
      "2\n",
      " :  התחלה סיפור ראשון \n",
      "3\n",
      " : #\n",
      "4\n",
      " :  מילים של מטפל \n",
      "5\n",
      " : &\n",
      "6\n",
      " :  התחלה סיפור חדש\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\n",
    "    \"beginning of the paragraph & this is story one # and so paragraph continues & this is story two # and this is not story\",\n",
    "                \"beginning of the paragraph &  story one started\",\n",
    "                \"story one ended # something after the story\",\n",
    "                \"this is end of story one # something after the story & start of new story\",\n",
    "                \"just prefrase & entire story one # something after the story & start of second story\",\n",
    "    \"לא סיפור & סיפור ראשון# אחרי סיפור ראשון\",\n",
    "    \"סוף סיפור ראשון # סתם מילים & התחלה סיפור שני\",\n",
    "    \"סתם מילים & התחלה סיפור ראשון # מילים של מטפל & התחלה סיפור חדש\",\n",
    "        \"סןף סיפור הקודם#  & התחלה סיפור ראשון # מילים של מטפל & התחלה סיפור חדש\",\n",
    "    \"פה בכלל אין סיפור\",\n",
    "    \"&פה יש רק סיפור וזהו#\"\n",
    "               ]\n",
    "for i,string in enumerate(test_strings):\n",
    "    my_list = split_par_to_blocks(string)\n",
    "    if i == 7:\n",
    "        for j, block in enumerate(my_list):\n",
    "            print(\"{}\\n : {}\".format(j,block))\n",
    "#     print(\"{} story:\\n{}\".format(i,my_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(doc_utils)  \n",
    "doc_utils.save_docs_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>file_name</th>\n",
       "      <th>client_tag</th>\n",
       "      <th>therapist_tag</th>\n",
       "      <th>num_par</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./tmp/kafhey_23_l.docx</td>\n",
       "      <td>kafhey_23_l.docx</td>\n",
       "      <td>כהקל</td>\n",
       "      <td>כהמט</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./tmp/lamedbet21_l.docx</td>\n",
       "      <td>lamedbet21_l.docx</td>\n",
       "      <td>לבקל</td>\n",
       "      <td>לבמט</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./tmp/kafdalet_15_l.docx</td>\n",
       "      <td>kafdalet_15_l.docx</td>\n",
       "      <td>כדקל</td>\n",
       "      <td>כדמט</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./tmp/nun2_l.docx</td>\n",
       "      <td>nun2_l.docx</td>\n",
       "      <td>נקל</td>\n",
       "      <td>נמט</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./tmp/aingimel3_l.docx</td>\n",
       "      <td>aingimel3_l.docx</td>\n",
       "      <td>עגקל</td>\n",
       "      <td>עגמט</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./tmp/alefsameh3_l.docx</td>\n",
       "      <td>alefsameh3_l.docx</td>\n",
       "      <td>סאקל</td>\n",
       "      <td>סאמט</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       path           file_name client_tag therapist_tag  \\\n",
       "0    ./tmp/kafhey_23_l.docx    kafhey_23_l.docx       כהקל          כהמט   \n",
       "1   ./tmp/lamedbet21_l.docx   lamedbet21_l.docx       לבקל          לבמט   \n",
       "2  ./tmp/kafdalet_15_l.docx  kafdalet_15_l.docx       כדקל          כדמט   \n",
       "3         ./tmp/nun2_l.docx         nun2_l.docx        נקל           נמט   \n",
       "4    ./tmp/aingimel3_l.docx    aingimel3_l.docx       עגקל          עגמט   \n",
       "5   ./tmp/alefsameh3_l.docx   alefsameh3_l.docx       סאקל          סאמט   \n",
       "\n",
       "  num_par  \n",
       "0     111  \n",
       "1     451  \n",
       "2     106  \n",
       "3     164  \n",
       "4     229  \n",
       "5     276  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_utils.doc_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0 paragraphs saved\n",
      "Doc 1 paragraphs saved\n",
      "Doc 2 paragraphs saved\n",
      "Doc 3 paragraphs saved\n",
      "Doc 4 paragraphs saved\n",
      "Doc 5 paragraphs saved\n"
     ]
    }
   ],
   "source": [
    "doc_utils.save_all_docs_paragraphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_utils.plane_par_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_utils.plane_par_db.query(\"nar_per_par >=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_utils.plane_par_db.loc[9,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "אני קצתהצטננתי אני די מצוננת אבל חוץ מזה בסדר אמא שלי התחילה להעלות הילוך בקטע דיבורים על הנסיעה XXX כי היינו צריכים להחליט בין כמה רגע עוד לא החלטתי איזה זה איזה זה כבר הזמנת כרטיסהיא עוד לא הזמינה אבל התחילה וואו XXX כי היא צריכה להתארגן  מהמה לעשות עם הבעל שלה צוחקת אבל בסדר XXX תהיה איתו רוב הזמן בבית כן אצלה בבית שנסענו לפני שנה וחצי לפורטוגל אז היא באה הייתה איתו סוף שבוע אז הוא לא היה במצב שהוא עכשיו וכאילו הסתדרו מצוין ביחד אז צוחקת באופן מפתיע כן אז היא תהיה איתו לא יודעת מה היא תעשה שם כל הזמן אבל סך הכל היא פנסיונרית \n",
      " not in \n",
      "['', '', 'אני קצתהצטננתי אני די מצוננת אבל חוץ מזה בסדר', '', ' אמא שלי התחילה להעלות הילוך בקטע דיבורים על הנסיעה XXX כי היינו צריכים להחליט בין כמה רגע עוד לא החלטתי איזה זה איזה זה כבר הזמנת כרטיסהיא עוד לא הזמינה אבל התחילה וואו XXX כי היא צריכה להתארגן  מהמה לעשות עם הבעל שלה צוחקת אבל בסדר XXX תהיה איתו רוב הזמן בבית כן אצלה בבית שנסענו לפני שנה וחצי לפורטוגל אז היא באה הייתה איתו סוף שבוע אז הוא לא היה במצב שהוא עכשיו וכאילו הסתדרו מצוין ביחד אז צוחקת באופן מפתיע כן אז היא תהיה איתו לא יודעת מה היא תעשה שם כל הזמן אבל סך הכל היא פנסיונרית', '', 'לעשות הליכות מסביב לקיבוץ היא יכולה גם בקיבוץ שלנו כן יסתדרו כנראה עוד לא סגרנו בדיוק מתי XXX הילוך ']\n",
      "כן אבל ההחמצה הכי גדולה זה זה שלא עשיתי כלום XXX השחזור זה בכלל אל באותו level זה כולה כסף זה בכלל לא בסדר אני חושבת  XXX רואה את הניירות האלה וזה אני זוכרת אז את התחושה הזאת שהעובדת הסוציאלית באה הנה זה וזה וכאילו אני ככה צוחקת אז אחד מהניירות שם הייתי צריכה שהרופאה תמלא אז באותו יום הייתי אצל הרופאה היא מילאה אבל הייתי צריכה משהו בהנהלת חשבונות של העבודה שלי וזה טבלה כזאת נוראית כאילו אני אמרתי איך אני בכלל מביאה את זה חשבתי איך אני ממלאה איך היא ממלאה זה כזה הפחיד אותי גם בכלל איך להתעסק עם זה ובטח שבאותו רגע לא יכולתי אבל פשוט זה הציף לי את כל הזה ועוד כל מיני אנשים ש היחס שקיבלתי ש גם זה קשה וזהו ויש את הקלסר עכשיו ברור שבקלסר הזה אני מניחה יש דברים שלא צריכים להיות שם שבכלל צריכים לעוף אבל זה סיפור אחר לא יכולתי להתמודד עם זה XXX אולי בשלב מאוחר יש עוד קלסרים שצריכים להסתדר אבל לדעתי הם הרבה יותר טכניים אז יהיה יותר פשוט לסדר איתם ומבחינת לסדר יש ארגזים אני לא יודעת אם כל הארגזים יעופו אבל לפחות שכל ארגז אני אדע מה באמת מה יש בו התחלתי לעשות את זה כשעברתי לדירה ואיכשהו באיזשהו שלב זה נתקע \n",
      " not in \n",
      "['כן אבל ההחמצה הכי גדולה זה זה שלא עשיתי כלום XXX השחזור זה בכלל אל באותו level זה כולה כסף זה בכלל לא בסדר אני חושבת  XXX רואה את הניירות האלה וזה אני זוכרת אז את התחושה הזאת שהעובדת הסוציאלית באה הנה זה וזה וכאילו אני ככה צוחקת אז אחד מהניירות שם הייתי צריכה שהרופאה תמלא אז באותו יום הייתי אצל הרופאה היא מילאה אבל הייתי צריכה משהו בהנהלת חשבונות של העבודה שלי וזה טבלה כזאת נוראית כאילו אני אמרתי איך אני בכלל מביאה את זה חשבתי איך אני ממלאה איך היא ממלאה זה כזה הפחיד אותי גם בכלל איך להתעסק עם זה ובטח שבאותו רגע לא יכולתי אבל פשוט זה הציף לי את כל הזה ועוד כל מיני אנשים ש היחס שקיבלתי ש גם זה קשה וזהו', '', ' ויש את הקלסר עכשיו ברור שבקלסר הזה אני מניחה יש דברים שלא צריכים להיות שם שבכלל צריכים לעוף אבל זה סיפור אחר לא יכולתי להתמודד עם זה XXX אולי בשלב מאוחר יש עוד קלסרים שצריכים להסתדר אבל לדעתי הם הרבה יותר טכניים אז יהיה יותר פשוט לסדר איתם ומבחינת לסדר יש ארגזים אני לא יודעת אם כל הארגזים יעופו אבל לפחות שכל ארגז אני אדע מה באמת מה יש בו התחלתי לעשות את זה כשעברתי לדירה ואיכשהו באיזשהו שלב זה נתקע', '', ' תקופה מעצבת בחייים  לא יודעת אם סיפרתי לך אבל ממש', '', '', '', ' ']\n",
      " היו לי הרבה מחשבות על העבר ו כאילו כל מני דברים דברים שקשה לי להתמודד איתם כאילו רגשות שלא רי אני עובדת מגיל צעיר אני עובדת מגיל 12 רצוף כאילו באמת טסתי לתאילנד חודש ושבועיים הייתי בבית אחרי זה חיפשתי עבודה ונכנסתי כבר לבנק והתחלתי ללמוד הסדר יום שלי היה עמוס במטלות ו כאילו עבדתי משרה מלאה ושילבתי לימודים כאילו אף פעם לא היה לי רגע דל ופתאום החצי שנה הזאת בבית חטפתי כאפה רצינית \n",
      " not in \n",
      "[' אני מרגישה שאני צריכה להתאושש כאילו מהתקופה הזאת שהייתי בבית שהשפיעה עלי מאוד כאילו גם תוך כדי הלמידה וה כאילו שהייה הזאת בבית ברור X 342  צוחקת אה', '', ' היו לי הרבה מחשבות על העבר ו כאילו כל מני דברים דברים שקשה לי להתמודד איתם כאילו רגשות שלא ', '', 'רי אני עובדת מגיל צעיר אני עובדת מגיל 12 רצוף כאילו באמת טסתי לתאילנד חודש ושבועיים הייתי בבית אחרי זה חיפשתי עבודה ונכנסתי כבר לבנק והתחלתי ללמוד הסדר יום שלי היה עמוס במטלות ו כאילו עבדתי משרה מלאה ושילבתי לימודים כאילו אף פעם לא היה לי רגע דל ופתאום החצי שנה הזאת בבית חטפתי כאפה רצינית', '', ' ']\n",
      "All blocks saved\n"
     ]
    }
   ],
   "source": [
    "doc_utils.save_all_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_utils.block_db.to_csv(\"block_db.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_utils.block_db.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_utils.block_db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_utils.block_db.query(\"doc_idx == 5 and is_nar == 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_colwidth\", -1)\n",
    "doc_utils.block_db.query(\"par_idx >=11 and par_idx <=20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='block_type',data=doc_utils.block_db).set_title(\"Count per block type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_utils.block_db.groupby(['doc_idx'])['nar_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='nar_per_par',data=doc_utils.plane_par_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_utils.plane_par_db.query(\"is_nar==1 and doc_idx == 0 and nar_idx == 7\")#['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(doc_utils)  \n",
    "sent_db=  pd.DataFrame()\n",
    "par_db_columns = ['doc_idx','text','par_type','is_nar','nar_idx','glob_nar_idx','nar_len','nar_len_words']\n",
    "par_db = pd.DataFrame(columns=par_db_columns)\n",
    "par_db.iloc[:,1:] = par_db.iloc[:,1:].astype('int32')\n",
    "\n",
    "for doc_idx in doc_db.index:\n",
    "    doc_utils.add_paragraphs_to_db(doc_idx,doc_db,par_db,sent_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_db.query(\"is_nar ==1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_db.info()\n",
    "sns.countplot(x='is_nar',data=sent_db)\n",
    "sns.displot(data=sent_db, x=\"sent_len\", hue=\"is_nar\", multiple=\"stack\", kind=\"kde\",palette=\"light:m_r\")\n",
    "sns.displot(data=sent_db, x=\"sent_len\", hue=\"par_type\", multiple=\"stack\", kind=\"kde\",palette=\"light:m_r\")\n",
    "sns.catplot(data=sent_db, x=\"par_type\", y=\"sent_len\", kind=\"box\",palette=\"light:m_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_db.to_csv(\"sent_db.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par_db= doc_utils.add_length_of_paragraphs(par_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "par_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db.loc[par_db['nar_idx']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db.loc[par_db['par_type']=='no_mark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db.loc[par_db['is_nar']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='par_type',data=par_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='is_nar',data=par_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(x='nar_len',data=par_db, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='nar_len_words',data=par_db, kde=True,bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='par_len',data=par_db, kde=True,bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db.groupby(by='is_nar').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(x='par_len',data=par_db,hue='is_nar')\n",
    "sns.displot(data=par_db, x=\"par_len\", hue=\"is_nar\", multiple=\"stack\", kind=\"kde\",palette=\"light:m_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=par_db, x=\"is_nar\", y=\"par_len\", kind=\"box\",palette=\"light:m_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=par_db, x=\"par_len\", y=\"nar_len\", hue=\"is_nar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding features - previous 1,2 paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db['one_before_is_nar']=par_db['is_nar'].shift(periods=1, fill_value=0)\n",
    "par_db['two_before_is_nar']=par_db['is_nar'].shift(periods=2, fill_value=0)\n",
    "par_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db[par_db['nar_idx']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save par_db to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db.to_csv(\"par_db.csv\",index=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_utils.run_model(par_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize hebrew - past time and special words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(doc_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db['text'] = par_db['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_nar_par = doc_utils.get_random_par(par_db,1)\n",
    "random_nar_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_str = random_nar_par['text'].tolist()[0]\n",
    "random_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hebrew_tokenizer as ht\n",
    "tokens = ht.tokenize(random_str)  # tokenize returns a generator!\n",
    "for grp, token, token_num, (start_index, end_index) in tokens:\n",
    "    print('{}, {}'.format(grp, token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give a try to AlphaBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alephbert_tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\n",
    "alephbert = BertModel.from_pretrained('onlplab/alephbert-base')\n",
    "\n",
    "# if not finetuning - disable dropout\n",
    "alephbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = alephbert_tokenizer.tokenize(random_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = alephbert_tokenizer.encode(random_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = sent_db['text'].apply((lambda x: alephbert_tokenizer.encode(x, add_special_tokens=True,padding='max_length', truncation=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = alephbert_tokenizer(sent_db.loc[0,'text'], return_tensors=\"pt\")\n",
    "output = alephbert(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try hebrew word2vec - hebrew_cc_300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "# from sparknlp.embeddings import *\n",
    "import sparknlp\n",
    "sys.path.append('./external_src/hebrew_cc_300d_he/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session with Spark NLP\n",
    "spark = sparknlp.start()\n",
    "embeddings = sparknlp.annotator.WordEmbeddingsModel.pretrained(\"hebrew_cc_300d\", \"he\") \\\n",
    "        .setInputCols([\"document\", \"token\"]) \\\n",
    "        .setOutputCol(\"embeddings\")\n",
    "nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings])\n",
    "pipeline_model = nlp_pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
    "result = pipeline_model.transform(spark.createDataFrame([['כמו גם התקפות והאשמות נגד בראון']], [\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try hebrew word2vec - words_vectors.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine numeric and non-numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(feature_utils)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what features to take from text? doc2vec?\n",
    "# train gensim doc2vec on my own?\n",
    "# is there a pre-trained hebrew?\n",
    "# take word2vec?\n",
    "# what does tagged doc means? is it tag 0/1 ? (not narrative/narrative)?\n",
    "# look at Jonathan's pdf lectures (file 10?)\n",
    "# missing line in some doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data to train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_ter_db = par_db.query(\"par_type == 'client' or par_type =='therapist'\")\n",
    "client_ter_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = client_ter_db['text']\n",
    "train_y = client_ter_db['is_nar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_text, train_y, stratify=y_trainval, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = y_train.index\n",
    "test_idx = y_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenSim - doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def read_corpus(text, y = [], tokens_only=False):\n",
    "    for i, line in enumerate(text):\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(\n",
    "                gensim.utils.simple_preprocess(line),  tags=[y[i]]) #[i])\n",
    "\n",
    "# train_corpus = list(read_corpus(X_train.tolist(),y_train.tolist()))\n",
    "# test_corpus = list(read_corpus(X_test.tolist(),_, tokens_only=True))\n",
    "all_corpus =  list(read_corpus(client_ter_db['text'].tolist(),client_ter_db['is_nar'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.doc2vec.Doc2Vec(vector_size=vector_size, min_count=2, epochs=40)\n",
    "# model.build_vocab(train_corpus)\n",
    "# model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_on_all = gensim.models.doc2vec.Doc2Vec(\n",
    "    vector_size=vector_size,\n",
    "    min_count=2,\n",
    "    epochs=40)\n",
    "model_on_all.build_vocab(all_corpus)\n",
    "model_on_all.train(all_corpus, total_examples=model_on_all.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding using doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = [model.infer_vector(train_corpus[doc_id].words)\n",
    "#           for doc_id in range(len(train_corpus))]\n",
    "\n",
    "# doc2vec_train = np.vstack(vectors)\n",
    "\n",
    "# test_vectors = [model.infer_vector(test_corpus[doc_id])\n",
    "#                 for doc_id in range(len(test_corpus))]\n",
    "# doc2vec_test = np.vstack(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_on_all = [model_on_all.infer_vector(all_corpus[doc_id].words)\n",
    "          for doc_id in range(len(all_corpus))]\n",
    "doc2vec_all = np.vstack(vectors_on_all)\n",
    "doc2vec_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_on_all.dv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_on_all.dv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "result = 1 - spatial.distance.cosine(model_on_all.dv[0], model_on_all.dv[1])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.DataFrame(data=doc2vec_train,index=train_idx)\n",
    "# test_df = pd.DataFrame(data=doc2vec_test,index=test_idx)\n",
    "# doc2vec_db = pd.concat([train_df,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec_db.sort_index(inplace=True)\n",
    "# doc2vec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_db = pd.DataFrame(data=doc2vec_all,index=client_ter_db.index)\n",
    "merged_db = pd.concat([merged_db,client_ter_db],axis=1)\n",
    "merged_db['par_type'].replace({\"therapist\": 0, \"client\": 1}, inplace=True)\n",
    "merged_db.drop(['nar_idx','nar_len','idx_in_nar','doc_idx','text','glob_nar_idx','nar_len_words'],axis=1,inplace=True)\n",
    "merged_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(doc_utils)\n",
    "doc_utils.run_classifier(merged_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create narrative DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nar_db = par_db.groupby(by='nar_idx')['text'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nar_db = doc_utils.clean_text(nar_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_db['block_idx'] = par_db['is_nar'].diff().ne(0).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nar_db = par_db[par_db['is_nar']==0].groupby(by='block_idx')['text'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(not_nar_db[not_nar_db['block_idx']==21]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nar_db['is_nar'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nar_db['is_nar'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_nar_db = not_nar_db.append(nar_db,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_nar_db.drop(['block_idx','nar_idx'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_nar_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_utils.run_model(mixed_nar_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_utils.show_data_basic_information(doc_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_utils.show_data_basic_information(par_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial processing:\n",
    "- drop 'Unnamed 0' column\n",
    "- encode author_gender\n",
    "- add feature - length of the narrative\n",
    "- (?) - replace birth year with age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_gender(_df):\n",
    "    df = _df.copy()\n",
    "    man = \"זכר\"\n",
    "    woman = \"נקבה\"\n",
    "    gender = {man: 0, woman: 1}\n",
    "    df['author_gender'] = [gender[item] for item in df['author_gender']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_length_of_narrative(_df):\n",
    "    df = _df.copy()\n",
    "    df['nar_length'] = df['narrative'].str.len()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_year_to_age(_df):\n",
    "#     df = _df.copy()\n",
    "#     df['nar_length'] = df['narrative'].str.len()\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_steps['1_dropped'] = drop_columns(df_steps['0_full_df'],['Unnamed: 0'])\n",
    "df_steps['2_gender'] = encode_gender(df_steps['1_dropped'])\n",
    "df_steps['3_nar_length'] = add_length_of_narrative(df_steps['2_gender'])\n",
    "df_steps['3_nar_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot see the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_steps['3_nar_length'], hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='label',data=df_steps['3_nar_length'])\n",
    "plt.title('Class Distributions \\n (0: False || 1: True)', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_text(df_steps['3_nar_length'],'narrative',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclustion from step 1\n",
    "- data is balanced\n",
    "- there is no significant differences between features distribution per label\n",
    "- narrative has to be cleaned from \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_steps['4_clean_nar'] = df_steps['3_nar_length']\n",
    "df_steps['4_clean_nar']['narrative'] = df_steps['4_clean_nar']['narrative'].replace(b\"<br />\", b\" \")\n",
    "df_steps['4_clean_nar']['narrative'] = df_steps['4_clean_nar']['narrative'].replace(\"n\\\\\",' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_text(df_steps['4_clean_nar'],'narrative',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - tokenize the narrative\n",
    "exaustive search for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = df_steps['4_clean_nar']['narrative'].tolist()\n",
    "y_train = df_steps['4_clean_nar']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=4)\n",
    "X_train = vect.fit_transform(text_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary\n",
    "feature_names = vect.get_feature_names()\n",
    "print(feature_names[:10])\n",
    "print(feature_names[20000:20020])\n",
    "print(feature_names[::2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(n_jobs=-1).fit(X_train, y_train)\n",
    "lr.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [\n",
    "    LogisticRegression(),\n",
    "    LogisticRegressionCV(),\n",
    "    PassiveAggressiveClassifier(),\n",
    "    Perceptron(),\n",
    "    RidgeClassifier(),\n",
    "    RidgeClassifierCV(),\n",
    "    SGDClassifier()\n",
    "]\n",
    "scores_df = pd.DataFrame(dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for regr in regressors:\n",
    "    get_cross_val_score(scores_df,regr, X_train, y_train,\"count_vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scores_df.sort_values(by='f1').tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer\n",
    "tdif = TfidfVectorizer(min_df=4)\n",
    "X_train = tdif.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for regr in regressors:\n",
    "    get_cross_val_score(scores_df,regr, X_train, y_train,\"TfidfVectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scores_df.sort_values(by='f1').tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 normalization with CountVectorizer\n",
    "count_vec = CountVectorizer(min_df=4)\n",
    "X_train = count_vec.fit_transform(text_train)\n",
    "X_train = normalize(X_train,norm=\"l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for regr in regressors:\n",
    "    get_cross_val_score(scores_df,regr, X_train, y_train,\"TfidfVectorizer_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scores_df.sort_values(by='f1').tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngrams\n",
    "ngrm = CountVectorizer(ngram_range=(1, 3), min_df=4)\n",
    "X_train = ngrm.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for regr in regressors:\n",
    "    get_cross_val_score(scores_df,regr, X_train, y_train,\"ngrm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scores_df.sort_values(by='f1').tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 3), analyzer=\"char_wb\")\n",
    "X_train = cv.fit_transform(text_train)\n",
    "print(f\"vocaulary size:{len(cv.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for regr in regressors:\n",
    "    get_cross_val_score(scores_df,regr, X_train, y_train,\"_char_wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scores_df.sort_values(by='f1').tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an impression of data by looking at most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdif = TfidfVectorizer(stop_words='english',min_df=4)\n",
    "X_train = tdif.fit_transform(text_train)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_features(coef, feature_names, top_n=20, ax=None, rotation=60):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    inds = np.argsort(coef)\n",
    "    low = inds[:top_n]\n",
    "    high = inds[-top_n:]\n",
    "    important = np.hstack([low, high])\n",
    "    myrange = range(len(important))\n",
    "    colors = ['red'] * top_n + ['blue'] * top_n\n",
    "    \n",
    "    ax.bar(myrange, coef[important], color=colors)\n",
    "    ax.set_xticks(myrange)\n",
    "    ax.set_xticklabels(feature_names[important], rotation=rotation, ha=\"right\")\n",
    "    ax.set_xlim(-.7, 2 * top_n)\n",
    "    ax.set_frame_on(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plot_important_features(lr.coef_.ravel(), np.array(tdif.get_feature_names()), top_n=20, rotation=40)\n",
    "ax = plt.gca()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_label_and_drop(df_steps['4_clean_nar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=101,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_text_pipeline(df,regr_num,regr_text):\n",
    "    numeric_cols = X_train.columns[X_train.columns.dtype != object].tolist()\n",
    "    \n",
    "    transformer_text = FunctionTransformer(lambda x: x['narrative'], validate=False)\n",
    "    transfomer_numeric = FunctionTransformer(lambda x: x[numeric_cols], validate=False)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', transfomer_numeric)\n",
    "                ])),\n",
    "                 ('text_features', Pipeline([\n",
    "                    ('selector', transformer_text),\n",
    "                    (regr_text.__class__.__name__, regr_text)\n",
    "                ]))\n",
    "             ])),\n",
    "        ('estimator', regr_num)\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_text = TfidfVectorizer(min_df=4)\n",
    "\n",
    "for regr in regressors:\n",
    "    pipe = get_num_text_pipeline(X_train,regr,regr_text)\n",
    "    get_cross_val_score(scores_df,pipe, X_train, y_train,regr_text.__class__.__name__+\"_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scores_df.sort_values(by='f1').tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meantime combined featues haven't improved the results so far..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # is it relevant for hebrew? TBD check what we saw on a class on hebrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"tagger\", \"parser\", \"ner\",\"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
