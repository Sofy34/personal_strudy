{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1cbf8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensembling\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os, glob\n",
    "import imp\n",
    "import seaborn as sns\n",
    "sys.path.append('./src/')\n",
    "import my_ensembler, feature_utils, defines, model_utils, my_bert\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn_crfsuite import scorers, CRF\n",
    "from sklearn.pipeline import Pipeline\n",
    "import json\n",
    "import common_utils\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from sklearn_crfsuite.utils import flatten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907fb36",
   "metadata": {},
   "source": [
    "### Load docs map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b91af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_par=True\n",
    "seq_len=3\n",
    "step=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a7a977d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"scaled_maxabs\"\n",
    "doc_map_path =  os.path.join(os.getcwd(),defines.PATH_TO_DFS,dir_name,\"nan_max.abs_sacled_docs_map.json\")\n",
    "with open(doc_map_path, 'r') as fp:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "489e48c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.93034056, 0.67478355])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_utils.get_class_weights(flatten(y_test_crf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a7ac76",
   "metadata": {},
   "source": [
    "## Try BERT Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3958d69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>fit() called.\n",
      "\n",
      "\n",
      ">>>>>>>transform() called.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsofya/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# alephbert_tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\n",
    "imp.reload(my_bert)\n",
    "bert_preprocess = my_bert.BertTransformer(tokenizer=alephbert_tokenizer)\n",
    "X_tensor_map = bert_preprocess.fit_transform(X=docs_map,indices=np.arange(1,71))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bcf46e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31956, tensor(8974))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_tensor_map['y']),sum(X_tensor_map['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576075a0",
   "metadata": {},
   "source": [
    "## Try BERT estmimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea06d95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>init() called.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BertClassifier</label><div class=\"sk-toggleable__content\"><pre>BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(my_bert)\n",
    "alephbert_model = BertModel.from_pretrained('onlplab/alephbert-base', return_dict=False)\n",
    "bert_estimator = my_bert.BertClassifier(pretrained_model=alephbert_model)\n",
    "bert_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "afe114d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['seq', 'mask', 'y', 'y_list'])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d8d28937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_tensor_map['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "65a835ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.69523975 1.78047693]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NLLLoss()"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(my_bert)\n",
    "my_bert.get_cross_entropy(np.asarray(X_tensor_map['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d95fff81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(X_tensor_map['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "00b45690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      ">>>>>>> fit() called\n",
      "Class Weights: [0.69523975 1.78047693]\n",
      "CPU times: user 3h 16min 46s, sys: 1h 15min 5s, total: 4h 31min 52s\n",
      "Wall time: 15min 31s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BertClassifier</label><div class=\"sk-toggleable__content\"><pre>BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "imp.reload(my_bert)\n",
    "bert_estimator = my_bert.BertClassifier(pretrained_model=alephbert_model)\n",
    "bert_estimator.fit(X=X_tensor_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "15f44a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BertClassifier</label><div class=\"sk-toggleable__content\"><pre>BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a4fd716e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>fit() called.\n",
      "\n",
      "\n",
      ">>>>>>>transform() called.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_tensor_map_train = bert_preprocess.fit_transform(X=docs_map,indices=np.arange(72,81))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9a55a17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor_map_train['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6cc82a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = ['is_nar' if i==1 else 'not_nar' for i in list(X_tensor_map_train['y'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b7a9977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[46m\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     not_nar       0.80      0.95      0.87      2772\n",
      "      is_nar       0.69      0.30      0.42       969\n",
      "\n",
      "    accuracy                           0.78      3741\n",
      "   macro avg       0.74      0.63      0.64      3741\n",
      "weighted avg       0.77      0.78      0.75      3741\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_utils.get_prediction_report(y_test,bert_pred,labels=['not_nar','is_nar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ebf25813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7778702 , 0.22212978],\n",
       "       [0.7609829 , 0.23901714],\n",
       "       [0.7513149 , 0.24868506],\n",
       "       ...,\n",
       "       [0.76287687, 0.23712313],\n",
       "       [0.7348781 , 0.26512188],\n",
       "       [0.2802126 , 0.71978736]], dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_proba = bert_estimator.predict_proba(X_tensor_map_train)\n",
    "bert_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bc6a8a",
   "metadata": {},
   "source": [
    "## Try pipeline: transform -> fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "9e1aa541",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices=np.arange(1,11)\n",
    "test_indices=np.arange(12,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "756605d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "1fa3d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = common_utils.select_dic_keys(docs_map,train_indices)\n",
    "X_test = common_utils.select_dic_keys(docs_map,test_indices)\n",
    "y_train=common_utils.get_y_labels(docs_map,train_indices)\n",
    "y_test=common_utils.get_y_labels(docs_map,test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "72fea859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>init() called.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imp.reload(model_utils)\n",
    "imp.reload(common_utils)\n",
    "\n",
    "\n",
    "crf_preprocess = model_utils.CrfTransformer(seq_len=3,step=3)\n",
    "\n",
    "crf = CRF(\n",
    "    **best_params_crf,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    ")\n",
    "\n",
    "crf_estimator=model_utils.CrfClassifier(crf_model=crf)\n",
    "crf_pipe = Pipeline(steps=[('preprocessor',crf_preprocess), ('classifier', crf_estimator)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "7241fc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "c8cea92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', 'y', 'groups', 'X_bert', 'y_bert', 'X_3_3', 'y_3_3', 'groups_3_3'])"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "0557d9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-29 {color: black;background-color: white;}#sk-container-id-29 pre{padding: 0;}#sk-container-id-29 div.sk-toggleable {background-color: white;}#sk-container-id-29 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-29 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-29 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-29 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-29 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-29 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-29 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-29 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-29 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-29 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-29 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-29 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-29 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-29 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-29 div.sk-item {position: relative;z-index: 1;}#sk-container-id-29 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-29 div.sk-item::before, #sk-container-id-29 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-29 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-29 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-29 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-29 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-29 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-29 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-29 div.sk-label-container {text-align: center;}#sk-container-id-29 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-29 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-29\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;, CrfTransformer()),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 CrfClassifier(crf_model=CRF(algorithm=&#x27;lbfgs&#x27;,\n",
       "                                             all_possible_transitions=True,\n",
       "                                             c1=0.5052489623208797,\n",
       "                                             c2=0.03723629092212718,\n",
       "                                             linesearch=&#x27;MoreThuente&#x27;,\n",
       "                                             max_iterations=100,\n",
       "                                             min_freq=9)))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-104\" type=\"checkbox\" ><label for=\"sk-estimator-id-104\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;, CrfTransformer()),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 CrfClassifier(crf_model=CRF(algorithm=&#x27;lbfgs&#x27;,\n",
       "                                             all_possible_transitions=True,\n",
       "                                             c1=0.5052489623208797,\n",
       "                                             c2=0.03723629092212718,\n",
       "                                             linesearch=&#x27;MoreThuente&#x27;,\n",
       "                                             max_iterations=100,\n",
       "                                             min_freq=9)))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-105\" type=\"checkbox\" ><label for=\"sk-estimator-id-105\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CrfTransformer</label><div class=\"sk-toggleable__content\"><pre>CrfTransformer()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-106\" type=\"checkbox\" ><label for=\"sk-estimator-id-106\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: CrfClassifier</label><div class=\"sk-toggleable__content\"><pre>CrfClassifier(crf_model=CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True,\n",
       "                            c1=0.5052489623208797, c2=0.03723629092212718,\n",
       "                            linesearch=&#x27;MoreThuente&#x27;, max_iterations=100,\n",
       "                            min_freq=9))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-107\" type=\"checkbox\" ><label for=\"sk-estimator-id-107\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">crf_model: CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.5052489623208797,\n",
       "    c2=0.03723629092212718, linesearch=&#x27;MoreThuente&#x27;, max_iterations=100,\n",
       "    min_freq=9)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-108\" type=\"checkbox\" ><label for=\"sk-estimator-id-108\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.5052489623208797,\n",
       "    c2=0.03723629092212718, linesearch=&#x27;MoreThuente&#x27;, max_iterations=100,\n",
       "    min_freq=9)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor', CrfTransformer()),\n",
       "                ('classifier',\n",
       "                 CrfClassifier(crf_model=CRF(algorithm='lbfgs',\n",
       "                                             all_possible_transitions=True,\n",
       "                                             c1=0.5052489623208797,\n",
       "                                             c2=0.03723629092212718,\n",
       "                                             linesearch='MoreThuente',\n",
       "                                             max_iterations=100,\n",
       "                                             min_freq=9)))])"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "de216c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>transform() called.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-30 {color: black;background-color: white;}#sk-container-id-30 pre{padding: 0;}#sk-container-id-30 div.sk-toggleable {background-color: white;}#sk-container-id-30 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-30 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-30 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-30 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-30 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-30 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-30 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-30 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-30 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-30 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-30 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-30 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-30 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-30 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-30 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-30 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-30 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-30 div.sk-item {position: relative;z-index: 1;}#sk-container-id-30 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-30 div.sk-item::before, #sk-container-id-30 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-30 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-30 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-30 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-30 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-30 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-30 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-30 div.sk-label-container {text-align: center;}#sk-container-id-30 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-30 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-30\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CrfClassifier(crf_model=CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True,\n",
       "                            c1=0.5052489623208797, c2=0.03723629092212718,\n",
       "                            linesearch=&#x27;MoreThuente&#x27;, max_iterations=100,\n",
       "                            min_freq=9))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-109\" type=\"checkbox\" ><label for=\"sk-estimator-id-109\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CrfClassifier</label><div class=\"sk-toggleable__content\"><pre>CrfClassifier(crf_model=CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True,\n",
       "                            c1=0.5052489623208797, c2=0.03723629092212718,\n",
       "                            linesearch=&#x27;MoreThuente&#x27;, max_iterations=100,\n",
       "                            min_freq=9))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-110\" type=\"checkbox\" ><label for=\"sk-estimator-id-110\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">crf_model: CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.5052489623208797,\n",
       "    c2=0.03723629092212718, linesearch=&#x27;MoreThuente&#x27;, max_iterations=100,\n",
       "    min_freq=9)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-111\" type=\"checkbox\" ><label for=\"sk-estimator-id-111\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.5052489623208797,\n",
       "    c2=0.03723629092212718, linesearch=&#x27;MoreThuente&#x27;, max_iterations=100,\n",
       "    min_freq=9)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "CrfClassifier(crf_model=CRF(algorithm='lbfgs', all_possible_transitions=True,\n",
       "                            c1=0.5052489623208797, c2=0.03723629092212718,\n",
       "                            linesearch='MoreThuente', max_iterations=100,\n",
       "                            min_freq=9))"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_pipe.fit_transform(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "caec6f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>transform() called.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crf_proba=crf_pipe.predict_proba(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "962a0091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91155729, 0.08844271],\n",
       "       [0.94704095, 0.05295905],\n",
       "       [0.95368004, 0.04631996],\n",
       "       ...,\n",
       "       [0.99726994, 0.00273006],\n",
       "       [0.99043378, 0.00956622],\n",
       "       [0.86430078, 0.13569922]])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "042b8195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>init() called.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imp.reload(my_bert)\n",
    "imp.reload(common_utils)\n",
    "bert_preprocess = my_bert.BertTransformer(tokenizer=alephbert_tokenizer)\n",
    "bert_estimator = my_bert.BertClassifier(pretrained_model=alephbert_model)\n",
    "bert_pipe = Pipeline(steps=[('preprocessor',bert_preprocess), ('classifier', bert_estimator)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "7391e6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>fit() called.\n",
      "\n",
      "\n",
      ">>>>>>>transform() called.\n",
      "\n",
      ">>>>>>> fit() called\n",
      "Class Weights: [0.70645766 1.71090201]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-31 {color: black;background-color: white;}#sk-container-id-31 pre{padding: 0;}#sk-container-id-31 div.sk-toggleable {background-color: white;}#sk-container-id-31 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-31 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-31 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-31 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-31 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-31 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-31 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-31 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-31 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-31 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-31 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-31 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-31 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-31 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-31 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-31 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-31 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-31 div.sk-item {position: relative;z-index: 1;}#sk-container-id-31 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-31 div.sk-item::before, #sk-container-id-31 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-31 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-31 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-31 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-31 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-31 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-31 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-31 div.sk-label-container {text-align: center;}#sk-container-id-31 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-31 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-31\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-112\" type=\"checkbox\" checked><label for=\"sk-estimator-id-112\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BertClassifier</label><div class=\"sk-toggleable__content\"><pre>BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pipe.fit_transform(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "6bff1f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>transform() called.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_proba=bert_pipe.predict_proba(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "26ef8cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58403546, 0.41596457],\n",
       "       [0.48069516, 0.5193048 ],\n",
       "       [0.6089756 , 0.3910244 ],\n",
       "       ...,\n",
       "       [0.61370355, 0.38629642],\n",
       "       [0.5848076 , 0.41519243],\n",
       "       [0.56672853, 0.43327147]], dtype=float32)"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "c39a952e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<model_utils.DocsMapFold at 0x2b0c858b9d30>"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(model_utils)\n",
    "my_split=model_utils.DocsMapFold()\n",
    "my_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "0570e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 16\n",
      "[1, 3, 4, 5, 6] [32, 2, 34, 68, 36]\n",
      "64 16\n",
      "[1, 2, 4, 5, 6] [32, 33, 3, 36, 69]\n",
      "64 16\n",
      "[1, 2, 3, 5, 7] [64, 33, 32, 35, 4]\n"
     ]
    }
   ],
   "source": [
    "for tr,ts in my_split.split(docs_map):\n",
    "    print(len(tr),len(ts))\n",
    "    print(list(tr)[:5],list(ts)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "0456fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(\n",
    "    **best_params_crf,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_preprocess = model_utils.CrfTransformer(seq_len=3,step=3)\n",
    "crf_estimator=model_utils.CrfClassifier(crf_model=crf.clone())\n",
    "x_train_crf,y_train_crf=crf_preprocess.fit_transform(docs_map,{'indices':train_idx})\n",
    "x_test_crf,y_test_crf=crf_preprocess.fit_transform(docs_map,{'indices':test_idx})\n",
    "crf_estimator=\n",
    "\n",
    "bert_preprocess = my_bert.BertTransformer(tokenizer=alephbert_tokenizer)\n",
    "x_train_bert,y_train_bert=bert_preprocess.fit_transform(docs_map,{'indices':train_idx})\n",
    "x_test_bert,y_test_bert=bert_preprocess.fit_transform(docs_map,{'indices':test_idx}\n",
    "bert_estimator = my_bert.BertClassifier(pretrained_model=alephbert_model)\n",
    "bert_estimator                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "a0eef271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(y_train,np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "4649ef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.keys()), len(X_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "61423470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>transform() called.\n",
      "\n",
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>fit() called.\n",
      "\n",
      "\n",
      ">>>>>>>transform() called.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsofya/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/zsofya/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> fit() called\n",
      "Class Weights: [0.70645766 1.71090201]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-38 {color: black;background-color: white;}#sk-container-id-38 pre{padding: 0;}#sk-container-id-38 div.sk-toggleable {background-color: white;}#sk-container-id-38 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-38 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-38 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-38 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-38 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-38 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-38 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-38 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-38 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-38 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-38 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-38 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-38 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-38 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-38 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-38 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-38 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-38 div.sk-item {position: relative;z-index: 1;}#sk-container-id-38 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-38 div.sk-item::before, #sk-container-id-38 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-38 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-38 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-38 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-38 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-38 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-38 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-38 div.sk-label-container {text-align: center;}#sk-container-id-38 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-38 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-38\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MyVotingClassifier(estimators=[(&#x27;crf_pipe&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                                                 CrfTransformer()),\n",
       "                                                (&#x27;classifier&#x27;,\n",
       "                                                 CrfClassifier(crf_model=CRF(algorithm=&#x27;lbfgs&#x27;,\n",
       "                                                                             all_possible_transitions=True,\n",
       "                                                                             c1=0.5052489623208797,\n",
       "                                                                             c2=0.03723629092212718,\n",
       "                                                                             linesearch=&#x27;MoreThuente&#x27;,\n",
       "                                                                             max_iterations=100,\n",
       "                                                                             min_freq=9)))])),\n",
       "                               (&#x27;bert_pipe&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                                                 BertTransformer(...\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")))]))],\n",
       "                   voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-153\" type=\"checkbox\" ><label for=\"sk-estimator-id-153\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MyVotingClassifier</label><div class=\"sk-toggleable__content\"><pre>MyVotingClassifier(estimators=[(&#x27;crf_pipe&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                                                 CrfTransformer()),\n",
       "                                                (&#x27;classifier&#x27;,\n",
       "                                                 CrfClassifier(crf_model=CRF(algorithm=&#x27;lbfgs&#x27;,\n",
       "                                                                             all_possible_transitions=True,\n",
       "                                                                             c1=0.5052489623208797,\n",
       "                                                                             c2=0.03723629092212718,\n",
       "                                                                             linesearch=&#x27;MoreThuente&#x27;,\n",
       "                                                                             max_iterations=100,\n",
       "                                                                             min_freq=9)))])),\n",
       "                               (&#x27;bert_pipe&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                                                 BertTransformer(...\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")))]))],\n",
       "                   voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>crf_pipe</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-154\" type=\"checkbox\" ><label for=\"sk-estimator-id-154\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CrfTransformer</label><div class=\"sk-toggleable__content\"><pre>CrfTransformer()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-155\" type=\"checkbox\" ><label for=\"sk-estimator-id-155\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: CrfClassifier</label><div class=\"sk-toggleable__content\"><pre>CrfClassifier(crf_model=CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True,\n",
       "                            c1=0.5052489623208797, c2=0.03723629092212718,\n",
       "                            linesearch=&#x27;MoreThuente&#x27;, max_iterations=100,\n",
       "                            min_freq=9))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-156\" type=\"checkbox\" ><label for=\"sk-estimator-id-156\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">crf_model: CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.5052489623208797,\n",
       "    c2=0.03723629092212718, linesearch=&#x27;MoreThuente&#x27;, max_iterations=100,\n",
       "    min_freq=9)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-157\" type=\"checkbox\" ><label for=\"sk-estimator-id-157\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.5052489623208797,\n",
       "    c2=0.03723629092212718, linesearch=&#x27;MoreThuente&#x27;, max_iterations=100,\n",
       "    min_freq=9)</pre></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>bert_pipe</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-158\" type=\"checkbox\" ><label for=\"sk-estimator-id-158\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BertTransformer</label><div class=\"sk-toggleable__content\"><pre>BertTransformer(tokenizer=PreTrainedTokenizerFast(name_or_path=&#x27;onlplab/alephbert-base&#x27;, vocab_size=52000, model_max_len=512, is_fast=True, padding_side=&#x27;right&#x27;, special_tokens={&#x27;unk_token&#x27;: &#x27;[UNK]&#x27;, &#x27;sep_token&#x27;: &#x27;[SEP]&#x27;, &#x27;pad_token&#x27;: &#x27;[PAD]&#x27;, &#x27;cls_token&#x27;: &#x27;[CLS]&#x27;, &#x27;mask_token&#x27;: &#x27;[MASK]&#x27;}))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-159\" type=\"checkbox\" ><label for=\"sk-estimator-id-159\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BertClassifier</label><div class=\"sk-toggleable__content\"><pre>BertClassifier(pretrained_model=BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): B...\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "))</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MyVotingClassifier(estimators=[('crf_pipe',\n",
       "                                Pipeline(steps=[('preprocessor',\n",
       "                                                 CrfTransformer()),\n",
       "                                                ('classifier',\n",
       "                                                 CrfClassifier(crf_model=CRF(algorithm='lbfgs',\n",
       "                                                                             all_possible_transitions=True,\n",
       "                                                                             c1=0.5052489623208797,\n",
       "                                                                             c2=0.03723629092212718,\n",
       "                                                                             linesearch='MoreThuente',\n",
       "                                                                             max_iterations=100,\n",
       "                                                                             min_freq=9)))])),\n",
       "                               ('bert_pipe',\n",
       "                                Pipeline(steps=[('preprocessor',\n",
       "                                                 BertTransformer(...\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")))]))],\n",
       "                   voting='soft')"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier,StackingClassifier\n",
    "%%time\n",
    "imp.reload(model_utils)\n",
    "imp.reload(common_utils)\n",
    "\n",
    "# crf pipe\n",
    "crf_preprocess = model_utils.CrfTransformer(seq_len=3,step=3)\n",
    "\n",
    "crf = CRF(\n",
    "    **best_params_crf,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    ")\n",
    "\n",
    "crf_estimator=model_utils.CrfClassifier(crf_model=crf)\n",
    "crf_pipe = Pipeline(steps=[('preprocessor',crf_preprocess), ('classifier', crf_estimator)])\n",
    "# bert pipe\n",
    "bert_preprocess = my_bert.BertTransformer(tokenizer=alephbert_tokenizer)\n",
    "bert_estimator = my_bert.BertClassifier(pretrained_model=alephbert_model)\n",
    "bert_pipe = Pipeline(steps=[('preprocessor',bert_preprocess), ('classifier', bert_estimator)]) \n",
    "\n",
    "#voting\n",
    "vote = model_utils.MyVotingClassifier(\n",
    "estimators=[('crf_pipe',crf_pipe),('bert_pipe', bert_pipe)],\n",
    "   voting='soft')\n",
    "vote.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "bda4e9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>transform() called.\n",
      "\n",
      "\n",
      ">>>>>>>transform() called.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsofya/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.76063667, 0.23936328],\n",
       "       [0.7032875 , 0.29671247],\n",
       "       [0.77802077, 0.22197923],\n",
       "       ...,\n",
       "       [0.79033748, 0.20966253],\n",
       "       [0.78208809, 0.2179119 ],\n",
       "       [0.73401475, 0.26598522]])"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "af767235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>transform() called.\n",
      "\n",
      "\n",
      ">>>>>>>transform() called.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imp.reload(model_utils)\n",
    "voting_pred=stack.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "1143d1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['is_nar', 'is_nar', 'is_nar', ..., 'is_nar', 'is_nar', 'is_nar'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "3abd42e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[46m\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      is_nar       0.16      0.50      0.24       648\n",
      "     not_nar       0.46      0.14      0.21      1994\n",
      "\n",
      "    accuracy                           0.23      2642\n",
      "   macro avg       0.31      0.32      0.23      2642\n",
      "weighted avg       0.39      0.23      0.22      2642\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_utils.get_prediction_report(flatten(y_test),voting_pred,labels=vote.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "fc6b3f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 2642)"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test),len(flatten(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "ed6e821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>transform() called.\n",
      "\n",
      "\n",
      ">>>>>>>transform() called.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsofya/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "imp.reload(model_utils)\n",
    "imp.reload(my_bert)\n",
    "vote=vote.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "dad1b179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2148920917440065"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "2c6d3130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 164)"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "97a540b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>transform() called.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [164, 2642]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-570-d5e5008b3263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mscore_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscore_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sim_reg6/users/zsofya/classroom/MSc/personal_study/thesis/src/my_bert.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcommon_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sim_reg6/users/zsofya/classroom/MSc/personal_study/thesis/src/common_utils.py\u001b[0m in \u001b[0;36mget_score\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     output_dict=classification_report(\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2123\u001b[0m     \"\"\"\n\u001b[1;32m   2124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2125\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [164, 2642]"
     ]
    }
   ],
   "source": [
    "stack.estimators_[1].score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "18fc3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fit_and_score(\n",
    "    estimator_pipe,\n",
    "    docs_map,\n",
    "    scorer,\n",
    "    train_idx,\n",
    "    test_idx):\n",
    "    \n",
    "    result = {}\n",
    "    X_train,y_train = estimator_pipe.named_steps['preprocessor'].fit_transform(docs_map,train_idx)\n",
    "    X_test,y_test = estimator_pipe.named_steps['preprocessor'].fit_transform(docs_map,test_idx)\n",
    "    test_scores = estimator_pipee.named_steps['classifier'].fit(X_train)\n",
    "    test_scores = estimator_pipee.named_steps['classifier'].score(X_test,y_test)\n",
    "    result[\"test_scores\"] = test_scores\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "b32ffa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByDocFold:\n",
    "    def __init__(self, n_splits=3):\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        doc_indices = set(groups)\n",
    "        doc_count = len(doc_indices)\n",
    "        test_count = int(defines.TEST_PERSENT * doc_count)\n",
    "        for i in range(self.n_splits):\n",
    "            test_docs = set(random.sample(doc_indices, test_count))\n",
    "            train_docs = doc_indices - test_docs\n",
    "            train_idx = [i for i, j in enumerate(groups) if j in train_docs]\n",
    "            test_idx = [i for i, j in enumerate(groups) if j in test_docs]\n",
    "            yield train_idx, test_idx\n",
    "\n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a607ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'lbfgs',\n",
       " 'c1': 0.5052489623208797,\n",
       " 'c2': 0.03723629092212718,\n",
       " 'linesearch': 'MoreThuente',\n",
       " 'min_freq': 9}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "019b5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(\n",
    "    **best_params_crf,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "97a958eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>init() called.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imp.reload(model_utils)\n",
    "crf_preprocess = model_utils.CrfTransformer(seq_len,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1e698a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_pipe= Pipeline(steps=[('preprocessor', crf_preprocess), ('classifier', crf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9217a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>fit() called.\n",
      "\n",
      "\n",
      ">>>>>>>transform() called.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-ffefa0355805>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrf_preprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sim_reg6/users/zsofya/classroom/MSc/personal_study/thesis/src/model_utils.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, indices)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n>>>>>>>transform() called.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0mX_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mX_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X_{}_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "crf_preprocess.fit_transform(X=docs_map,indices=np.arange(1,70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ebe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df1.index\n",
    "x = preprocessing.scale(df1)\n",
    "\n",
    "phy_features = ['A', 'B', 'C']\n",
    "phy_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "phy_processer = ColumnTransformer(transformers=[('phy', phy_transformer, phy_features)])\n",
    "\n",
    "fa_features = ['D', 'E', 'F']\n",
    "fa_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "fa_processer = ColumnTransformer(transformers=[('fa', fa_transformer, fa_features)])\n",
    "\n",
    "\n",
    "pipe_phy = Pipeline(steps=[('preprocessor', phy_processer ),('classifier', SVM)])\n",
    "pipe_fa = Pipeline(steps=[('preprocessor', fa_processer ),('classifier', SVM)])\n",
    "\n",
    "ens = VotingClassifier(estimators=[pipe_phy, pipe_fa])\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "for train_index, test_index in cv.split(x):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    ens.fit(x_train,y_train)\n",
    "    print(ens.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "058e9efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', 'y', 'groups', 'X_3_3', 'y_3_3', 'groups_3_3', 'text'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_map[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ByDocFold()\n",
    "for train_index, test_index in cv.split(x):\n",
    "    x_train_crf, y_train_crf,_ = model_utils.get_X_y_by_doc_indices(docs_map,train_idx,seq_len,step)\n",
    "    x_test_crf, y_test_crf,_ = model_utils.get_X_y_by_doc_indices(docs_map,test_index,seq_len,step)\n",
    "\n",
    "    x_train_crf, y_train_crf,_ = model_utils.get_X_y_by_doc_indices(docs_map,train_idx,seq_len,step)\n",
    "    x_test_crf, y_test_crf,_ = model_utils.get_X_y_by_doc_indices(docs_map,test_index,seq_len,step)\n",
    "    \n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    ens.fit(x_train,y_train)\n",
    "    print(ens.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f2c99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr,ts=model_utils.get_test_train_idx(docs_map,0.2,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5b20d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(model_utils)\n",
    "y_test = model_utils.get_y_by_doc_indices(docs_map,tr,seq_len,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "319b9d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'not_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar', 'is_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'not_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'is_nar', 'is_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar',\n",
       "  'is_nar'],\n",
       " ['is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar', 'is_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar',\n",
       "  'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar', 'not_nar'],\n",
       " ['not_nar', 'not_nar', 'not_nar'],\n",
       " ...]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dcd2cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>>>init() called.\n",
      "\n",
      "\n",
      ">>>>>>>fit() called.\n",
      "\n",
      "\n",
      ">>>>>>>transform() called.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;transformer&#x27;,\n",
       "                 CrfTransformer(dir_name=&#x27;recalc_tfidf&#x27;, seq_len=3, step=3)),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.1,\n",
       "                     c2=0.1, max_iterations=100))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;transformer&#x27;,\n",
       "                 CrfTransformer(dir_name=&#x27;recalc_tfidf&#x27;, seq_len=3, step=3)),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.1,\n",
       "                     c2=0.1, max_iterations=100))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CrfTransformer</label><div class=\"sk-toggleable__content\"><pre>CrfTransformer(dir_name=&#x27;recalc_tfidf&#x27;, seq_len=3, step=3)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    max_iterations=100)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('transformer',\n",
       "                 CrfTransformer(dir_name='recalc_tfidf', seq_len=3, step=3)),\n",
       "                ('classifier',\n",
       "                 CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1,\n",
       "                     c2=0.1, max_iterations=100))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(model_utils)\n",
    "seq_len=3\n",
    "step=3\n",
    "crf_transformer = model_utils.CrfTransformer(seq_len,step,dir_name)\n",
    "crf_clf=CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True)\n",
    "crf_pipe = Pipeline(steps=[('transformer', crf_transformer ),('classifier', crf_clf)])\n",
    "crf_pipe.fit(tr,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e672fb33",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-12cefe8bd2b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocs_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/core/displayhook.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_displayhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_output_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_format_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_user_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_exec_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/core/displayhook.py\u001b[0m in \u001b[0;36mcompute_format_data\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \"\"\"\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m# This can be set to True by the write_output_prompt method in a subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreakable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;31m# Special case for 1-item tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreakable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;31m# Special case for 1-item tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreakable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tools/common/apps/python/3.8.5/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_GeneratorContextManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tools/common/apps/python/3.8.5/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Issue 19330: ensure context manager instances have good docstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__doc__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "docs_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a04d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e25dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting = VotingClassifier(\n",
    "    [\n",
    "    ('crf', \n",
    "      crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    )),\n",
    "#      ('alephbert',\n",
    "#       #loadmodel\n",
    "#      ),\n",
    "#         ('hebert',\n",
    "#         #loadmodel\n",
    "#         )\n",
    "    ('knn', KNeighborsClassifier())\n",
    "    ],\n",
    "    voting='soft')\n",
    "voting.fit(X_train, y_train)\n",
    "lr, tree = voting.estimators_\n",
    "tree.score(X_test, y_test), lr.score(X_test, y_test), voting.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd0d2b",
   "metadata": {},
   "source": [
    "### Prepare data for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a839e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_vectors(dim = 300):\n",
    "    X_vec = pd.DataFrame()\n",
    "    y_vec = pd.Series(dtype=int)\n",
    "    doc_list = glob.glob(os.path.join(os.getcwd(),defines.PATH_TO_DFS,\"*_sent_vec{}_db.csv\".format(dim)))\n",
    "    doc_list.sort()\n",
    "    for i,doc in enumerate(doc_list):\n",
    "        doc_idx = feature_utils.get_doc_idx_from_name(doc)\n",
    "        sent_vec_db = pd.read_csv(doc)\n",
    "        sent_db =  pd.read_csv(os.path.join(os.getcwd(),defines.PATH_TO_DFS,\"{:02d}_sent_db.csv\".format(doc_idx)),usecols=['is_nar'])\n",
    "        y_vec = pd.concat([y_vec,sent_db['is_nar']],ignore_index=True)\n",
    "        X_vec = pd.concat([X_vec,sent_vec_db],ignore_index=True)\n",
    "    return X_vec,y_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05108cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec,y_vec =concat_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9935e273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029905</td>\n",
       "      <td>0.038024</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>-0.002114</td>\n",
       "      <td>-0.001024</td>\n",
       "      <td>-0.023301</td>\n",
       "      <td>0.052745</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>-0.046377</td>\n",
       "      <td>0.020095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014652</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>-0.035123</td>\n",
       "      <td>-0.030111</td>\n",
       "      <td>-0.027191</td>\n",
       "      <td>0.026381</td>\n",
       "      <td>0.005289</td>\n",
       "      <td>0.005408</td>\n",
       "      <td>-0.020530</td>\n",
       "      <td>0.001053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.044579</td>\n",
       "      <td>0.024742</td>\n",
       "      <td>0.028893</td>\n",
       "      <td>-0.060388</td>\n",
       "      <td>0.039829</td>\n",
       "      <td>0.025243</td>\n",
       "      <td>-0.117620</td>\n",
       "      <td>-0.028324</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>-0.002992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>0.038958</td>\n",
       "      <td>-0.027126</td>\n",
       "      <td>-0.003454</td>\n",
       "      <td>-0.037883</td>\n",
       "      <td>0.018410</td>\n",
       "      <td>0.046375</td>\n",
       "      <td>0.052125</td>\n",
       "      <td>-0.023152</td>\n",
       "      <td>0.060160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.085588</td>\n",
       "      <td>0.067969</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>0.032841</td>\n",
       "      <td>-0.052474</td>\n",
       "      <td>-0.082648</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.004346</td>\n",
       "      <td>-0.054798</td>\n",
       "      <td>0.033916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015144</td>\n",
       "      <td>0.047520</td>\n",
       "      <td>0.017374</td>\n",
       "      <td>0.017066</td>\n",
       "      <td>-0.094182</td>\n",
       "      <td>-0.041293</td>\n",
       "      <td>-0.041231</td>\n",
       "      <td>-0.052368</td>\n",
       "      <td>0.037051</td>\n",
       "      <td>-0.036378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.114521</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.036471</td>\n",
       "      <td>-0.024207</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.014730</td>\n",
       "      <td>-0.104159</td>\n",
       "      <td>0.058651</td>\n",
       "      <td>0.005404</td>\n",
       "      <td>0.056529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028992</td>\n",
       "      <td>0.024537</td>\n",
       "      <td>-0.002414</td>\n",
       "      <td>0.039244</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.025045</td>\n",
       "      <td>0.097398</td>\n",
       "      <td>-0.004409</td>\n",
       "      <td>0.041561</td>\n",
       "      <td>-0.049388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.036749</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>-0.027156</td>\n",
       "      <td>-0.004715</td>\n",
       "      <td>-0.005032</td>\n",
       "      <td>0.006106</td>\n",
       "      <td>-0.031141</td>\n",
       "      <td>-0.070769</td>\n",
       "      <td>-0.020561</td>\n",
       "      <td>0.019293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034837</td>\n",
       "      <td>-0.046828</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>-0.009508</td>\n",
       "      <td>-0.058013</td>\n",
       "      <td>0.021407</td>\n",
       "      <td>-0.048019</td>\n",
       "      <td>-0.067683</td>\n",
       "      <td>-0.061374</td>\n",
       "      <td>-0.075125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33231</th>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.030761</td>\n",
       "      <td>0.006861</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>-0.034013</td>\n",
       "      <td>-0.000644</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>-0.025179</td>\n",
       "      <td>0.023791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004880</td>\n",
       "      <td>0.018460</td>\n",
       "      <td>-0.012988</td>\n",
       "      <td>-0.015199</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>0.018707</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>0.018673</td>\n",
       "      <td>-0.018850</td>\n",
       "      <td>-0.015083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33232</th>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.025412</td>\n",
       "      <td>0.033902</td>\n",
       "      <td>0.024731</td>\n",
       "      <td>-0.006604</td>\n",
       "      <td>0.030173</td>\n",
       "      <td>-0.049072</td>\n",
       "      <td>0.008212</td>\n",
       "      <td>-0.030461</td>\n",
       "      <td>-0.011216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>-0.001061</td>\n",
       "      <td>-0.019386</td>\n",
       "      <td>0.016321</td>\n",
       "      <td>-0.066378</td>\n",
       "      <td>0.016374</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>0.017209</td>\n",
       "      <td>0.015820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33233</th>\n",
       "      <td>-0.000916</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>-0.006212</td>\n",
       "      <td>-0.010558</td>\n",
       "      <td>-0.056428</td>\n",
       "      <td>-0.062275</td>\n",
       "      <td>-0.031020</td>\n",
       "      <td>0.041746</td>\n",
       "      <td>-0.062375</td>\n",
       "      <td>0.090405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031854</td>\n",
       "      <td>-0.019589</td>\n",
       "      <td>-0.008862</td>\n",
       "      <td>0.013844</td>\n",
       "      <td>-0.076933</td>\n",
       "      <td>-0.032281</td>\n",
       "      <td>-0.060818</td>\n",
       "      <td>-0.041205</td>\n",
       "      <td>-0.029383</td>\n",
       "      <td>-0.056909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33234</th>\n",
       "      <td>0.048067</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>-0.032234</td>\n",
       "      <td>-0.009326</td>\n",
       "      <td>-0.017668</td>\n",
       "      <td>-0.028665</td>\n",
       "      <td>0.105883</td>\n",
       "      <td>-0.009100</td>\n",
       "      <td>-0.036032</td>\n",
       "      <td>-0.006796</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016274</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.031665</td>\n",
       "      <td>-0.035877</td>\n",
       "      <td>0.018119</td>\n",
       "      <td>0.033564</td>\n",
       "      <td>-0.041964</td>\n",
       "      <td>-0.011305</td>\n",
       "      <td>0.029903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33235</th>\n",
       "      <td>-0.000916</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>-0.006212</td>\n",
       "      <td>-0.010558</td>\n",
       "      <td>-0.056428</td>\n",
       "      <td>-0.062275</td>\n",
       "      <td>-0.031020</td>\n",
       "      <td>0.041746</td>\n",
       "      <td>-0.062375</td>\n",
       "      <td>0.090405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031854</td>\n",
       "      <td>-0.019589</td>\n",
       "      <td>-0.008862</td>\n",
       "      <td>0.013844</td>\n",
       "      <td>-0.076933</td>\n",
       "      <td>-0.032281</td>\n",
       "      <td>-0.060818</td>\n",
       "      <td>-0.041205</td>\n",
       "      <td>-0.029383</td>\n",
       "      <td>-0.056909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33236 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.029905  0.038024  0.000409 -0.002114 -0.001024 -0.023301  0.052745   \n",
       "1      0.044579  0.024742  0.028893 -0.060388  0.039829  0.025243 -0.117620   \n",
       "2     -0.085588  0.067969  0.007988  0.032841 -0.052474 -0.082648  0.003187   \n",
       "3     -0.114521  0.000108  0.036471 -0.024207  0.002051  0.014730 -0.104159   \n",
       "4      0.036749  0.012629 -0.027156 -0.004715 -0.005032  0.006106 -0.031141   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "33231  0.000403  0.030761  0.006861  0.004358  0.004020 -0.034013 -0.000644   \n",
       "33232  0.005242  0.025412  0.033902  0.024731 -0.006604  0.030173 -0.049072   \n",
       "33233 -0.000916  0.064209 -0.006212 -0.010558 -0.056428 -0.062275 -0.031020   \n",
       "33234  0.048067  0.013103 -0.032234 -0.009326 -0.017668 -0.028665  0.105883   \n",
       "33235 -0.000916  0.064209 -0.006212 -0.010558 -0.056428 -0.062275 -0.031020   \n",
       "\n",
       "              7         8         9  ...       290       291       292  \\\n",
       "0      0.006430 -0.046377  0.020095  ...  0.014652  0.000285 -0.035123   \n",
       "1     -0.028324  0.013425 -0.002992  ...  0.009427  0.038958 -0.027126   \n",
       "2      0.004346 -0.054798  0.033916  ...  0.015144  0.047520  0.017374   \n",
       "3      0.058651  0.005404  0.056529  ... -0.028992  0.024537 -0.002414   \n",
       "4     -0.070769 -0.020561  0.019293  ...  0.034837 -0.046828  0.011486   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "33231  0.006724 -0.025179  0.023791  ... -0.004880  0.018460 -0.012988   \n",
       "33232  0.008212 -0.030461 -0.011216  ...  0.003355 -0.001061 -0.019386   \n",
       "33233  0.041746 -0.062375  0.090405  ... -0.031854 -0.019589 -0.008862   \n",
       "33234 -0.009100 -0.036032 -0.006796  ... -0.016274 -0.000202  0.001330   \n",
       "33235  0.041746 -0.062375  0.090405  ... -0.031854 -0.019589 -0.008862   \n",
       "\n",
       "            293       294       295       296       297       298       299  \n",
       "0     -0.030111 -0.027191  0.026381  0.005289  0.005408 -0.020530  0.001053  \n",
       "1     -0.003454 -0.037883  0.018410  0.046375  0.052125 -0.023152  0.060160  \n",
       "2      0.017066 -0.094182 -0.041293 -0.041231 -0.052368  0.037051 -0.036378  \n",
       "3      0.039244 -0.094238  0.025045  0.097398 -0.004409  0.041561 -0.049388  \n",
       "4     -0.009508 -0.058013  0.021407 -0.048019 -0.067683 -0.061374 -0.075125  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "33231 -0.015199  0.007455  0.018707  0.010102  0.018673 -0.018850 -0.015083  \n",
       "33232  0.016321 -0.066378  0.016374  0.000689  0.015705  0.017209  0.015820  \n",
       "33233  0.013844 -0.076933 -0.032281 -0.060818 -0.041205 -0.029383 -0.056909  \n",
       "33234  0.031665 -0.035877  0.018119  0.033564 -0.041964 -0.011305  0.029903  \n",
       "33235  0.013844 -0.076933 -0.032281 -0.060818 -0.041205 -0.029383 -0.056909  \n",
       "\n",
       "[33236 rows x 300 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dffe44e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "33231    0.0\n",
       "33232    0.0\n",
       "33233    0.0\n",
       "33234    0.0\n",
       "33235    0.0\n",
       "Length: 33236, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05669a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y_vec, test_size=0.33, random_state=42)\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14815320",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4828648f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.86      0.80      7570\n",
      "         1.0       0.52      0.33      0.40      3398\n",
      "\n",
      "    accuracy                           0.70     10968\n",
      "   macro avg       0.63      0.60      0.60     10968\n",
      "weighted avg       0.67      0.70      0.67     10968\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_utils.get_prediction_report(y_test,y_pred,knn.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8241162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators={}\n",
    "estimators['crf'] = {}\n",
    "estimators['crf']['mod'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da614b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf=estimators['crf']['mod']\n",
    "alephbert=estimators['alephbert']['mod']\n",
    "hebert=estimators['hebert']['mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [crf,alephbert,hebert]\n",
    "scv=my_ensembler.ensemble()\n",
    "\n",
    "# scv.fit(X_train,y_train)\n",
    "# scv_predicted = scv.predict(X_test)\n",
    "# scv_conf_matrix = confusion_matrix(y_test, scv_predicted)\n",
    "# scv_acc_score = accuracy_score(y_test, scv_predicted)\n",
    "# print(\"confussion matrix\")\n",
    "# print(scv_conf_matrix)\n",
    "# print(\"\\n\")\n",
    "# print(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\n",
    "# print(classification_report(y_test,scv_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b78cfc",
   "metadata": {},
   "source": [
    "# Manual ensemble crf + BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06191465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp, model_utils, feature_utils, my_bert, common_utils\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from sklearn_crfsuite import scorers, CRF\n",
    "from sklearn_crfsuite.utils import flatten\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eda6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_map_path =  os.path.join(os.getcwd(),defines.PATH_TO_DFS,dir_name,\"docs_map.json\")\n",
    "with open(doc_map_path, 'r') as fp:\n",
    "    docs_map = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "82d535ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 69, 70, 72, 12, 13, 14, 78, 18, 27, 28, 29]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a276f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_map = {int(k):v for k,v in docs_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef2ca3",
   "metadata": {},
   "source": [
    "### Load test docs that BERT has not seen yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "6e1c43ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs = [int(i) for i in docs_map.keys()]\n",
    "all_docs = set(all_docs)\n",
    "train_docs = all_docs - set(test_docs)\n",
    "len(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afd2fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, sys, numpy as np\n",
    "sys.path.append('./src/')\n",
    "import defines\n",
    "dir_name=\"recalc_tfidf\"\n",
    "json_path = os.path.join(os.getcwd(),defines.PATH_TO_DFS,dir_name,\"test_doc_indices.json\")\n",
    "with open(json_path, 'r') as fp:\n",
    "    test_docs = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a9e094",
   "metadata": {},
   "source": [
    "#### Prepare tested docs for input to CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "ad5d9d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68] 439 -> 147 [57] 271 -> 91 [42] 136 -> 46 [63] 338 -> 113 [76] 132 -> 44 [49] 206 -> 69 [21] 108 -> 36 [34] 49 -> 17 [15] 121 -> 41 [39] 111 -> 37 [13] 90 -> 30 [6] 420 -> 140 [27] 67 -> 23 [32] 73 -> 25 [18] 62 -> 21 [65] 335 -> 112 [70] 183 -> 61 [51] 208 -> 70 [44] 199 -> 67 [48] 223 -> 75 [62] 248 -> 83 [77] 176 -> 59 [56] 268 -> 90 [43] 99 -> 33 [69] 517 -> 173 [14] 129 -> 43 [1] 203 -> 68 [20] 90 -> 30 [35] 63 -> 21 [19] 265 -> 89 [26] 114 -> 38 [33] 91 -> 31 [12] 150 -> 50 [7] 248 -> 83 [38] 90 -> 30 [50] 184 -> 62 [45] 204 -> 68 [64] 347 -> 116 [71] 156 -> 52 [2] 156 -> 52 [17] 322 -> 108 [28] 167 -> 56 [80] 223 -> 75 [9] 276 -> 92 [36] 87 -> 29 [23] 74 -> 25 [74] 95 -> 32 [61] 433 -> 145 [40] 148 -> 50 [55] 249 -> 83 [46] 200 -> 67 [53] 149 -> 50 [79] 172 -> 58 [58] 384 -> 128 [72] 420 -> 140 [67] 407 -> 136 [30] 82 -> 28 [25] 122 -> 41 [4] 132 -> 44 [11] 121 -> 41 [37] 99 -> 33 [22] 89 -> 30 [8] 82 -> 28 [29] 105 -> 35 [3] 113 -> 38 [16] 102 -> 34 [41] 137 -> 46 [54] 243 -> 81 [75] 68 -> 23 [60] 245 -> 82 [73] 85 -> 29 [66] 366 -> 122 [59] 370 -> 124 [78] 122 -> 41 [47] 160 -> 54 [52] 275 -> 92 [5] 65 -> 22 [10] 123 -> 41 [31] 79 -> 27 [24] 158 -> 53 "
     ]
    }
   ],
   "source": [
    "imp.reload(model_utils)\n",
    "seq_len=3\n",
    "step=3\n",
    "per_par=True\n",
    "feature_utils.reshape_docs_map_to_seq(docs_map,per_par,seq_len,step)\n",
    "X_test,y_test,groups_test = model_utils.get_X_y_by_doc_indices(docs_map,test_docs,seq_len,step)\n",
    "train_docs = set(docs_map.keys()) - set(test_docs)\n",
    "X_train,y_train,groups_train = model_utils.get_X_y_by_doc_indices(docs_map,train_docs,seq_len,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "275ecab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_question': 0.0,\n",
       "  'par_idx_in_doc': 27.0,\n",
       "  'sent_len': 33.0,\n",
       "  'is_client': 1,\n",
       "  'sent_idx_in_par': 0,\n",
       "  'sent_pos_in_par': 0.3333333333333333,\n",
       "  'sent_pos_in_doc': 0.1267605633802817,\n",
       "  'TOKEN': 8,\n",
       "  'POSTAG_BN': 0.125,\n",
       "  'POSTAG_CC': 0.25,\n",
       "  'POSTAG_PRP': 0.125,\n",
       "  'POSTAG_RB': 0.5,\n",
       "  'f_gen_M': 0.25,\n",
       "  'f_num_S': 0.25,\n",
       "  'f_per_1': 0.125,\n",
       "  'f_per_A': 0.125,\n",
       "  'sent_idx': 53,\n",
       "  '-1:is_question': 0.0,\n",
       "  '-1:par_idx_in_doc': 26.0,\n",
       "  '-1:sent_len': 2.0,\n",
       "  '-1:is_client': 0,\n",
       "  '-1:sent_idx_in_par': 1,\n",
       "  '-1:sent_pos_in_par': 1.0,\n",
       "  '-1:sent_pos_in_doc': 0.1244131455399061,\n",
       "  '-1:TOKEN': 1,\n",
       "  '-1:POSTAG_NNP': 1.0,\n",
       "  '-1:f_gen_M': 1.0,\n",
       "  '-1:f_num_S': 1.0,\n",
       "  '+1:is_question': 0.0,\n",
       "  '+1:par_idx_in_doc': 27.0,\n",
       "  '+1:sent_len': 56.0,\n",
       "  '+1:is_client': 1,\n",
       "  '+1:sent_idx_in_par': 1,\n",
       "  '+1:sent_pos_in_par': 0.6666666666666666,\n",
       "  '+1:sent_pos_in_doc': 0.1291079812206572,\n",
       "  '+1:TOKEN': 12,\n",
       "  '+1:POSTAG_BN': 0.0833333333333333,\n",
       "  '+1:POSTAG_CC': 0.1666666666666666,\n",
       "  '+1:POSTAG_COP': 0.1666666666666666,\n",
       "  '+1:POSTAG_DEF': 0.1666666666666666,\n",
       "  '+1:POSTAG_NN': 0.1666666666666666,\n",
       "  '+1:POSTAG_QW': 0.1666666666666666,\n",
       "  '+1:POSTAG_RB': 0.25,\n",
       "  '+1:f_gen_F': 0.25,\n",
       "  '+1:f_gen_M': 0.1666666666666666,\n",
       "  '+1:f_num_P': 0.1666666666666666,\n",
       "  '+1:f_num_S': 0.25,\n",
       "  '+1:f_per_3': 0.1666666666666666,\n",
       "  '+1:f_per_A': 0.0833333333333333,\n",
       "  '+1:f_tense_PAST': 0.0833333333333333,\n",
       "  '-2:is_question': 0.0,\n",
       "  '-2:par_idx_in_doc': 26.0,\n",
       "  '-2:sent_len': 54.0,\n",
       "  '-2:is_client': 0,\n",
       "  '-2:sent_idx_in_par': 0,\n",
       "  '-2:sent_pos_in_par': 0.5,\n",
       "  '-2:sent_pos_in_doc': 0.1220657276995305,\n",
       "  '-2:TOKEN': 13,\n",
       "  '-2:POSTAG_AT': 0.0769230769230769,\n",
       "  '-2:POSTAG_BN': 0.0769230769230769,\n",
       "  '-2:POSTAG_JJ': 0.0769230769230769,\n",
       "  '-2:POSTAG_MD': 0.0769230769230769,\n",
       "  '-2:POSTAG_PRP': 0.3846153846153846,\n",
       "  '-2:POSTAG_RB': 0.3076923076923077,\n",
       "  '-2:POSTAG_REL': 0.1538461538461538,\n",
       "  '-2:f_gen_M': 0.6153846153846154,\n",
       "  '-2:f_num_S': 0.6153846153846154,\n",
       "  '-2:f_per_1': 0.0769230769230769,\n",
       "  '-2:f_per_2': 0.0769230769230769,\n",
       "  '-2:f_per_3': 0.2307692307692307,\n",
       "  '-2:f_per_A': 0.1538461538461538,\n",
       "  '+2:is_question': 0.0,\n",
       "  '+2:par_idx_in_doc': 27.0,\n",
       "  '+2:sent_len': 14.0,\n",
       "  '+2:is_client': 1,\n",
       "  '+2:sent_idx_in_par': 2,\n",
       "  '+2:sent_pos_in_par': 1.0,\n",
       "  '+2:sent_pos_in_doc': 0.1314553990610328,\n",
       "  '+2:TOKEN': 4,\n",
       "  '+2:POSTAG_AT': 0.25,\n",
       "  '+2:POSTAG_NN': 0.25,\n",
       "  '+2:POSTAG_PRP': 0.25,\n",
       "  '+2:POSTAG_REL': 0.25,\n",
       "  '+2:POSTAG_VB': 0.25,\n",
       "  '+2:f_gen_M': 0.75,\n",
       "  '+2:f_num_P': 0.5,\n",
       "  '+2:f_num_S': 0.25,\n",
       "  '+2:f_per_3': 0.5,\n",
       "  '+2:f_tense_PAST': 0.25,\n",
       "  '-1.sim': 0.2357505610910759,\n",
       "  '+1.sim': 0.8824433723867267,\n",
       "  '-2.sim': 0.7555743345476257,\n",
       "  '+2.sim': 0.488730448331177,\n",
       "  'tfidf_word_13431': 0.4466707119079723,\n",
       "  'tfidf_word_13048': 0.3123365526019856,\n",
       "  'tfidf_word_12722': 0.2701771703902461,\n",
       "  'tfidf_word_12147': 0.40828801384198504,\n",
       "  'tfidf_word_1787': 0.5496803914073314,\n",
       "  'tfidf_word_1009': 0.24303144909436122,\n",
       "  'tfidf_word_305': 0.3194112230943005,\n",
       "  'tfidf_char_wb_26095': 0.08024483853662347,\n",
       "  'tfidf_char_wb_22192': 0.09239539296791002,\n",
       "  'tfidf_char_wb_21601': 0.16176129663472158,\n",
       "  'tfidf_char_wb_21324': 0.11609707373681546,\n",
       "  'tfidf_char_wb_20913': 0.09916457529972059,\n",
       "  'tfidf_char_wb_20912': 0.0991534548897552,\n",
       "  'tfidf_char_wb_20909': 0.09908680324271692,\n",
       "  'tfidf_char_wb_20017': 0.09720485334786105,\n",
       "  'tfidf_char_wb_20016': 0.0963710081129579,\n",
       "  'tfidf_char_wb_19556': 0.20431965756554601,\n",
       "  'tfidf_char_wb_19555': 0.20383196633913309,\n",
       "  'tfidf_char_wb_19462': 0.15260147043245048,\n",
       "  'tfidf_char_wb_19461': 0.1313110751778557,\n",
       "  'tfidf_char_wb_19460': 0.1313110751778557,\n",
       "  'tfidf_char_wb_17082': 0.17542450312281838,\n",
       "  'tfidf_char_wb_15594': 0.15130882497457968,\n",
       "  'tfidf_char_wb_15593': 0.1291805059486046,\n",
       "  'tfidf_char_wb_12550': 0.14915642681020483,\n",
       "  'tfidf_char_wb_12382': 0.20441797996000424,\n",
       "  'tfidf_char_wb_12381': 0.20431965756554601,\n",
       "  'tfidf_char_wb_12374': 0.19902624827660162,\n",
       "  'tfidf_char_wb_10888': 0.11617662623085485,\n",
       "  'tfidf_char_wb_10208': 0.20461542016829046,\n",
       "  'tfidf_char_wb_10207': 0.20422159841743873,\n",
       "  'tfidf_char_wb_10201': 0.18727797957037848,\n",
       "  'tfidf_char_wb_9609': 0.08398958364075809,\n",
       "  'tfidf_char_wb_9608': 0.08393817487162421,\n",
       "  'tfidf_char_wb_9301': 0.09938769551422189,\n",
       "  'tfidf_char_wb_9300': 0.09913678060313003,\n",
       "  'tfidf_char_wb_9299': 0.09907015926608893,\n",
       "  'tfidf_char_wb_8864': 0.11901834378307745,\n",
       "  'tfidf_char_wb_8863': 0.11890377099952977,\n",
       "  'tfidf_char_wb_4356': 0.16739357295751084,\n",
       "  'tfidf_char_wb_4355': 0.162849526712869,\n",
       "  'tfidf_char_wb_4239': 0.11868516995908009,\n",
       "  'tfidf_char_wb_4238': 0.11660384938781772,\n",
       "  'tfidf_char_wb_4095': 0.10089539490134311,\n",
       "  'tfidf_char_wb_4093': 0.10084296100557338,\n",
       "  'tfidf_char_wb_4087': 0.09818784888516953,\n",
       "  'tfidf_char_wb_3870': 0.13153918589471936,\n",
       "  'tfidf_char_wb_3869': 0.13153918589471936,\n",
       "  'tfidf_char_wb_3863': 0.10979375294917838,\n",
       "  'tfidf_char_wb_744': 0.20511372812640738,\n",
       "  'tfidf_char_wb_741': 0.19562058196963855,\n",
       "  'tfidf_char_wb_737': 0.1753353909865464,\n",
       "  'tfidf_char_wb_438': 0.09083876287832576,\n",
       "  'tfidf_char_wb_437': 0.09076794701106286,\n",
       "  'tfidf_char_wb_426': 0.0869024787833756,\n",
       "  'tfidf_char_wb_140': 0.11945121241929049,\n",
       "  'tfidf_char_wb_139': 0.11933527962945976,\n",
       "  'tfidf_char_wb_124': 0.11734541934755983,\n",
       "  'tfidf_lemma_4968': 0.4420939378933875,\n",
       "  'tfidf_lemma_4851': 0.3190021806327028,\n",
       "  'tfidf_lemma_4681': 0.2784336934852219,\n",
       "  'tfidf_lemma_4338': 0.35426876192410106,\n",
       "  'tfidf_lemma_2132': 0.17092151339674028,\n",
       "  'tfidf_lemma_1118': 0.6045104963978305,\n",
       "  'tfidf_lemma_192': 0.3242094654532107},\n",
       " {'is_question': 0.0,\n",
       "  'par_idx_in_doc': 27.0,\n",
       "  'sent_len': 56.0,\n",
       "  'is_client': 1,\n",
       "  'sent_idx_in_par': 1,\n",
       "  'sent_pos_in_par': 0.6666666666666666,\n",
       "  'sent_pos_in_doc': 0.1291079812206572,\n",
       "  'TOKEN': 12,\n",
       "  'POSTAG_BN': 0.0833333333333333,\n",
       "  'POSTAG_CC': 0.1666666666666666,\n",
       "  'POSTAG_COP': 0.1666666666666666,\n",
       "  'POSTAG_DEF': 0.1666666666666666,\n",
       "  'POSTAG_NN': 0.1666666666666666,\n",
       "  'POSTAG_QW': 0.1666666666666666,\n",
       "  'POSTAG_RB': 0.25,\n",
       "  'f_gen_F': 0.25,\n",
       "  'f_gen_M': 0.1666666666666666,\n",
       "  'f_num_P': 0.1666666666666666,\n",
       "  'f_num_S': 0.25,\n",
       "  'f_per_3': 0.1666666666666666,\n",
       "  'f_per_A': 0.0833333333333333,\n",
       "  'f_tense_PAST': 0.0833333333333333,\n",
       "  'sent_idx': 54,\n",
       "  '-1:is_question': 0.0,\n",
       "  '-1:par_idx_in_doc': 27.0,\n",
       "  '-1:sent_len': 33.0,\n",
       "  '-1:is_client': 1,\n",
       "  '-1:sent_idx_in_par': 0,\n",
       "  '-1:sent_pos_in_par': 0.3333333333333333,\n",
       "  '-1:sent_pos_in_doc': 0.1267605633802817,\n",
       "  '-1:TOKEN': 8,\n",
       "  '-1:POSTAG_BN': 0.125,\n",
       "  '-1:POSTAG_CC': 0.25,\n",
       "  '-1:POSTAG_PRP': 0.125,\n",
       "  '-1:POSTAG_RB': 0.5,\n",
       "  '-1:f_gen_M': 0.25,\n",
       "  '-1:f_num_S': 0.25,\n",
       "  '-1:f_per_1': 0.125,\n",
       "  '-1:f_per_A': 0.125,\n",
       "  '+1:is_question': 0.0,\n",
       "  '+1:par_idx_in_doc': 27.0,\n",
       "  '+1:sent_len': 14.0,\n",
       "  '+1:is_client': 1,\n",
       "  '+1:sent_idx_in_par': 2,\n",
       "  '+1:sent_pos_in_par': 1.0,\n",
       "  '+1:sent_pos_in_doc': 0.1314553990610328,\n",
       "  '+1:TOKEN': 4,\n",
       "  '+1:POSTAG_AT': 0.25,\n",
       "  '+1:POSTAG_NN': 0.25,\n",
       "  '+1:POSTAG_PRP': 0.25,\n",
       "  '+1:POSTAG_REL': 0.25,\n",
       "  '+1:POSTAG_VB': 0.25,\n",
       "  '+1:f_gen_M': 0.75,\n",
       "  '+1:f_num_P': 0.5,\n",
       "  '+1:f_num_S': 0.25,\n",
       "  '+1:f_per_3': 0.5,\n",
       "  '+1:f_tense_PAST': 0.25,\n",
       "  '-2:is_question': 0.0,\n",
       "  '-2:par_idx_in_doc': 26.0,\n",
       "  '-2:sent_len': 2.0,\n",
       "  '-2:is_client': 0,\n",
       "  '-2:sent_idx_in_par': 1,\n",
       "  '-2:sent_pos_in_par': 1.0,\n",
       "  '-2:sent_pos_in_doc': 0.1244131455399061,\n",
       "  '-2:TOKEN': 1,\n",
       "  '-2:POSTAG_NNP': 1.0,\n",
       "  '-2:f_gen_M': 1.0,\n",
       "  '-2:f_num_S': 1.0,\n",
       "  '+2:is_question': 0.0,\n",
       "  '+2:par_idx_in_doc': 28.0,\n",
       "  '+2:sent_len': 15.0,\n",
       "  '+2:is_client': 0,\n",
       "  '+2:sent_idx_in_par': 0,\n",
       "  '+2:sent_pos_in_par': 1.0,\n",
       "  '+2:sent_pos_in_doc': 0.1338028169014084,\n",
       "  '+2:TOKEN': 3,\n",
       "  '+2:POSTAG_CC': 0.3333333333333333,\n",
       "  '+2:POSTAG_CONJ': 0.3333333333333333,\n",
       "  '+2:POSTAG_PRP': 0.3333333333333333,\n",
       "  '+2:POSTAG_RB': 0.3333333333333333,\n",
       "  '+2:f_gen_M': 0.3333333333333333,\n",
       "  '+2:f_num_P': 0.3333333333333333,\n",
       "  '+2:f_per_1': 0.3333333333333333,\n",
       "  '-1.sim': 0.8824433723867267,\n",
       "  '+1.sim': 0.5365377910415532,\n",
       "  '-2.sim': 0.2112911061488303,\n",
       "  '+2.sim': 0.6642002077305322,\n",
       "  'tfidf_word_16202': 0.35753841546442555,\n",
       "  'tfidf_word_13431': 0.2558534455884961,\n",
       "  'tfidf_word_12722': 0.15475776253254023,\n",
       "  'tfidf_word_11198': 0.32907670728618216,\n",
       "  'tfidf_word_8008': 0.38223993298986947,\n",
       "  'tfidf_word_6079': 0.4428932916692584,\n",
       "  'tfidf_word_5624': 0.2982057982237194,\n",
       "  'tfidf_word_5573': 0.309676388879174,\n",
       "  'tfidf_word_1787': 0.3148574965062283,\n",
       "  'tfidf_word_408': 0.2196374519472877,\n",
       "  'tfidf_char_wb_32980': 0.1235164602317582,\n",
       "  'tfidf_char_wb_32979': 0.11680698830846664,\n",
       "  'tfidf_char_wb_32973': 0.11094067550311282,\n",
       "  'tfidf_char_wb_32807': 0.06260355764955808,\n",
       "  'tfidf_char_wb_31156': 0.07259493519838375,\n",
       "  'tfidf_char_wb_31013': 0.1394227434372419,\n",
       "  'tfidf_char_wb_31012': 0.13908182616669224,\n",
       "  'tfidf_char_wb_30997': 0.0963090410931358,\n",
       "  'tfidf_char_wb_24321': 0.14012181725568548,\n",
       "  'tfidf_char_wb_24318': 0.11939437567685167,\n",
       "  'tfidf_char_wb_24317': 0.11887608124141101,\n",
       "  'tfidf_char_wb_23807': 0.11474805435881774,\n",
       "  'tfidf_char_wb_22192': 0.05246153159090577,\n",
       "  'tfidf_char_wb_21601': 0.09184706186092736,\n",
       "  'tfidf_char_wb_21433': 0.11535294813584093,\n",
       "  'tfidf_char_wb_20913': 0.056305031373067194,\n",
       "  'tfidf_char_wb_20912': 0.056298717273197456,\n",
       "  'tfidf_char_wb_20909': 0.056260872881021974,\n",
       "  'tfidf_char_wb_20818': 0.0983755401561193,\n",
       "  'tfidf_char_wb_20817': 0.09816176398473021,\n",
       "  'tfidf_char_wb_20017': 0.0551923133923928,\n",
       "  'tfidf_char_wb_20016': 0.05471886123501098,\n",
       "  'tfidf_char_wb_19905': 0.10778502610472704,\n",
       "  'tfidf_char_wb_19904': 0.10763541763382888,\n",
       "  'tfidf_char_wb_19900': 0.07880675021372816,\n",
       "  'tfidf_char_wb_19556': 0.11601143548077929,\n",
       "  'tfidf_char_wb_19555': 0.11573452742444411,\n",
       "  'tfidf_char_wb_19445': 0.07845568521931788,\n",
       "  'tfidf_char_wb_18562': 0.13994483331285704,\n",
       "  'tfidf_char_wb_18558': 0.12856064778889284,\n",
       "  'tfidf_char_wb_18554': 0.1009926989498037,\n",
       "  'tfidf_char_wb_18049': 0.1240019800540141,\n",
       "  'tfidf_char_wb_18048': 0.1235164602317582,\n",
       "  'tfidf_char_wb_18041': 0.07851991135213876,\n",
       "  'tfidf_char_wb_17616': 0.12250184496990543,\n",
       "  'tfidf_char_wb_17615': 0.11757400104015088,\n",
       "  'tfidf_char_wb_17614': 0.11745414198925447,\n",
       "  'tfidf_char_wb_17427': 0.05205605741665134,\n",
       "  'tfidf_char_wb_17348': 0.09435061490338474,\n",
       "  'tfidf_char_wb_17347': 0.0923811168562119,\n",
       "  'tfidf_char_wb_17082': 0.09960494583959403,\n",
       "  'tfidf_char_wb_16289': 0.1204024318824251,\n",
       "  'tfidf_char_wb_16288': 0.11461017318623555,\n",
       "  'tfidf_char_wb_14993': 0.1345431368056829,\n",
       "  'tfidf_char_wb_14990': 0.13210888864005593,\n",
       "  'tfidf_char_wb_14986': 0.09701254962669163,\n",
       "  'tfidf_char_wb_13908': 0.13959532108960757,\n",
       "  'tfidf_char_wb_13907': 0.13925158619317984,\n",
       "  'tfidf_char_wb_13897': 0.12876553109899327,\n",
       "  'tfidf_char_wb_13563': 0.10797356309660885,\n",
       "  'tfidf_char_wb_13560': 0.07970152248844038,\n",
       "  'tfidf_char_wb_13550': 0.0779936449209551,\n",
       "  'tfidf_char_wb_13529': 0.10862768416944552,\n",
       "  'tfidf_char_wb_13528': 0.0747407017201631,\n",
       "  'tfidf_char_wb_12382': 0.1160672623270892,\n",
       "  'tfidf_char_wb_12381': 0.11601143548077929,\n",
       "  'tfidf_char_wb_12374': 0.11300587048759829,\n",
       "  'tfidf_char_wb_10497': 0.11000108914199194,\n",
       "  'tfidf_char_wb_10496': 0.10894277334531166,\n",
       "  'tfidf_char_wb_10208': 0.11617936765389829,\n",
       "  'tfidf_char_wb_10207': 0.11595575810411621,\n",
       "  'tfidf_char_wb_10201': 0.10633527631539716,\n",
       "  'tfidf_char_wb_9301': 0.056431717648276765,\n",
       "  'tfidf_char_wb_9300': 0.056289249716575375,\n",
       "  'tfidf_char_wb_9299': 0.05625142253422849,\n",
       "  'tfidf_char_wb_8954': 0.07844652622874428,\n",
       "  'tfidf_char_wb_5346': 0.13426406479971106,\n",
       "  'tfidf_char_wb_5345': 0.12292017878703317,\n",
       "  'tfidf_char_wb_4356': 0.09504503345610572,\n",
       "  'tfidf_char_wb_4355': 0.09246495215598496,\n",
       "  'tfidf_char_wb_4095': 0.0572877800176908,\n",
       "  'tfidf_char_wb_4093': 0.05725800837658405,\n",
       "  'tfidf_char_wb_4087': 0.05575045216725723,\n",
       "  'tfidf_char_wb_3454': 0.11757400104015088,\n",
       "  'tfidf_char_wb_3453': 0.11745414198925447,\n",
       "  'tfidf_char_wb_3446': 0.10389370192536423,\n",
       "  'tfidf_char_wb_2729': 0.1350940428704851,\n",
       "  'tfidf_char_wb_2726': 0.11059407601714118,\n",
       "  'tfidf_char_wb_2695': 0.08522864107234744,\n",
       "  'tfidf_char_wb_2217': 0.14178634440348448,\n",
       "  'tfidf_char_wb_2213': 0.13139583063218785,\n",
       "  'tfidf_char_wb_2181': 0.08029346438238151,\n",
       "  'tfidf_char_wb_2069': 0.08361786295359491,\n",
       "  'tfidf_char_wb_2064': 0.08210108895030588,\n",
       "  'tfidf_char_wb_2055': 0.11481970128385362,\n",
       "  'tfidf_char_wb_2054': 0.09550340667202241,\n",
       "  'tfidf_char_wb_2042': 0.11864102301832148,\n",
       "  'tfidf_char_wb_744': 0.11646230382470749,\n",
       "  'tfidf_char_wb_741': 0.11107215426201897,\n",
       "  'tfidf_char_wb_737': 0.09955434852081008,\n",
       "  'tfidf_char_wb_192': 0.08145621595165378,\n",
       "  'tfidf_char_wb_191': 0.05225001039656054,\n",
       "  'tfidf_lemma_9879': 0.35587038627717765,\n",
       "  'tfidf_lemma_5757': 0.38603174568435217,\n",
       "  'tfidf_lemma_5510': 0.38927968141750585,\n",
       "  'tfidf_lemma_4968': 0.2810755367320129,\n",
       "  'tfidf_lemma_4681': 0.17702323676627385,\n",
       "  'tfidf_lemma_3662': 0.3529193542855242,\n",
       "  'tfidf_lemma_2388': 0.35937369304462763,\n",
       "  'tfidf_lemma_1118': 0.3843371231119005,\n",
       "  'tfidf_lemma_271': 0.24706246040654178},\n",
       " {'is_question': 0.0,\n",
       "  'par_idx_in_doc': 27.0,\n",
       "  'sent_len': 14.0,\n",
       "  'is_client': 1,\n",
       "  'sent_idx_in_par': 2,\n",
       "  'sent_pos_in_par': 1.0,\n",
       "  'sent_pos_in_doc': 0.1314553990610328,\n",
       "  'TOKEN': 4,\n",
       "  'POSTAG_AT': 0.25,\n",
       "  'POSTAG_NN': 0.25,\n",
       "  'POSTAG_PRP': 0.25,\n",
       "  'POSTAG_REL': 0.25,\n",
       "  'POSTAG_VB': 0.25,\n",
       "  'f_gen_M': 0.75,\n",
       "  'f_num_P': 0.5,\n",
       "  'f_num_S': 0.25,\n",
       "  'f_per_3': 0.5,\n",
       "  'f_tense_PAST': 0.25,\n",
       "  'sent_idx': 55,\n",
       "  '-1:is_question': 0.0,\n",
       "  '-1:par_idx_in_doc': 27.0,\n",
       "  '-1:sent_len': 56.0,\n",
       "  '-1:is_client': 1,\n",
       "  '-1:sent_idx_in_par': 1,\n",
       "  '-1:sent_pos_in_par': 0.6666666666666666,\n",
       "  '-1:sent_pos_in_doc': 0.1291079812206572,\n",
       "  '-1:TOKEN': 12,\n",
       "  '-1:POSTAG_BN': 0.0833333333333333,\n",
       "  '-1:POSTAG_CC': 0.1666666666666666,\n",
       "  '-1:POSTAG_COP': 0.1666666666666666,\n",
       "  '-1:POSTAG_DEF': 0.1666666666666666,\n",
       "  '-1:POSTAG_NN': 0.1666666666666666,\n",
       "  '-1:POSTAG_QW': 0.1666666666666666,\n",
       "  '-1:POSTAG_RB': 0.25,\n",
       "  '-1:f_gen_F': 0.25,\n",
       "  '-1:f_gen_M': 0.1666666666666666,\n",
       "  '-1:f_num_P': 0.1666666666666666,\n",
       "  '-1:f_num_S': 0.25,\n",
       "  '-1:f_per_3': 0.1666666666666666,\n",
       "  '-1:f_per_A': 0.0833333333333333,\n",
       "  '-1:f_tense_PAST': 0.0833333333333333,\n",
       "  '+1:is_question': 0.0,\n",
       "  '+1:par_idx_in_doc': 28.0,\n",
       "  '+1:sent_len': 15.0,\n",
       "  '+1:is_client': 0,\n",
       "  '+1:sent_idx_in_par': 0,\n",
       "  '+1:sent_pos_in_par': 1.0,\n",
       "  '+1:sent_pos_in_doc': 0.1338028169014084,\n",
       "  '+1:TOKEN': 3,\n",
       "  '+1:POSTAG_CC': 0.3333333333333333,\n",
       "  '+1:POSTAG_CONJ': 0.3333333333333333,\n",
       "  '+1:POSTAG_PRP': 0.3333333333333333,\n",
       "  '+1:POSTAG_RB': 0.3333333333333333,\n",
       "  '+1:f_gen_M': 0.3333333333333333,\n",
       "  '+1:f_num_P': 0.3333333333333333,\n",
       "  '+1:f_per_1': 0.3333333333333333,\n",
       "  '-2:is_question': 0.0,\n",
       "  '-2:par_idx_in_doc': 27.0,\n",
       "  '-2:sent_len': 33.0,\n",
       "  '-2:is_client': 1,\n",
       "  '-2:sent_idx_in_par': 0,\n",
       "  '-2:sent_pos_in_par': 0.3333333333333333,\n",
       "  '-2:sent_pos_in_doc': 0.1267605633802817,\n",
       "  '-2:TOKEN': 8,\n",
       "  '-2:POSTAG_BN': 0.125,\n",
       "  '-2:POSTAG_CC': 0.25,\n",
       "  '-2:POSTAG_PRP': 0.125,\n",
       "  '-2:POSTAG_RB': 0.5,\n",
       "  '-2:f_gen_M': 0.25,\n",
       "  '-2:f_num_S': 0.25,\n",
       "  '-2:f_per_1': 0.125,\n",
       "  '-2:f_per_A': 0.125,\n",
       "  '+2:is_question': 0.0,\n",
       "  '+2:par_idx_in_doc': 29.0,\n",
       "  '+2:sent_len': 129.0,\n",
       "  '+2:is_client': 1,\n",
       "  '+2:sent_idx_in_par': 0,\n",
       "  '+2:sent_pos_in_par': 0.5,\n",
       "  '+2:sent_pos_in_doc': 0.136150234741784,\n",
       "  '+2:TOKEN': 28,\n",
       "  '+2:POSTAG_AT': 0.0357142857142857,\n",
       "  '+2:POSTAG_BN': 0.0357142857142857,\n",
       "  '+2:POSTAG_CC': 0.1428571428571428,\n",
       "  '+2:POSTAG_COP': 0.0357142857142857,\n",
       "  '+2:POSTAG_DEF': 0.1428571428571428,\n",
       "  '+2:POSTAG_EX': 0.0357142857142857,\n",
       "  '+2:POSTAG_IN': 0.1071428571428571,\n",
       "  '+2:POSTAG_JJ': 0.0714285714285714,\n",
       "  '+2:POSTAG_NN': 0.0714285714285714,\n",
       "  '+2:POSTAG_NNP': 0.0714285714285714,\n",
       "  '+2:POSTAG_NNT': 0.0357142857142857,\n",
       "  '+2:POSTAG_PREPOSITION': 0.0357142857142857,\n",
       "  '+2:POSTAG_PRP': 0.1428571428571428,\n",
       "  '+2:POSTAG_RB': 0.1785714285714285,\n",
       "  '+2:POSTAG_REL': 0.1071428571428571,\n",
       "  '+2:POSTAG_VB': 0.0357142857142857,\n",
       "  '+2:f_gen_F': 0.0357142857142857,\n",
       "  '+2:f_gen_M': 0.4642857142857143,\n",
       "  '+2:f_num_P': 0.0357142857142857,\n",
       "  '+2:f_num_S': 0.4642857142857143,\n",
       "  '+2:f_per_1': 0.0714285714285714,\n",
       "  '+2:f_per_3': 0.1428571428571428,\n",
       "  '+2:f_per_A': 0.0357142857142857,\n",
       "  '+2:f_tense_PAST': 0.0357142857142857,\n",
       "  '-1.sim': 0.5365377910415532,\n",
       "  '+1.sim': 0.2315473318283262,\n",
       "  '-2.sim': 0.488730448331177,\n",
       "  '+2.sim': 0.5503591328608799,\n",
       "  'tfidf_word_23018': 0.8685959097051716,\n",
       "  'tfidf_word_11147': 0.17487384054508914,\n",
       "  'tfidf_word_1326': 0.21589916012412488,\n",
       "  'tfidf_word_913': 0.41030213037974034,\n",
       "  'tfidf_char_wb_31265': 0.36084609081449803,\n",
       "  'tfidf_char_wb_31259': 0.3080940665027793,\n",
       "  'tfidf_char_wb_27695': 0.32986104871908145,\n",
       "  'tfidf_char_wb_27694': 0.26944176418522525,\n",
       "  'tfidf_char_wb_22192': 0.10187887274074094,\n",
       "  'tfidf_char_wb_17565': 0.074227014501217,\n",
       "  'tfidf_char_wb_14309': 0.3197118980591895,\n",
       "  'tfidf_char_wb_14304': 0.28052469687916287,\n",
       "  'tfidf_char_wb_9951': 0.09929738741772733,\n",
       "  'tfidf_char_wb_9501': 0.20444578300432006,\n",
       "  'tfidf_char_wb_7728': 0.36407855742996986,\n",
       "  'tfidf_char_wb_7725': 0.30981280562061647,\n",
       "  'tfidf_char_wb_7663': 0.13076268499003776,\n",
       "  'tfidf_char_wb_3437': 0.08842635051948133,\n",
       "  'tfidf_char_wb_3436': 0.0870509295507272,\n",
       "  'tfidf_char_wb_588': 0.1085114016424572,\n",
       "  'tfidf_char_wb_587': 0.0973010838740888,\n",
       "  'tfidf_char_wb_390': 0.2062291887071737,\n",
       "  'tfidf_char_wb_389': 0.14212080971112684,\n",
       "  'tfidf_lemma_3560': 0.20326500001482353,\n",
       "  'tfidf_lemma_2653': 0.7699060286040647,\n",
       "  'tfidf_lemma_1016': 0.2516563733870334,\n",
       "  'tfidf_lemma_740': 0.5500882807529869}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(common_utils)\n",
    "common_utils.get_random_sample(docs_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76a9689a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['אז היום אנחנו נפגשים פה ב18 אבל שבוע הבא בחמישי',\n",
       " 'כן',\n",
       " 'אוקיי מה שלומך?',\n",
       " 'ברוך השם',\n",
       " 'מחר יש לכם יום טיול',\n",
       " 'כן',\n",
       " 'מה אתם עושים?',\n",
       " 'אה נוסעים ליערות הכרמל',\n",
       " 'שזה יום כזה',\n",
       " 'אה',\n",
       " 'כן היום זה עובד ככה שפונים לחברות שעושות את כל הדברים האלה כבר',\n",
       " 'מארגנות הכל אז יש שם פעילויות יש יום',\n",
       " 'כן זה מן מלון ספא כזה נכון?',\n",
       " 'משהו כזה',\n",
       " 'לא ליערות הכרמל כן יש שם גם את זה אבל אנחנו נוסעים אה רגע יכול להיות שלא קוראים לזה יערות הכרמל?',\n",
       " 'כן איפה שהיה השריפה',\n",
       " 'כן',\n",
       " 'אבל שמה יש איזה יום שאנחנו נעשה פעילויות וזה',\n",
       " 'לא במלון',\n",
       " 'אה גם למלון אני חושב קוראים יערות הכרמל חשבתי לזה התכוונת',\n",
       " 'כן וזה בא בדיוק בתקופה',\n",
       " 'אנחנו עכשיו בחברה בגלל שיש את כל הצמצומים אז חלק מהחברה החברה הרי עברו עם משרדים וחלק השכירו לחברה אחרת והתכנון היה שבעתיד אנחנו נגדל ככה שנגיד לחברה הזאת ביי ביי ונכנס לשטח הזה',\n",
       " 'מה שקרה זה שהגלגל התהפך והחברה במצב לא משהו אז והם גדלו החברה השנייה אז איפה שאני ישבתי המחלקה שלי אנחנו בעצם סוגרים אותה ונותנים את השטח הזה משכירים את השטח הזה לחברה השנייה בעצם כדי לחסוך כסף',\n",
       " 'ואותנו בתוך החברה באמת לא היה צריך את כל השטח הזה אז מעבירים את כל החברה מקומות',\n",
       " 'אז יש המון בלאגן עכשיו בחברה אז היום טיול הזה יצא בדיוק בזמן הנכון',\n",
       " 'למה?',\n",
       " 'כי לא נהיה שם בעצם כשכל העובדים יסתובבו שם ויסדרו את הכל',\n",
       " 'אה זאת אומרת יש עובדים שנשארים ומסדרים הכל?',\n",
       " 'יש קבלים כאלה כן שבאו לשים קירות גבס וכל מיני דברים כאלה',\n",
       " 'הבנתי',\n",
       " 'כן יום ראשון אנחנו עומדים כבר להתחיל',\n",
       " 'אני כאילו התפזרנו כבר השבוע בכל מיני מקומות מפוזרים במשרד והמחלקה שלי ושבוע הבא כבר כולם עוברים',\n",
       " 'מה המשמעות של זה בשבילך?',\n",
       " 'אף אחד לא סבבה עם זה',\n",
       " 'כאילו כולם זה מבאס את כולם',\n",
       " 'כל אחד התרגל כבר למקום שלו את הנישה שלו ואת הכל',\n",
       " 'וכל החברה עוברת אין אף אחד שנשאר חוץ מהמנהלים אבל אין אף אחד שנשאר במקום שלו',\n",
       " 'אז יש כאלה שעוברים למקומות טיפה יותר טובים יש כאלה למקומות פחות טובים',\n",
       " 'אנחנו הצוות שלי שהוא אנחנו 4 אנשים אז אנחנו עוברים ליחסית מקום מבודד מכל השאר',\n",
       " 'כאילו לא מבודד אתה יודע זה מרחק של מטרים זה לא איזה שני מטר שלוש מטר',\n",
       " 'אבל זה לא עם כולם',\n",
       " 'אני אוהב תמיד לשבת עם כולם אני אוהב להיות עם כולם',\n",
       " 'באמצע שאם יש צחוקים אני אוהב להיות במרכז העניינים לדעת מה קורה ופה אני קצת דחקו אותנו אחורה ואותי בפרט כאילו אני גם יושב רחוק מכולם אבל אה בסדר',\n",
       " 'אף פעם לא היה לי בעיות עם מקומות',\n",
       " 'כאילו זה עברתי הרבה פעמים מקומות בחברה אף פעם לא התלוננתי',\n",
       " 'כאילו מתרגלים כל מקום מתרגלים',\n",
       " 'אני יותר  אוהב עם כולם',\n",
       " 'אבל בסדר מקווה שיהיה בסדר',\n",
       " 'בגלל בחיים עברת הרבה מקומות',\n",
       " 'אה כן לא יודע אם זה הרבה',\n",
       " 'יש כאלה שבטח עוברים יותר ממני',\n",
       " 'כאילו עברתי כמה מוסדות אבל',\n",
       " 'כן',\n",
       " 'כן',\n",
       " 'אצלך אני לא יודע מה לא עשיתי מחקר אצל אחרים',\n",
       " 'אבל אני יודע שאצלך המעברים היו משמעותיים',\n",
       " 'זאת אומרת זה לא רק לעבור',\n",
       " ' אף מעבר לא היה קל',\n",
       " 'כן',\n",
       " 'אבל אני  אבל כל מעבר זה היה להכיר אנשים חדשים לעשות דברים שונים',\n",
       " 'ופה המעבר הוא בתוך החברה',\n",
       " 'את האנשים אני מכיר את החברה אני אוהב',\n",
       " 'המקום הוא טוב לי',\n",
       " 'אבל לעבור בפנים אז בסדר',\n",
       " 'י XXX תספר לי קצת על המשפחה על ההורים על האחים',\n",
       " 'אממ ההורים שלי  הם כבר כאילו אבא שלי כבר קרוב לפנסיה יחסית כבר 60 ומשו הוא עובד בנק כבר 30 שנה',\n",
       " 'הוא היה בעבר יועץ השקעות והיום הוא לא המשיך לכיוון הזה הוא עובד בנק בבני ברק',\n",
       " 'הוא בנקאי',\n",
       " 'הוא בנקאי היום ההגדרה שלו זה פקיד כללי בכיר',\n",
       " 'הוא כבר לא יועץ השקעות בטייטל אבל הוא',\n",
       " 'אוקיי',\n",
       " 'והוא איש עשייה בוגר ישיבה היה בישיבת הסדר',\n",
       " 'הוא אדם מן השורה',\n",
       " 'אדם הכי רגיל נורמטיבי אדם טוב',\n",
       " 'זה אין לי ספק',\n",
       " 'מה זה אומר רגיל נורמטיבי?',\n",
       " 'זה אומר שהוא לא איזשהו לא יודע מנכל אה זה הוא לא איזושהי דמות תורנית',\n",
       " 'בן אדם שהולך לשיעורים לתפילות',\n",
       " 'יהודי מהסיפורים',\n",
       " 'יהודי טוב',\n",
       " 'יהודי טוב כזה כן',\n",
       " 'אמא שלי היא  ילידת ארגנטינה עלתה לארץ בגיל 10 כזה',\n",
       " 'היא בכורה במשפחה שלה הם מאוד בקשר בתוך המשפחה של אמא שלי הם מאוד בקשר כל הזמן טלפונים עם סבתא שלי כי כאילו',\n",
       " 'הם שם בארגנטינה?',\n",
       " 'לא כולם בארץ',\n",
       " 'כולם פה',\n",
       " 'אבל כל הזמן הם בטלפון כל הזמן הם מדברים אחד עם השני ובקשר',\n",
       " 'ומאוד כזה ארגנטינאים כמו אופרת סבון היא אמרה ככה הוא אמר ככה',\n",
       " 'אמממ מעליי שלוש אחיות נשואות אממ כאילו שהקטנה היא גדולה ממני בשלוש שנים משהו כזה ככה בהפרשים של שנה שנתיים',\n",
       " 'ומתחתיי יש את אחי הצעיר',\n",
       " 'בהפרש יחסית יותר גדול 8 שנים הפרש',\n",
       " 'הוא בן 20',\n",
       " 'הוא בן 20 כן',\n",
       " 'עכשיו הוא בן 20 הוא לומד בישיבה בירושלים',\n",
       " 'בית וגן הוא  הוא לא הלך כאילו הוא לא הלך למסלול שאני הלכתי לישיבה תיכונית שם כי ההורים שלי אצלו הם ידעו שהוא לא מתאים למשטר ובכלל לימודים',\n",
       " 'הוא יותר טיפוס הוא יותר חברמן כזה יותר אהב כאילו הוא תמיד התגאה בזה שהוא בהקבצה ב בחשבון',\n",
       " 'הוא לא אהב לימודים',\n",
       " 'אז איפה הוא כן היה?',\n",
       " 'הוא למד בישיבה קטנה תורנית',\n",
       " 'כאילו ישיבה חרדית באלעד',\n",
       " 'כשמחשבה הייתה ששם יש פחות דרישות לימודיות',\n",
       " 'שלא היה שם אין לימודי חול',\n",
       " 'רק לימודי קודש',\n",
       " 'אבל בשביל הלימודי קודש צריך אה',\n",
       " 'נכון וההורים שלי באמת זה לא היה קל איתו כי הם לא ידעו הוא לא מצא את עצמו שם',\n",
       " 'היה לו נורא קשה',\n",
       " 'גם בלימודים ביסודי היה לו נורא קשה וההורים שלי התרוצצו איתו בכל מיני רופאים ומרפאים ואלרנטיבי וחיפשו את זה',\n",
       " 'עד השנים האחרונות שהוא הלך לאבחון ובעצם הבינו שיש לו בעיות קשב וריכוז',\n",
       " 'ונתנו לו ריטלין ומאז פתאום הוא התחיל לפרוח',\n",
       " 'זהו היום הוא לומד בישיבה גדולה בירושלים מגיע פעם בשבוע שבועיים',\n",
       " 'הוא די מפונק הוא הילד האחרון בבית הוא מאוד קשור לאמא שלי אז  אז הם מאוד בקשר טוב',\n",
       " 'הוא מאוד מפונק בהגדרה',\n",
       " 'אני הייתי כזה 8 שנים',\n",
       " 'בן זקונים עד שהוא תפס את המקום',\n",
       " 'ואיך הקשר עם האחיות הגדולות?',\n",
       " 'ביום יום אין לי קשר איתן',\n",
       " 'כאילו אנחנו נפגשים באירועים ואני אומר שלום והכל אבל לא  פעם הן גם היו מגיעות יותר לשבתות',\n",
       " 'השתיים הגדולות כמעט ולא מגיעות לשבתות כי יש להן לכל אחת 6 ילדים אז לעיתים בחגים הן באות',\n",
       " 'האחות הקטנה מבין שלושתן היא היא מגיעה בשבת בלילה היא גרה קרוב אלינו אז היא באה לאכול אצלנו',\n",
       " 'הגדולה תמיד אמרו שאני הכי דומה להכי גדולה ית קוראים לה',\n",
       " 'אממ  ובסגנון וזה',\n",
       " 'גם במראה כאילו',\n",
       " 'גם במראה וגם ב תמיד אמרו שאנחנו דומים',\n",
       " 'השנייה היא יותר צדיקה היא גרה באלעד בעלה אברך',\n",
       " 'משפחה יותר הם יותר דוסים',\n",
       " 'והשלישית היא רגילה בתור ילד הייתי מציק לה הרבה היום בסדר',\n",
       " 'אנחנו יחסים אה רגילים',\n",
       " ' יש לכם הרבה רגיל במשפחה אני שומע',\n",
       " 'כן אין איתם זה לא קשר כמו שיש עם חברים אני לא משתף אותם בדברים',\n",
       " 'אני לא  חולק איתם או משהו',\n",
       " 'אני מדבר איתם רגיל להגיד שלום וזה אבל לא מעבר',\n",
       " 'אין לי',\n",
       " 'כן וככה הן גם אחת עם השנייה?',\n",
       " 'אממ כאילו ית הגדולה והשלישית הן דווקא בקשר טוב',\n",
       " 'אבל זו שיותר באמצע  יש להם קבוצת וואצאפ של המשפחה וזה היא לא בקבוצה אין לה וואצאפ היא דוסית יותר',\n",
       " 'אז פחות בקשר איתן',\n",
       " 'אבל בסדר כשהן נפגשות בן ביחד וכן',\n",
       " 'כל אחת יש לה את המאפיינים שלה',\n",
       " 'כאילו הגדולה נגיד היא אחת כזאת שהכל צריכה שעכשיו יהיה והכל היא מאוד עכשיו שאנרגטית והכל מה שהיא רוצה צריך לקרות עכשיו',\n",
       " 'זאת שאמרת שאומרים שאתם דומים',\n",
       " 'כן לא בזה אבל אנחנו דומים',\n",
       " 'אנחנו דומים במראה יותר',\n",
       " 'כאילו היא ואבא שלי ואני בעצם דומים',\n",
       " 'בצד של אבא שלי ובצד של אמא שלי יש את שאר המשפחה',\n",
       " 'אתה מדבר על המראה או',\n",
       " 'יותר במראה כן האופי לא אני לא כזה זה לא שאני צריך עכשיו זה יותר מראה האמת השנייה אז היא יש לה יש לה תמיד היא לה אישיוז מאז שהיא ילדה היא הייתה קונה דברים ומיד הייתה מרגישה שהיא צריכה להחליף מאז שהיא ילדה תמיד היא חושבת שדיברו עליה מאחורי הגב ותמיד היא  היא אוכלת סרטים כזה זאת השנייה איתה יש לי הכי מבין כולם אם הייתי צריך להגיד עם מי יש לי הכי פחות קשר בשנייה כי גם לא יוצא לי לדבר איתה וגם כשאני מדבר איתה אין כל כך מה',\n",
       " 'והשלישית זה האמת שרוב הילדות הייתי מציק לה זה היה ממש אה  לא נעים',\n",
       " 'כן כי אני הכי לא כזה היום',\n",
       " 'בתור ילד הייתי ממש כזה',\n",
       " 'אני לא יכול לחזור אחורה בזמן אבל אז הייתי ממש נוראי',\n",
       " ' פעם יצא לכם לדבר על זה?',\n",
       " 'בבגרות?',\n",
       " 'לא אף פעם אבל זה תמיד אני קשה לי לדבר איתה',\n",
       " 'היום  שיחה פתוחה',\n",
       " 'כאילו גם כשאני מדבר איתה אני מרגיש איזשהו קרירות כזה',\n",
       " 'אני לא מדבר איתה מעבר לשתי מילים',\n",
       " 'קרירות ממנה?',\n",
       " 'אממ  גם כן כאילו  אממ כן הדדי כזה אני חושב',\n",
       " 'אתה חושב שזה קשור למה שהיה בילדות?',\n",
       " 'אממ  אני לא יודע אני תמיד חושב שבטח היא זוכרת כאילו היא גדלה ככה',\n",
       " 'נשמע שאתה זוכר מאוד',\n",
       " 'אני זוכר מאוד אני זוכר מאוד כי גם ההורים שלי תמיד היו כאילו מתעצבנים ואבא שלי היה הולך לשיחות בבית ספר עם המורה מדי פעם היו עושים כאלה שיחות אה זה והוא היה הולך הוא היה תמיד אומר לי אני אדבר עם המורה על זה שאתה מציק',\n",
       " 'כאילו פחדתי מזה מאיך ה מאיך הוא יגיב',\n",
       " 'אממ  וזהו היום אנחנו מדברים רגיל',\n",
       " 'כאילו אני הולך אליה לבייביסיטר',\n",
       " ' אנחנו',\n",
       " 'אה אתה עושה עליהם?',\n",
       " 'אני עושה על הילדים בייביסיטר יש לה ילדים מאוד חמודים',\n",
       " 'רגע הקטנה זאת שגרה לידכם',\n",
       " 'כן',\n",
       " 'כן',\n",
       " 'לשם אני הולך לפעמים לבייביסיטר הם באים אלינו שבתות בלילה יש לה את הילדים הכי חמודים ואני מת עליהם והכל',\n",
       " 'וגיסי גיסי בסדר',\n",
       " 'כאילו איתו אני מדבר יותר',\n",
       " 'היא יש לי תמיד אני מרגיש כאילו זה מישהו שעשיתי לו XXX בעבר והיום אני  אני חי איתו כאילו זה אחותי',\n",
       " 'אבל זה לא נעים',\n",
       " 'אני לא פותח את זה',\n",
       " 'כאילו אף פעם לא דיברנו על זה',\n",
       " 'מה אתה חושב שיקרה אם תפתח את זה?',\n",
       " 'מה חוץ מלהאשים אותי אין',\n",
       " 'שהיא תאשים אותך?',\n",
       " 'כן אין לי איך להתגונן',\n",
       " 'אין לי תירוץ אני לא יכול להגיד עשיתי את זה כי כי זה לא היה בלי סיבה',\n",
       " 'זה הכי נוראי',\n",
       " 'אוקיי',\n",
       " 'בסדר הייתי ילד כאילו',\n",
       " 'כן אני חושב שאפשר לפתוח דברים גם אם אין סיבה',\n",
       " 'כאילו אפילו להגיד סליחה אני הייתי ילד אבל עכשיו אני מרגיש אשם זה לא היה יפה',\n",
       " 'כן אבל אה',\n",
       " 'לא אני מבין שזה קשה אני לא אומר לך לך תעשה את זה מחר',\n",
       " 'לא ברור זה משהו שאני יודע שהוא שם כאילו',\n",
       " 'לא מעיר אותו',\n",
       " 'כן אתה יש לך מחשבות למה הצקת לה?',\n",
       " 'אין לי שום הסבר',\n",
       " 'ואיך ההורים הגיבו לדבר הזה?',\n",
       " 'צועקים עליי מרביצים וזה',\n",
       " 'אבל זה לא  כאילו בצדק',\n",
       " 'הייתי עושה אותו דבר',\n",
       " 'אין לי שום הסבר',\n",
       " 'בן כמה היית?',\n",
       " 'באיזה גיל אנחנו מדברים?',\n",
       " 'באני רואה את זה כאילו זה היה כל הילדות',\n",
       " 'ילדות זה',\n",
       " ' גם בית ספר גם בית ספר ג ד ה ו ',\n",
       " 'כן אני רואה גם שעכשיו כשאתה מדבר זה זה נוגע בך',\n",
       " ' כי זה אישיו שאני לא יכול זה לא שאני יכול ללחוץ על הכפתור ולמחוק',\n",
       " 'כאילו מה אתה חושב היום?',\n",
       " 'אתה מרגיש שזה  כאילו מאיך שאתה מספר זה נשמע שאתה היום כבר 15 שנה מאז אפילו אולי יותר אבל אתה מרגיש שזה חלק ממך שאתה אשם במשהו',\n",
       " 'אה כן אני חושב שזה בצדק כי כאילו זה לא שאני אין לי שום זה הגיע למצב כזה שהייתי כל כך',\n",
       " ' נאי זוכר סתם איזו סיטואציה שישבנו מול הטלוויזיה אני ואחותי ואני זוכר שצבטתי אותה',\n",
       " 'ואז התברר לי שזאת לא אחותי הזאתי זאת אחותי אחת מעליה והיא התחילה לצעוק עליי למה צבטת אותי וזה ואני הייתי כל כך נבוך כי הייתי בטוח שזאת האחות שאני רגיל לעשות לה את זה',\n",
       " 'בלי שום סיבה',\n",
       " 'אין לי שום משהו שאני יכול להסביר את זה',\n",
       " 'ואני לא אלים',\n",
       " 'כאילו אני הכי',\n",
       " 'רואים רואים דווקא בגלל זה אני חושב שהייתה לך סיבה',\n",
       " 'אני לא יודע מה הסיבה אבל הייתה לך סיבה',\n",
       " 'אני לא  כאילו ברור שפסיכולוגים כאילו מישהו שמבין ילדים וזה היה אומר או שהוא מקנא או ש כמו סופר נני כזה',\n",
       " 'אבל אני באמת לא יודע להגיד כאילו אין לי משהו שאני יכול להצביע שקינאתי בה בגללו',\n",
       " 'לא יודע או שאולי כן אני רק לא יודע מה',\n",
       " 'טוב',\n",
       " 'וזהו יש את אחי מתחתיי',\n",
       " 'שאיתו אני יחסית כאילו כשהוא מגיע שבתות אנחנו לפעמים ביחד',\n",
       " 'שבתות כשהוא בבית',\n",
       " 'כן וכשהייתם ילדים ההורים איזה סוג הורים הם היו?',\n",
       " 'אבא אמא',\n",
       " 'הם הורים מאוד דואגים לילדים שלהם',\n",
       " 'כאילו אמא שלי תמיד כל יום בבית ספר אחרי שחוזרים איך היה היום?',\n",
       " 'מה למדת ?',\n",
       " 'מה אכלת?',\n",
       " 'עד היום',\n",
       " 'יש שאלה של איך עבר היום מה אכלת צהריים',\n",
       " 'כל יום היום זה כבר אני מרגיש קצת שזה אני בן 28 זה לא משהו שאמורים לעשות',\n",
       " 'מה היית עונה לה אז?',\n",
       " 'היית חוזר מה נשמע?',\n",
       " 'איך עבר היום?',\n",
       " 'מה אכלת?',\n",
       " 'אז?',\n",
       " 'הייתי מפרט היום אני עונה באדישות במטרה שאולי היא תפסיק באיזשהו שלב',\n",
       " 'אבל בסדר גם היום אני לא ככ נעים לי כי אני היום אוכל כאילו אני כבר לא ככ מקפיד על הכשרויות כאילו אני מקפיד על כשרות רבנות רגילה',\n",
       " 'כן',\n",
       " 'וההורים שלי לא הם יותר שומרים',\n",
       " 'אז לא נעים לי לספר לה שאני לא מקפיד',\n",
       " 'כי לא אז תמיד אני אומר שאני אוכל באותו מקום',\n",
       " 'כן ואכלת פסטה ואכלת סלט',\n",
       " 'כאילו אכלת באיזה מקום שהיא יודעת שהוא כשר',\n",
       " 'שהיא יודעת שכן כן אין טעם להיכנס לזה',\n",
       " 'אבל היא בתור מאז שהיינו ילדים אני זוכר כל יום איך היה היום מה היה?',\n",
       " 'אבא שלי פחות זה פחות מעניין אותו',\n",
       " 'הוא אוהב לשמוע חדשות אוהב אוהב לקרוא המון יודע המון',\n",
       " 'הוא פחות יורד לפרטים של אה ',\n",
       " 'אבל הייתה לכם אינטרקציה כשהיית ילד?',\n",
       " 'היינו לומדים ביחד היינו יוצאים לים ביום שישי',\n",
       " 'היינו כן לא צמודה כמו עם אמא',\n",
       " 'היא יותר דומיננטית',\n",
       " 'כן איך היה לצאת איתו לים?',\n",
       " 'היה נחמד',\n",
       " 'רק אתה?',\n",
       " 'או באים כל האחים',\n",
       " 'כש כאילו יש לי את אחי מתחתיי אז כשהוא זה אז גם איתו',\n",
       " 'גם לאבא שלי אין יותר מדי על מה לדבר',\n",
       " 'כאילו אני מראש מניח שדברים שוליים שיש לי לא מעניינים אותו',\n",
       " 'עם אמא שלי יותר נוח לי לדבר',\n",
       " 'היום גם על דברים כאילו אני יכול להגיע הביתה ולהגיד אמא עברנו מקומות כאילו לספר לה על המעבר מקומות במשרד  אבא שלי אני לא אספר לו את זה',\n",
       " 'הוא  זה לא לא מעניין אותו',\n",
       " 'כאילו זה די חולף לו ליד האוזן',\n",
       " 'זה לא משהו שהוא  דברים יותר משמעותיים מבחנים את האירוע של הפיטורים בחברה דברים כאלה כן',\n",
       " 'אבל דברים של היומיום לא כאילו הוא לא אדם שאתה תספר לו והוא יתעניין וישאל',\n",
       " 'כשאני מרגיש בן אדם לא מתעניין ושואל אז אני מרגיש כאילו אין טעם אני לא מוצא טעם לספר',\n",
       " 'התחושה היא ש בחוויה שלך אבא לא תמיד מתעניין או לא מתעניין בהכל',\n",
       " 'אמא לפעמים מתעניינת יותר מדי',\n",
       " 'כן',\n",
       " 'אפילו כשאתה לא רוצה',\n",
       " 'בעבר לא הרגשתי את זה ככה',\n",
       " 'וכשהגעתי לישיבה אז היינו מדברים פעם היום בטלפון וכשעברתי מהישיבה הראשונה בירושלים לקריית ספר אז היה לי נורא קשה המעבר אז הייתי מתקשר הרבה לחברים מהישיבה הקודמת וזה וגם לאמא שלי',\n",
       " 'כאילו זה היה ממלא לי איזשהו כאילו היה לי קשה שם חברתית',\n",
       " 'אממ  אז הייתי מדבר איתה פעם ביום פעמיים ביום',\n",
       " 'אבל שמה דווקא זה היה נראה לי הכי בסדר ולפעמים גם כשראיתי שהיא אומרת לי תמיד טוב יש לי טלפון באמצע כאילו היה לה טלפון בבית או משהו כזה סבתא שלי אז היא הייתה אומרת טוב אז אפשר טוב אני אחזור',\n",
       " 'אז הייתי אומר רגע מה עכשיו טלפון?',\n",
       " 'אני רוצה כאילו שיש לי מישהו שרוצה לשמוע',\n",
       " 'אבל זהו מאז שהתחלתי לעבוד התחלתי כאילו לגלות עולם חדש יותר אז היום קצת מרגיש לי כאילו יותר מדי כאילו היא יותר מדי כאילו הייתי מעדיף שהיא תדע פחות',\n",
       " 'כי מה נניח נתאר את הסיטואציה הקודמת',\n",
       " 'אתה חוזר מהעבודה היום',\n",
       " 'ואז היא שואלת כל מיני שאלות',\n",
       " 'מה אכלת איך היה בעבודה מה עשית איפה היית',\n",
       " 'מה אתה מרגיש שם',\n",
       " 'אני מרגיש כאילו היא מחפשת לבקר אותי',\n",
       " 'כי  היא הרבה פעמים מבקרת אותי',\n",
       " 'אם זה על הלבוש אם זה על ה אם יש איזה אירוע חברה רגע ומה תאכל שם?',\n",
       " ' אז אני ואני יודע שזה מקום לא כשר',\n",
       " 'לא אני אשתה בירה וזה באמת מה שאני אעשה',\n",
       " 'זה מרגיש לי קצת כמו',\n",
       " 'רגע אני גדלתי',\n",
       " 'אני לא אותו ילד שצריך ל  לשאול אותו את כל השאלות האלה ולהכווין אותו',\n",
       " 'אני כבר יודע להחליט בעצמי',\n",
       " ' פעם היה איזה  כשהגעתי לחברה בערך חצי שנה אחרי זה הגיעה איזו מישהי מחול אז עשו לכבודה איזה אירוע בבר בתל אביב',\n",
       " 'אני לא בטוח שסיפרתי את זה',\n",
       " 'ו  שאלו מי מגיע מי מגיע וזה',\n",
       " 'ואז הגיעו אליי עוד הייתי עם כל הלבוש הדוסי הזה ושאלו אותי אם אני רוצה לבוא בחיים לא הייתי בבר בתל אביב',\n",
       " 'תבוא תבוא יהיה נחמד אתה תראה מה זה',\n",
       " 'תראה מה זה',\n",
       " 'ולא אז עוד לא ידעתי מה זה ולא וחשבתי על זה טוב אני אלך לנסות מה יכול להיות?',\n",
       " 'ואז הלכתי ו ואמרתי ההורים שלי שאלו אותי מה זה',\n",
       " 'אמרתי להם שהולכים לאיזו מסעדה בתל אביב ידעתי שזה בר אבל לא רציתי להגיד להם',\n",
       " 'והלכתי לשם וזה היה איזשהו בר למפרע התברר שזה מקום בילוי מאוד נחשב בתל אביב שם ברוטשילד',\n",
       " 'וזהו ואני זוכר שישבנו שם כמה אנשים מהעבודה',\n",
       " 'ולא הבנתי מה',\n",
       " 'כאילו שאלתי אותם כל הזמן מה הקונספט פה',\n",
       " 'כאילו אנחנו יושבים ויש מוזיקה בקולי קולות אני לא שומע אף אחד קשה לי לשמוע מה אומרים',\n",
       " 'אז מה עושים?',\n",
       " 'כאילו גם אין אוכל על השולחן כאילו זה לא שאוכלים או משהו',\n",
       " 'ולא הבנתי מה קורה שם',\n",
       " 'והם אמרו לי בהמשך אתה תראה בהמשך אתה תראה',\n",
       " 'זהו ואז התחילו להגיע עוד אנשים ועוד אנשים',\n",
       " 'באיזשהו שלב התחילו להביא שתייה כאילו זה היה אירוע של החברה היא מימנה את זה',\n",
       " 'כאילו לא כולם הגיעו',\n",
       " 'מקום מאו רועש וצפוף ואיזה חדר קטן וזה',\n",
       " 'והביאו שתייה וזה',\n",
       " 'וכאילו הגעתי לשם אחרי שהייתי אצל מישהו מהעבודה לפני זה ואצלו הוא נתן לי בירה ואז הגעתי לשם ושם ישבתי ליד איזה אחד מהחברה ואז הוא אמר לי אז כבר שתית בירה אז אסור לך לרדת מהרמת אלכוהול הזה אתה חייב לשמור על רמת אלכוהול גבוהה משהו כזה',\n",
       " 'שתיתי שם פעם ראשונה שבעצם השתכרתי ממש',\n",
       " 'כאילו אף פעם לא השתכרתי אני לא אחד ששותה יין',\n",
       " 'לא בפורים לא בשבתות',\n",
       " 'אני בחיים לא הגעתי למצב של שכרות',\n",
       " 'אממ ושם בעצם שתיתי והשתכרתי וכנראה שיותר מדי',\n",
       " 'אז אמרתי כאילו ואני הייתי עם האופנוע אז אמרתי טוב אני אלך לאותו אחד שהייתי מהעבודה מקודם אני אנוח אצלו',\n",
       " 'והלכתי אליו ונרדמתי שם על הספה',\n",
       " 'ולא אמרתי שום דבר להורים שלי עכשיו אני אף פעם לא ישנתי מחוץ לבית עד אז',\n",
       " 'כאילו אני הייתי בישיבה או בפנימייה אבל אף פעם לא יצא שאני ',\n",
       " 'ולמחרת בבוקר אני מתעורר ב8 בבוקר והפלאפון שלי מכובה כי נגמרה הבטרייה',\n",
       " 'ואני מיד מטעין אותו ומדליק',\n",
       " 'אני רואה מלא שיחות שלא נענו',\n",
       " 'אני מתקשר לאמא שלי אמא שלי בוכה בטלפון',\n",
       " 'שהיא הייתה בטוחה שמשהו קרה לי',\n",
       " 'כי הוא לא עונה לטלפון ומהלילה הוא לא הגיע',\n",
       " 'אבא שלי כבר היה בתחנת משטרה רצה לדווח על אירוע',\n",
       " 'אמרו לו שעוד לא עבר 24 שעות אז אין  אז כמובן שהרגשתי הכי נורא שבעולם כאילו אין דבר יותר נורא מזה',\n",
       " 'הרגשתי גם נורא כי היה לי הנגאובר באותו יום למחרת אבל הרגשתי יותר נורא מזה שפגעתי בהורים שלי',\n",
       " 'אמרתי להם שנכבה לי הזה והם אמרו לי אבל למה לא אמרת שום דבר כי לא לא אמרתי להם אמרתי להם שלא הרגשתי כל כך טוב אז הלכתי לנוח אצל חבר',\n",
       " 'לא רציתי לומר להם שהשתכרתי',\n",
       " 'זה היה נוראי',\n",
       " 'זה היה יום נוראי',\n",
       " 'כן זהו אז מאז אני הולך אני הולך לאירועים',\n",
       " 'היום כבר יותר קל להורים שלי להבין כאילו הם פחות  עדיין שואלים איפה איפה זה היה ומה תאכל שם ומה אבל בסדר',\n",
       " 'L1P אני כבר  טוב אבל באירועים של חברה ההורים שלי יש יש טראומה',\n",
       " 'בכל הסיטואציה הזאת שעכשיו סיפרת גם בסיטואציות הקודמות שאמא שואלת שאלתי אותך מקודם מה אתה מרגיש הסברת מאוד בפירוט איזה מחשבות יש לך שם',\n",
       " 'שאני ילד גדול ואני רוצה חופש',\n",
       " 'ואתה יכול לזהות לתת שמות לתחושות שלך במקום הזה?',\n",
       " 'מה אתה מרגיש מה הרגשת באותו בוקר כשהתעוררת וראית מלא שיחות שלא נענו והתקשרת לאמא ואז למה הרגשת נוראי?',\n",
       " 'מה הרגשת?',\n",
       " 'מה זה נוראי?',\n",
       " 'אני מדבר עם אמא שלי בטלפון והיא בוכה בטלפון',\n",
       " 'אני כאילו הכאבתי לה',\n",
       " 'אין  הכאבתי לה הכאבתי לאבא שלי כאילו אבא שלי בתחנת משטרה',\n",
       " 'אני כאילו עשיתי משהו משהו רע בלי להתכוון לעשות אותו',\n",
       " 'אממ  ואני לא יכול לתקן את זה',\n",
       " 'זה מאוד מזכיר מה שדיברנו בהתחלה עם אחותך',\n",
       " 'כן רק שאחותי זה היה מזמן',\n",
       " 'כאילו אז לא חשבתי על זה ככה',\n",
       " 'ואחרי שעבר הזמן אז כאילו פתאום היום אני מבין שהייתי  אבל אצל ההורים שלי זה היה בידיים שלי כאילו יכולתי פשוט להגיד להם משהו',\n",
       " 'כאילו באותו לילה ופשוט לא עשיתי את זה ו זהו השאירה אותי חסר מילים',\n",
       " 'אל תבכי אל תבכי',\n",
       " 'הרגשת המון המון אשמה',\n",
       " 'המון',\n",
       " 'אני רוצה רגע לשאול איזו שאלה מהצד',\n",
       " 'אני גם רואה שעכשיו אתה מוצף וגם  וגם בפגישות הקודמות איך אתה יוצא מפה?',\n",
       " 'אתה נכנס לפה מספר לי כל פעם איזה חלק אחר ואני שואל שאלות',\n",
       " 'וזה מאוד מציף זה מאוד קשה זה מאוד סיפורים שמורכבים לך',\n",
       " 'איך אתה יוצא מפה איך אתה עובר את השבוע?',\n",
       " 'אממ',\n",
       " 'למה אתה בא לפה בכלל עוד פעם?',\n",
       " 'אני בא כי אני חושב ש אני יכול להיות שזה דברים שאני מזניח כאילו',\n",
       " 'האמת שמהפגישה הקודמת אני יצאתי עם הרגשה נוראית',\n",
       " 'כאילו פתאום הבנתי שאני הכי דפוק בעולם',\n",
       " 'וזה כאילו יש דברים שאני יכול להיות שאני מזניח אום ביומיום כאילו לא מחשיב אותם כל כך וברגע שאני מדבר עליהם אז אני שם לב היי יש פה יש פה באמת משהו',\n",
       " 'קודם כל אני לא מדבר כל כך עם אנשים',\n",
       " 'זאת אומרת אני מדבר עם חברים שלי מה שאני מתכוון אני לא מדבר עם אנשים על הדברים האישיים שלי עד כדי כך',\n",
       " 'כן',\n",
       " 'יש לי חברים בעבודה ואני צוחק איתם והכל אנחנו יוצאים לפעמים אבל אין מישהו שאני באמת פותח איתו את הכל אין לי בת זוג כרגע או מישהו כאילו עם חברים מהישיבה אני לא כאילו רובם נשואים אלו שלא זה לא בשביל אהה  וגם קשה לי לדבר עם מישהו שאני לא חושב שזה מעניין אותו שהוא יכול להכיל מישהו שזה לא מעניין אותו ואני בא אליו ואני בעצם מפיל עליו נטל אז אני לא מנסה בכלל אז כל הזמן שאני לא מדבר אז יכול להיות שאני בעצם זה שם זה יושב באיזו פינה כלשהי',\n",
       " ' אז בפגישה הקודמת היציאה אחכ חזרתי אחכ לעבודה זה היה באמצע היום',\n",
       " 'נכון',\n",
       " 'חזרתי אחכ לעבודה והרגשתי היה לי נורא קשה לתפקד כאילו הרגשתי כאילו יש את החזית הזאת ויש גם את החזית הזאת כאילו מתקיפים אותי מכל החזיתות ואני צריך בעצם להתמודד עם הכל',\n",
       " 'אתה מתכוון בעצם לחזית המקצועית ולחזית של החיים ה',\n",
       " ' משפחה זוגיות אז כל הזמן שאני  לא מדבר על זה אז זה שם אבל כאילו אני אני אגיע לזה',\n",
       " 'אם זה לימודים אם זה זה',\n",
       " 'אבל כשאני מדבר על דברים האלה אז פתאום זה יש גם את זה ויש גם את זה ויש גם את זה ',\n",
       " 'אתה חושב שכמטפל יש לי סיבה לדאוג?',\n",
       " 'מאיזו בחינה?',\n",
       " 'שיקרה משהו',\n",
       " 'לא אני לא חושב',\n",
       " 'כאילו אני חושב ש אתה מתכוון לנטילת חיים אני מניח',\n",
       " 'זה מקרה קיצוני אבל  אני שואלת את זה קצת אולי ישירות כי אני רואה שמאוד מציף ואני מעדיף שזה יהיה פה על השולחן ולא בקירות',\n",
       " 'אז מה התחלת לומר שאתה מה?',\n",
       " 'לא אני אומר שאני לא מנסה לברוח מזה',\n",
       " 'כן',\n",
       " 'אני רוצה לפתור את זה',\n",
       " 'קודם כל אני אגיד שזה מאוד מאוד אופייני לשלבים ראשונים של טיפול',\n",
       " 'בגלל זה הרבה אנשים שמגיעים לטפיול נעלמים די מהר כי ההתחלה היא קשה',\n",
       " 'פתאום פותחים ואתה יודע כמו שפותחים ביוב ועתה יודע אתה מסתכל ואומר וואו',\n",
       " 'כמו שאמרת יצאת בתחושה של איזה דפוק אני',\n",
       " 'שאני חושב שהיא לא נכונה התחושה הזו זה שאתה מרגיש את זה זה בסדר אבל היא לא פרופורציונאלית',\n",
       " 'אבל זו התחושה כי באמת זה מאוד מציף נוגעים במקומות שלא נוגעים בהם בדכ והם לא נעימים',\n",
       " 'ואצלך זה גם הכל סביב החוויה שיXXX הוא לא בסדר',\n",
       " 'הוא לא בסדר הוא דפוק',\n",
       " 'או שהוא פוגע באמא או שהוא לא מספיק חרדי טוב או שהוא לא מוצא את עצמו מבחינה מקצועית ואין לו מספיק ניסיון',\n",
       " 'גם מולי אני מניח שאתה מרגיש למרות שלא אמרת',\n",
       " 'אולי שאתה גורם לי אולי הזכרת את זה קצת כשאמרת בהתחלה כשהזכרת שחשבת ללכת למטפלת',\n",
       " 'זא זו החוויה',\n",
       " 'שאתה לא בסדר',\n",
       " 'אממ אני מקווה שעם הזמן תראה שהתמונה יותר מורכבת',\n",
       " 'נראה לי מפחיד יותר מורכב ממה שהיום זה',\n",
       " 'יותר מורכב אני מתכוון שזה לא יי לא בסדר',\n",
       " 'או לא מספיק טוב',\n",
       " 'אני יודע שזה ניתוח די אבסטרקטי',\n",
       " 'זה לא ניתוח זו תחושה',\n",
       " 'אני יודע שאתה יודע שזה יותר מורכב אבל אתה לא מרגיש שזה יותר מורכב',\n",
       " 'ניקח את הדוגמא שדל הסיפור שהיה בבר או מה שהיה אחרי הבר בעיקר יכול להיות שיש אנשים שהיו מספרים את זה ואז אמא הייתה מתקשרת הם היו מתעצבנים עליה מה את רוצה?',\n",
       " 'כולה אני בן 27 26 היית אז נגיד ישנתי מחוץ לבית ואת הולכת למשטרה?',\n",
       " 'את לא יכולה כאילו מה את רוצה ממני??',\n",
       " 'אני בן אדם גדול',\n",
       " 'אז זו למשל ראייה יותר מורכבת',\n",
       " 'זו הראייה שתהיה לי נגיד היום אם יהיה דבר כזה כאילו היום אני במצב של  שאם  הם ממש יקחו עוד זה אני אכעס קצת',\n",
       " 'נגיד כשהיה באותו יום כשהיה הטיפול הקודם אז גררתי בעבודה כל המחלקה הייתה על הרגליים כי הם לא ידעו לאן נעלמתי בבוקר',\n",
       " 'וזה הכעיס אותי',\n",
       " 'אמרתי להם זה כמו ההורים שלי שאני צריך לתת להם דין וחשבון',\n",
       " 'היום זה כבר מכעיס אותי שאני מרגיש שאני צריך לעשות את זה',\n",
       " 'כשבאים אליי בטענה אתה לא בסדר כאילו זאת שיושבת לידי אמרה אתה לא בסדר שלא אמרת',\n",
       " 'אמרתי לה ככה זה ההורים שלי בדיוק',\n",
       " 'אז אני לא צריך לתת דין וחשבון כולה יצאתי לחצי שעה',\n",
       " 'אז באותו זמן אני הרגשתי שממש פגעתי בהורים שלי',\n",
       " 'אני גם חושב שבגלל החשש לפגוע שאולי למדת אותו מאבא ואמא אולי בעיקר מאמא',\n",
       " 'אתה מסתובב עם איזה חשש לפגוע ובגלל זה מלכתחילה כמו שאתה לא מספר לאמא אתה גם לא מספר בעבודה',\n",
       " 'כי בעבודה אני חושב מהסיפורים שלך שאם היית אומר תקשיבו יש לי איזה עניין אישי אני חוזר לחצי שעה היו אומרים לך בסדר',\n",
       " 'ככה נשמע לי מהסיפורים שלך',\n",
       " 'כן היו אומרים אבל אני לא רוצה שזה יגיע למצב שהם יראו שאני יוצא ליותר מדי ויבינו שאני שאני מחפש שיחשדו שאני מחפש יצא לי כמה פעמים בדכ אני כן אומר אבל אמרתי טוב כדי שאני לא אצבור רצף של יציאות ואז יתחילו לחשוד אולי הפעם אני לא אגיד ואז אף אחד לא ישים לב ואז כאילו  חסכתי פעם אחת שאני אומר עליהם',\n",
       " 'אגב בעניין הזה אני דיברתי כאילו היה איזשהו שינוי שהלכתי להיא ממשאבי אנוש ואמרתי לה לא אמרתי לה שאני מחפש אבל אמרתי לה שאני לא מרוצה שרציתי לדבר איתה על העניין ההוא',\n",
       " 'כאילו קבעתי איתה פגישה',\n",
       " 'אמרתי לה שבקשר למה שדיברנו לפני כמה שבועות אני רציתי לדבר איתה שאני לא ככ מרוצה ממה שאני עושה עכשיו',\n",
       " 'זא אני אוהב את המקום אוהב את האנשים אבל אני לא מרגיש שאני מתפתח מקצועית',\n",
       " 'כלומר אני נשארתי בדיוק במה שאני עושה',\n",
       " 'אמרתי לה התפקיד שלי מסתכם בדברים קטנים לא במשהו יותר מדי משמעותי ואני לא לומד מזה',\n",
       " 'אי כן מחפש להתקדם מקצועית',\n",
       " 'אז היא שאלה אותי מה זה שונה ממה שהיה בעבר כאילו ההוא שהיה לפניי בתפקיד הזה',\n",
       " 'אמרתי לה שמאז שפיטרו חלו שינויים בחלוקת תפקידים שאני מתעסק בדברים מאוד מסוימים',\n",
       " 'אני לא מתעסק בדברים מורכבים',\n",
       " 'זה יש מישהו אחר',\n",
       " 'אז אני מרגיש שאני לא לומד',\n",
       " 'אז היא אמרה לי יש מישהו שהיית רוצה להיות?',\n",
       " 'תפקיד שהיית רוצה?',\n",
       " 'אמרתי לה תראי אני יודע שכרגע אני לא במצב שמספיק בשביל תפקיד כזה אבל כן הייתי רוצה בסופו של דבר להגיע להיות מתכנת מנוסה שההוא ששואלים אותו תמיד שאלות',\n",
       " 'אני רוצה להגיע למחלקה של הזה אצלנו',\n",
       " 'יש לי זו השאיפה שלי',\n",
       " 'אבל בשביל להגיע לשם צריך להתקדם אבל זה לא קורה',\n",
       " 'אז אמרתי לה אני לא אומר זה בקטע של תלונה למשל אני אומר את זה כי בואי נחשוב ביחד',\n",
       " 'היא אמרה לי שהיא שמחה לשמוע והיא תנסה לדבר עם הבוס שלי שהיא אמרה לי בוא נתחיל מזה שאתה תדבר עם הבוס שלך',\n",
       " 'אז באמת באותו יום דיברתי עם הבוס שלי',\n",
       " 'ואמרתי לו את אותו דבר',\n",
       " 'והוא אמר לי ש הוא נתן לי איזשהו רעיון איזה פרויקט שהוא היה אולי רוצה שאני אעשה  הוא נתן לי איזשהו רעיון באותו רגע אבל הוא אמר לי שהוא ינסה לדבר עם כל מיני אנשים',\n",
       " 'אממ  אז כרגע אנ מנסה לעשות את הפרויקט הזה שהוא נתן לי זה לא כזה טריוויאלי בשבילי אז אני לומד את זה כרגע',\n",
       " 'זה מעבר למה שאתה עושה עכשיו?',\n",
       " 'זו העבודה כרגע כלומר אין ככ פרויקטים',\n",
       " 'אוקיי',\n",
       " 'אני עושה את זה די במהירות אז יש הרבה זמן פנוי',\n",
       " 'אז בזמן הזה אני לומד כדי לעשות את הפרויקט ההוא',\n",
       " 'מקווה שאני אצליח',\n",
       " 'ומבחינת מה שאמרת לי שבוע שעבר שאתה מרגיש בתפקיד הזה שאתה לא לומד וגם לא צובר נסיון לקראת ואפילו דיברת איתי שאתה רוצה לעשות פרויקט ולהעלות אותו לאתר פרויקט לא התנדבותי אבל זא',\n",
       " 'בכל המקומות האלה זה ממלא את הצורך הפרויקט שהוא הציע לך?',\n",
       " 'זה יכול למלא כן אני רק צריך להתמקד בזה ול יש לי כמה קשיים עם זה הקושי קודם כל זה לעשות משהו שלא עשיתי בעבר זה ללמוד דברים מ0 יש לי את הרצון לדעת יש לי את הרצון ללמוד אבל אני מתחיל ללמוד משהו ואני מתפזר כאילו אני עובר ממשהו למשהו ואני לא מספיק מבין לעומק את מה שאני צריך להבין אז החלטתי כאילו מיקדתי את עצמי באיזה כמה דברים שעליהם שאותם אני אעשה ובזה אני מתמקד כרגע זה נגיד מה שהיום ואתמול עשיתיL1A אני מקווה שאני אצליח לעשות אותו כמו שצריך אז כרגע החלטתי להפסיק את החיפוש עבודה כי כי ראיתי שזה גוזל ממני יותר מדי גם מבחינה בעבודה אני כל הזמן מתלבט זה לא נותן לי זה לא נותן לי שקטL1A כי אני כן רוצה להישאר בעבודה אבל אני רוצה למוד ולהתמקצע וזה לא קורה אני מקווה שזה  יקרה',\n",
       " 'אז אמרתי אחרי השיחה הזאת עם הבוס שלי אז אמרתי אני אתן לזה צאנס כאילו חודשיים כזה ונראה מה קורה',\n",
       " 'אם באמת לא יהיה שום שינוי אז אין ברירה',\n",
       " 'אני מקווה שכן יהיה שינוי',\n",
       " 'היום נגיד אני עברתי מקום כרגע לצוות אחר ודיברו שמה שאין להם מספיק מישהו שיעשה משהו מסוים',\n",
       " 'אז אמרתי אולי אפשר שאני אעשה כי אני יש לי זמ פנוי והם אמרו לי כן אבל צריך שיעשה את זה מישהו בדרגה יותר בכריה והבוס שלך לא יאשר כאילו איכשהו זה ירד מהפרק אבל אז כתבתי אחכ בסקייפ לאחד שהיה שם שינסה לדחוף אותי גם לדברים האלה כי אני לומד הכי טוב מתוך ה זה כולם ככה לומדים תוך כדי עבודה',\n",
       " 'אני תופס דברים מהר תוך כדי עבודה',\n",
       " 'אז אני רק רוצה שיתנו לי את ההזדמנות להוכיח את זה',\n",
       " 'כאילו עשיתי את זה כשהייתי בודק אני עושה את זה היום',\n",
       " 'אני רוצה אבל משהו שאני גם אלמד ממנו הלאה',\n",
       " 'זה נשמע שהפרויקט הזה אפילו אפשרות להוכיח משהו בעבודה',\n",
       " 'נכון רק שזה לא פשוט לי כלומר זה לא בא לי ככה',\n",
       " 'זה יש אנשים שיש להם שנים של נסיון בתכנות ומעיפים לא יצא לי המון לתכנת',\n",
       " 'מה שאני עושה היום בתכנות הוא ברמה ממש בסיסית אז קל לי',\n",
       " 'אם אני נתקע אני יודע איפה לחפש',\n",
       " 'אבל פה לבוא ולבנות משהו מדברים מאפס דברים שלא למדתי זה קצת יותר קשה לי',\n",
       " 'אבל כן אני זה יהיה סוג של הצלחה אם אני',\n",
       " ' כן  בהצלחה אני מקווה',\n",
       " 'תודה',\n",
       " 'אני חושב ש אלף אני שמח שאזרת אומץ לעשות את זה',\n",
       " 'האמת מישהי בעבודה שאני מתכתב איתה הרבה בסקייפ היא די דחפה אותי לעשות את זה',\n",
       " 'אני גם לא יודע אם אתה זוכר בדיוק מה שאמרת לי שבוע שעבר אבל זה מעניין כי שבוע שעבר נפגשנו לפני שנפגשת עם הבחורה מhr אמרת קוראים לזה?',\n",
       " 'כן',\n",
       " 'ולפני שפגשת את הבוס והיום אתה אחרי שניהם',\n",
       " 'אז סיפרת לי איך אתה חושב שזה ייראה אתה זוכר?',\n",
       " 'אפילו שאלתי אותך מה אתה חושב שיקרה ומה אתה חושב שיקרה לך ומה אתה היית עושה',\n",
       " 'והיום זה אחרי',\n",
       " 'אז הסיפורים מאוד שונים',\n",
       " 'החשש הגדול שלי היה אני גם שיתפתי אותם שזה היה החשש שלי שזה ייתפס כמישהו שבא ולא ולא מרוצה ממה נתנו לו',\n",
       " 'כאילו אחרי הכל קידמו אותי',\n",
       " 'כן',\n",
       " 'זה היה החשש הגדול שלי',\n",
       " 'שזה ייתפס לא טוב',\n",
       " 'גם ככה באת גם לבוס וגם לבחורה ההיא אמרת לפחות אמרת פה מקודם שאתה מתנצל שאתה מבקש סליחה ואתה לא מתכוון ל איך אמרת?',\n",
       " 'אני לא מתכוון להתלונן',\n",
       " 'אמרתי זה לא מגיע ממקום של תלונה של רע לי',\n",
       " 'טוב לי פה אני אוהב את האנשים אני אוהב את המקום אני אוהב את מה שאני עושה אבל אני כן רוצה ללמוד ממה שאני עושה וזה מה שחסר לי אמרתי להם כאילו אני לא רציתי להציג את זה כהנה רע לי ואני מפיל את זה עליכם תפתרו לי את הבעיה',\n",
       " 'אני לא מציג את זה ככה אני אומר בואו ביחד בואו נחשוב איך אפשר לפתור את זה',\n",
       " 'כן ונשמע מהתשובה שלהם שאולי הם אפילו לא הבינו מה אתה מתכוון במובן הזה ש',\n",
       " 'זהו שניהם גם ההיא מהHR אמרה לי שיש אנשים שבאמת חוששים שהיא לא מבינה למה לחשוש היא אמרה אתה צריך כרגע לשים את הקריירה שלך במרכז ואם באמת אתה מרגיש שזה אז צריך באמת לחשוב ובוא ננסה רגע לחשוב מה אפשר לעשות נעשה את זה נעשה את זה',\n",
       " 'תלך לבוס ואחרי זה תחזור ותספר לי מה',\n",
       " 'וגם הבוס שלי כאילו איך שהתחלתי להגיד תראה אני עושה אני עובד היום אתה יודע אני חושב שאני עושה את העבודה שלי בסדר אז הוא מיד אמר הרבה יותר מבסדר',\n",
       " 'אמרתי לו ואני מרגיש כאילו שאני לא לומד מזה',\n",
       " 'אני חושב שבהתחלה הוא פחד שאני הולך להגיד לו שאני מתפטר',\n",
       " 'כי הוא היה לו פנים כאלה של מה כאילו אני יזמתי את הישיבה אני אף פעם לא יוזם את הישיבות',\n",
       " 'וזהו ואז הוא מהר היה לו כמה רעיונות בשלוף אמר שהוא ינסה לדבר עם ההוא ינסה לדבר עם ההוא',\n",
       " 'מקווה שמשהו ישתנה',\n",
       " 'אנחנו עוד 5 דקות מסיימים יי',\n",
       " 'יש לי שאלה',\n",
       " 'אולי אני אפתח את השאלה כי 5 דקות זה מט אולי נמשי את זה כבר בפגישה הבאה',\n",
       " 'מה אנחנו מחפשים פה?',\n",
       " 'מה אתה מחפש?',\n",
       " 'אממ  אני מחפש אה  להגיד פתרונות זה לא יהיה נכון כי אני לא מצפה שיהיה פתרון אחד',\n",
       " 'אבל אולי איזשהו דרכים אני לא יודע איזשהם דרכי חשיבה כאילו  אני לא למדתי פסיכולוגיה אז אני לא יודע מה יש לפסיכולוגיה להציע לי ככ',\n",
       " 'אבל אולי מתוך הדברים משהו ישתנה אולי אצלי בחשיבה',\n",
       " 'משהו ישתנה כאילו בזה שאני מעלה דברים אולי אני פתאום אבין דברים אחרת',\n",
       " 'אני  אני כן חושב שזה דבר טוב',\n",
       " 'איך בדיוק זה יקרה אני לא יודע',\n",
       " 'לא שאלנו איך רק אנחנו בשאלה של המה',\n",
       " 'אז אתה אומר להבין דברים אחרת לראות דברים אחרת',\n",
       " 'לתפוס אולי את החוויות של החיים בצורה אחרת',\n",
       " 'ואם נמשיך את זה צעד יותר פנימה מהמקום של הרגש הדרך שבה אתה תופס היום את הדברים מסבה לך כאב',\n",
       " 'כן',\n",
       " 'ואולי אנחנו מחפשים דרך אחרת שבה תתפוס את הדברים ותרגיש פחות כאב',\n",
       " 'כולנו כבני אדם מרגישים כאב אבל',\n",
       " 'כן לא אני מניח שלכל בן אדם יש כל אחד והצרות שלו',\n",
       " 'כן',\n",
       " 'אולי אם אני אהיה אצל פסיכולוג דברים יהיו אחרת',\n",
       " 'מבינים דברים אחרת',\n",
       " 'או שיתברר שאי אפשר לטפל אי אפשר לפתור את זה ואז לפחות  ניסיתי',\n",
       " 'אה שזה גם חשש',\n",
       " 'אתה חושב שיש איזשהו שינוי אפשרי שאפשר לעשות בדרך שבה אתה חווה את החיים למשל את החוויות שדיברנו עכשיו למשל החוויה עם הבר או שאתה חוזר הביתה ואמא שואלת או כשאתה ניגש לבחורה מהHR כל החוויות האלה אתה רואה אפשרות לגשת אליהם עם פרספקטיבה אחרת עם מבט אחר עם תחושה אחרת?',\n",
       " 'מאיזו אממ  כאילו אם משהו ישתנה אצלי בעתיד?',\n",
       " 'כן אתה רואה אפשרות לגשת אדם כמוך במצב כמוך ניגש לבוס עם אותה בקשה אבל רואה את זה אחרת או מרגיש את זה אחרת',\n",
       " 'אני חושב שזה תלוי במה שאני מבין כאילו איך שאני אבין את ה תלוי בהרגשה שלי',\n",
       " 'אם אני רואה את זה כמשהו לא טוב אז אני ככה זה ייצא החוצה',\n",
       " 'אני חושב שזה מוצדק אני כן יודע לעמוד על שלי',\n",
       " 'אם אני מרגיש שאני ממש צודק אז אני כן יודע להעביר את זה',\n",
       " 'אז אני מניח שזה תלוי בהרגשה שלי',\n",
       " 'כן אני חושב שזה באמת ה  כרגע זה המקום שאנחנו עסוקים בו',\n",
       " 'זא החוויה שלך יש לנו הרבה מה להעמיק שם אני חושב',\n",
       " 'יש בסיפורים אנחנו כבר 3 פגישות מאחורינו והסיפורים שאתה מביא יש תחושה בסיסית גם של מחנק של אשמה של אני לא בסדר',\n",
       " 'אני מנסה לחיות את חיי ואני פוגע באחרים שזה חזר מול אמא וקצת אבא וגם בעבודה',\n",
       " 'שאתה מנסה לחיות את חייך מנסה להתפתח מנסה ללמוד דברים הכי לגיטימיים בעולם אבל אתה אולי תפגע באנשים אחרים אולי לחפש את המרחב להיות כל מיני דברים לעשות כל מיני דברים',\n",
       " 'שאלו דברים שיש להם השפעה בעולם במציאות בחוץ אבל הם מתחילים אצלך פנימה',\n",
       " 'במחשבות שלך ברגשות שלך באיך אתה תופס את הדברים',\n",
       " 'ואני אחזור קצת על מה שאמרתי שאני יודע שזה קשה אני רואה שזה קשה אני גם מבין למה זה קשה',\n",
       " 'אבל לא הייתי נשאר פה אם לא הייתי מאמין שזה מוביל למקום טוב',\n",
       " 'אני לא יודע איך אתה רואה את זה עכשיו',\n",
       " 'אני מקווה שזה יהיה ככה',\n",
       " 'אני רוצה להאמין שכרגע זה הביוב שיוצא',\n",
       " 'כן כן אני מניח שבאיזשהו מקום אתה יודע את זה',\n",
       " 'אתה חוזר לא רק שאתה מגיע פיזית הרי אני לא יכול לחייב שום דבר אתה פותח את הדברים בכנות ובאומץ',\n",
       " 'אני מניח מתוך ההבנה הזאת',\n",
       " 'אני אהיה איתך כנה יש לי המון דברים',\n",
       " 'לא את הכל עדיין סיפרתי',\n",
       " 'אני  נפתח לאט לאט',\n",
       " 'אני תמיד כזה',\n",
       " 'אתה אומר את זה למה כי אמרתי שאתה אמיץ?',\n",
       " 'ופותח בכנות?',\n",
       " 'כן כי זה נראה כאילו שפכתי הכל',\n",
       " 'לא שפכתי הכל',\n",
       " 'יש אני עשיתי',\n",
       " 'בסדר גמור יש לנו זמן וזה גם לא אני חושב שאתה הכי טוב מה הקצב שלך מה הכי נכון לך',\n",
       " 'כי אין לי שום צורך שתפתח מהר מדי או תרגיש מאוד מאוד רע ואז ייצא מכאן אתה עוד רק מתחיל להכיר אותי כמו שאני מתחיל להכיר אותך',\n",
       " 'אז יש לנו זמן תקשיב לעצמך',\n",
       " 'רק בקצב שלך',\n",
       " 'יש משהו אחרון שיכול לעזור?',\n",
       " 'לקראת השבוע שיהיה?',\n",
       " 'שאתה רוצה להגיד?',\n",
       " 'אממ  לא אין לי משהו קונקרטי',\n",
       " 'בסדר טוב אז  אני אמשיך את הנושא המחקרי ואז ניפרד?',\n",
       " 'כן',\n",
       " 'בסדר']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_map[1]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a1cef1",
   "metadata": {},
   "source": [
    "#### Prepare tested docs for imput to BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "23a8d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_db = common_utils.concat_dbs(dir_name,\"sent_db\",['text','is_nar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "6e6f0e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsofya/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "imp.reload(my_bert)\n",
    "alephbert_tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\n",
    "test_text, test_labels = my_bert.get_text_label_by_doc(sent_db,test_docs)\n",
    "test_tokens = my_bert.get_test_tokens(alephbert_tokenizer,test_text)\n",
    "tensor_map = my_bert.covert_test_token2tensor(test_tokens,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b339ea7",
   "metadata": {},
   "source": [
    "#### Load pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "279da557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"alephbert\"\n",
    "alephbert = BertModel.from_pretrained('onlplab/alephbert-base', return_dict=False)\n",
    "wrapped_model = my_bert.wrap_pretained_model(alephbert)\n",
    "saved_model = my_bert.load_saved_bert_model(wrapped_model,model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba3bd3",
   "metadata": {},
   "source": [
    "#### Instance CRF with selected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a9f053f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'lbfgs',\n",
       " 'c1': 0.5052489623208797,\n",
       " 'c2': 0.03723629092212718,\n",
       " 'linesearch': 'MoreThuente',\n",
       " 'min_freq': 9}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_path = os.path.join(os.getcwd(),defines.PATH_TO_DFS,dir_name,\"best_params_lemma.word_docFoldCv_2.json\")\n",
    "\n",
    "with open(json_path, 'r') as fp:\n",
    "    best_params_crf = json.load(fp)\n",
    "best_params_crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ce40815d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.5052489623208797,\n",
       "    c2=0.03723629092212718, linesearch=&#x27;MoreThuente&#x27;, max_iterations=100,\n",
       "    min_freq=9)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.5052489623208797,\n",
       "    c2=0.03723629092212718, linesearch=&#x27;MoreThuente&#x27;, max_iterations=100,\n",
       "    min_freq=9)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.5052489623208797,\n",
       "    c2=0.03723629092212718, linesearch='MoreThuente', max_iterations=100,\n",
       "    min_freq=9)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = CRF(\n",
    "    **best_params_crf,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    ")\n",
    "crf.fit(X_train,y_train)\n",
    "crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "15c78a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_label_from_prob(df):\n",
    "    return \"not_nar\" if df['avg_prob_0']>df['avg_prob_1'] else \"is_nar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "3d7f30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(my_bert)\n",
    "imp.reload(model_utils)\n",
    "\n",
    "def ensemble_predictions(crf_model,x_test_crf,bert_model,x_test_bert,weights):\n",
    "    pred_df = pd.DataFrame()\n",
    "    bert_label, bert_proba = my_bert.get_prediction(bert_model,x_test_bert)\n",
    "    crf_label = flatten(crf_model.predict(x_test_crf))\n",
    "    crf_proba = model_utils.get_predicted_prob_from_dict(flatten(crf_model.predict_marginals(x_test_crf)))\n",
    "    pred_df['bert'] = ['not_nar' if i ==0 else 'is_nar' for i in bert_label]\n",
    "    pred_df['crf'] = crf_label\n",
    "    pred_df['bert_proba_0']=bert_proba[:,0]\n",
    "    pred_df['bert_proba_1']=bert_proba[:,1]\n",
    "    pred_df['crf_proba_0']=crf_proba[:,0]\n",
    "    pred_df['crf_proba_1']=crf_proba[:,1]\n",
    "    probas = np.asarray([crf_proba,bert_proba])\n",
    "    avg = np.average(\n",
    "                probas, axis=0, weights=weigths\n",
    "            )\n",
    "    pred_df['avg_prob_0'] = avg[:,0]\n",
    "    pred_df['avg_prob_1'] = avg[:,1]\n",
    "    pred_df['voted_label'] = pred_df[['avg_prob_0','avg_prob_1']].idxmax(axis=1)\n",
    "    pred_df['voted_label'].replace('avg_prob_0','not_nar',inplace=True)\n",
    "    pred_df['voted_label'].replace('avg_prob_1','is_nar',inplace=True)\n",
    "    return pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "4010c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_prob_from_dict(y_pred_proba_flat):\n",
    "    pr_arr = np.zeros((len(y_pred_proba_flat),2))\n",
    "    pr_arr[:,0] = [sample['not_nar'] for sample in y_pred_proba_flat]\n",
    "    pr_arr[:,1] = [sample['is_nar'] for sample in y_pred_proba_flat]\n",
    "    return pr_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2ac71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00218d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ddc62035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert</th>\n",
       "      <th>crf</th>\n",
       "      <th>bert_proba_0</th>\n",
       "      <th>bert_proba_1</th>\n",
       "      <th>crf_proba_0</th>\n",
       "      <th>crf_proba_1</th>\n",
       "      <th>avg_prob_0</th>\n",
       "      <th>avg_prob_1</th>\n",
       "      <th>voted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>not_nar</td>\n",
       "      <td>0.718448</td>\n",
       "      <td>0.281552</td>\n",
       "      <td>0.981550</td>\n",
       "      <td>0.018450</td>\n",
       "      <td>0.902619</td>\n",
       "      <td>0.097381</td>\n",
       "      <td>not_nar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>not_nar</td>\n",
       "      <td>0.807705</td>\n",
       "      <td>0.192295</td>\n",
       "      <td>0.990942</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>0.935971</td>\n",
       "      <td>0.064029</td>\n",
       "      <td>not_nar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>not_nar</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.153494</td>\n",
       "      <td>0.997356</td>\n",
       "      <td>0.002644</td>\n",
       "      <td>0.952101</td>\n",
       "      <td>0.047899</td>\n",
       "      <td>not_nar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>is_nar</td>\n",
       "      <td>0.410462</td>\n",
       "      <td>0.589538</td>\n",
       "      <td>0.383633</td>\n",
       "      <td>0.616367</td>\n",
       "      <td>0.391682</td>\n",
       "      <td>0.608318</td>\n",
       "      <td>is_nar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>is_nar</td>\n",
       "      <td>0.243487</td>\n",
       "      <td>0.756513</td>\n",
       "      <td>0.336836</td>\n",
       "      <td>0.663164</td>\n",
       "      <td>0.308831</td>\n",
       "      <td>0.691169</td>\n",
       "      <td>is_nar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>1</td>\n",
       "      <td>not_nar</td>\n",
       "      <td>0.421347</td>\n",
       "      <td>0.578653</td>\n",
       "      <td>0.968799</td>\n",
       "      <td>0.031201</td>\n",
       "      <td>0.804563</td>\n",
       "      <td>0.195436</td>\n",
       "      <td>not_nar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6099</th>\n",
       "      <td>0</td>\n",
       "      <td>not_nar</td>\n",
       "      <td>0.586311</td>\n",
       "      <td>0.413688</td>\n",
       "      <td>0.965671</td>\n",
       "      <td>0.034329</td>\n",
       "      <td>0.851863</td>\n",
       "      <td>0.148137</td>\n",
       "      <td>not_nar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>0</td>\n",
       "      <td>not_nar</td>\n",
       "      <td>0.747379</td>\n",
       "      <td>0.252621</td>\n",
       "      <td>0.952480</td>\n",
       "      <td>0.047520</td>\n",
       "      <td>0.890950</td>\n",
       "      <td>0.109050</td>\n",
       "      <td>not_nar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6101</th>\n",
       "      <td>0</td>\n",
       "      <td>not_nar</td>\n",
       "      <td>0.830255</td>\n",
       "      <td>0.169745</td>\n",
       "      <td>0.906428</td>\n",
       "      <td>0.093572</td>\n",
       "      <td>0.883576</td>\n",
       "      <td>0.116424</td>\n",
       "      <td>not_nar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6102</th>\n",
       "      <td>0</td>\n",
       "      <td>not_nar</td>\n",
       "      <td>0.772771</td>\n",
       "      <td>0.227229</td>\n",
       "      <td>0.885166</td>\n",
       "      <td>0.114834</td>\n",
       "      <td>0.851448</td>\n",
       "      <td>0.148552</td>\n",
       "      <td>not_nar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6103 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bert      crf  bert_proba_0  bert_proba_1  crf_proba_0  crf_proba_1  \\\n",
       "0        0  not_nar      0.718448      0.281552     0.981550     0.018450   \n",
       "1        0  not_nar      0.807705      0.192295     0.990942     0.009058   \n",
       "2        0  not_nar      0.846506      0.153494     0.997356     0.002644   \n",
       "3        1   is_nar      0.410462      0.589538     0.383633     0.616367   \n",
       "4        1   is_nar      0.243487      0.756513     0.336836     0.663164   \n",
       "...    ...      ...           ...           ...          ...          ...   \n",
       "6098     1  not_nar      0.421347      0.578653     0.968799     0.031201   \n",
       "6099     0  not_nar      0.586311      0.413688     0.965671     0.034329   \n",
       "6100     0  not_nar      0.747379      0.252621     0.952480     0.047520   \n",
       "6101     0  not_nar      0.830255      0.169745     0.906428     0.093572   \n",
       "6102     0  not_nar      0.772771      0.227229     0.885166     0.114834   \n",
       "\n",
       "      avg_prob_0  avg_prob_1 voted_label  \n",
       "0       0.902619    0.097381     not_nar  \n",
       "1       0.935971    0.064029     not_nar  \n",
       "2       0.952101    0.047899     not_nar  \n",
       "3       0.391682    0.608318      is_nar  \n",
       "4       0.308831    0.691169      is_nar  \n",
       "...          ...         ...         ...  \n",
       "6098    0.804563    0.195436     not_nar  \n",
       "6099    0.851863    0.148137     not_nar  \n",
       "6100    0.890950    0.109050     not_nar  \n",
       "6101    0.883576    0.116424     not_nar  \n",
       "6102    0.851448    0.148552     not_nar  \n",
       "\n",
       "[6103 rows x 9 columns]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights=[0.7,0.3]\n",
    "pred_df = ensemble_predictions(crf,X_test,saved_model,tensor_map['test'],weights)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "b66bbae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[46mBERT alone\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     not_nar       0.69      0.57      0.63      4029\n",
      "      is_nar       0.38      0.50      0.43      2074\n",
      "\n",
      "    accuracy                           0.55      6103\n",
      "   macro avg       0.53      0.54      0.53      6103\n",
      "weighted avg       0.58      0.55      0.56      6103\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_pred = pred_df['bert'].tolist()\n",
    "labels = list(crf.classes_)\n",
    "bert_pred_str = ['not_nar' if i ==0 else 'is_nar' for i in bert_pred]\n",
    "feature_utils.get_prediction_report(flatten(y_test),bert_pred_str,labels,\"BERT alone\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
