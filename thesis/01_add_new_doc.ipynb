{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f91f9cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os,sys, imp\n",
    "sys.path.append('./src/')\n",
    "import doc_utils_clean as doc_utils\n",
    "import feature_utils\n",
    "import defines\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "45940f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>file_name</th>\n",
       "      <th>doc_idx_from_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./tmp/01_aingimel3_lc.docx</td>\n",
       "      <td>01_aingimel3_lc.docx</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./tmp/02_aingimel12_lc.docx</td>\n",
       "      <td>02_aingimel12_lc.docx</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./tmp/03_aingimel26_lc.docx</td>\n",
       "      <td>03_aingimel26_lc.docx</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>./tmp/04_nun2_lc.docx</td>\n",
       "      <td>04_nun2_lc.docx</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./tmp/05_kafhey_23_lc.docx</td>\n",
       "      <td>05_kafhey_23_lc.docx</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./tmp/06_lamedbet21_lc.docx</td>\n",
       "      <td>06_lamedbet21_lc.docx</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./tmp/07_alefsameh3_lc.docx</td>\n",
       "      <td>07_alefsameh3_lc.docx</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./tmp/08_kafdalet_15_lc.docx</td>\n",
       "      <td>08_kafdalet_15_lc.docx</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tmp/09_kafhey_23_l.docx</td>\n",
       "      <td>09_kafhey_23_l.docx</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./tmp/10_aingimel5_Mor.docx</td>\n",
       "      <td>10_aingimel5_Mor.docx</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>./tmp/11_aingimel10_Mor.docx</td>\n",
       "      <td>11_aingimel10_Mor.docx</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>./tmp/11_aingimel10_Mor.docx</td>\n",
       "      <td>11_aingimel10_Mor.docx</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>./tmp/12_aingimel16_Mor.docx</td>\n",
       "      <td>12_aingimel16_Mor.docx</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>./tmp/13_aingimel18_Mor.docx</td>\n",
       "      <td>13_aingimel18_Mor.docx</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>./tmp/14_aingimel22_Mor.docx</td>\n",
       "      <td>14_aingimel22_Mor.docx</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>./tmp/15_aingimel24_Mor.docx</td>\n",
       "      <td>15_aingimel24_Mor.docx</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>./tmp/16_aingimel7_Mor.docx</td>\n",
       "      <td>16_aingimel7_Mor.docx</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>./tmp/17_samehAleph16_DanaMor.docx</td>\n",
       "      <td>17_samehAleph16_DanaMor.docx</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>./tmp/18_kafTet7_DanaMor.docx</td>\n",
       "      <td>18_kafTet7_DanaMor.docx</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>./tmp/19_tsadek13_DanaMor.docx</td>\n",
       "      <td>19_tsadek13_DanaMor.docx</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>./tmp/20_kafDalet3_Dana.docx</td>\n",
       "      <td>20_kafDalet3_Dana.docx</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>./tmp/21_kafHey3_Dana.docx</td>\n",
       "      <td>21_kafHey3_Dana.docx</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>./tmp/22_kafDalet5_Dana.docx</td>\n",
       "      <td>22_kafDalet5_Dana.docx</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>./tmp/23_kafDalet7_Dana.docx</td>\n",
       "      <td>23_kafDalet7_Dana.docx</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>./tmp/24_kafHey5_Dana.docx</td>\n",
       "      <td>24_kafHey5_Dana.docx</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>./tmp/25_kafDalet9_Dana.docx</td>\n",
       "      <td>25_kafDalet9_Dana.docx</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>./tmp/26_kafHey9_Dana.docx</td>\n",
       "      <td>26_kafHey9_Dana.docx</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>./tmp/27_kafDalet11_Dana.docx</td>\n",
       "      <td>27_kafDalet11_Dana.docx</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>./tmp/28_kafHey11_Dana.docx</td>\n",
       "      <td>28_kafHey11_Dana.docx</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>./tmp/29_kafHey12_Dana.docx</td>\n",
       "      <td>29_kafHey12_Dana.docx</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>./tmp/30_kafDalet15_Dana.docx</td>\n",
       "      <td>30_kafDalet15_Dana.docx</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>./tmp/31_kafHey15_Dana.docx</td>\n",
       "      <td>31_kafHey15_Dana.docx</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>./tmp/32_kafDalet17_Dana.docx</td>\n",
       "      <td>32_kafDalet17_Dana.docx</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>./tmp/33_kafHey17_Dana.docx</td>\n",
       "      <td>33_kafHey17_Dana.docx</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>./tmp/34_kafHey18_Dana.docx</td>\n",
       "      <td>34_kafHey18_Dana.docx</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>./tmp/35_kafHey20_Dana.docx</td>\n",
       "      <td>35_kafHey20_Dana.docx</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  path                     file_name  \\\n",
       "3           ./tmp/01_aingimel3_lc.docx          01_aingimel3_lc.docx   \n",
       "2          ./tmp/02_aingimel12_lc.docx         02_aingimel12_lc.docx   \n",
       "1          ./tmp/03_aingimel26_lc.docx         03_aingimel26_lc.docx   \n",
       "7                ./tmp/04_nun2_lc.docx               04_nun2_lc.docx   \n",
       "5           ./tmp/05_kafhey_23_lc.docx          05_kafhey_23_lc.docx   \n",
       "6          ./tmp/06_lamedbet21_lc.docx         06_lamedbet21_lc.docx   \n",
       "0          ./tmp/07_alefsameh3_lc.docx         07_alefsameh3_lc.docx   \n",
       "4         ./tmp/08_kafdalet_15_lc.docx        08_kafdalet_15_lc.docx   \n",
       "35             tmp/09_kafhey_23_l.docx           09_kafhey_23_l.docx   \n",
       "8          ./tmp/10_aingimel5_Mor.docx         10_aingimel5_Mor.docx   \n",
       "9         ./tmp/11_aingimel10_Mor.docx        11_aingimel10_Mor.docx   \n",
       "10        ./tmp/11_aingimel10_Mor.docx        11_aingimel10_Mor.docx   \n",
       "11        ./tmp/12_aingimel16_Mor.docx        12_aingimel16_Mor.docx   \n",
       "12        ./tmp/13_aingimel18_Mor.docx        13_aingimel18_Mor.docx   \n",
       "13        ./tmp/14_aingimel22_Mor.docx        14_aingimel22_Mor.docx   \n",
       "14        ./tmp/15_aingimel24_Mor.docx        15_aingimel24_Mor.docx   \n",
       "15         ./tmp/16_aingimel7_Mor.docx         16_aingimel7_Mor.docx   \n",
       "16  ./tmp/17_samehAleph16_DanaMor.docx  17_samehAleph16_DanaMor.docx   \n",
       "17       ./tmp/18_kafTet7_DanaMor.docx       18_kafTet7_DanaMor.docx   \n",
       "18      ./tmp/19_tsadek13_DanaMor.docx      19_tsadek13_DanaMor.docx   \n",
       "19        ./tmp/20_kafDalet3_Dana.docx        20_kafDalet3_Dana.docx   \n",
       "20          ./tmp/21_kafHey3_Dana.docx          21_kafHey3_Dana.docx   \n",
       "21        ./tmp/22_kafDalet5_Dana.docx        22_kafDalet5_Dana.docx   \n",
       "22        ./tmp/23_kafDalet7_Dana.docx        23_kafDalet7_Dana.docx   \n",
       "23          ./tmp/24_kafHey5_Dana.docx          24_kafHey5_Dana.docx   \n",
       "24        ./tmp/25_kafDalet9_Dana.docx        25_kafDalet9_Dana.docx   \n",
       "25          ./tmp/26_kafHey9_Dana.docx          26_kafHey9_Dana.docx   \n",
       "26       ./tmp/27_kafDalet11_Dana.docx       27_kafDalet11_Dana.docx   \n",
       "27         ./tmp/28_kafHey11_Dana.docx         28_kafHey11_Dana.docx   \n",
       "28         ./tmp/29_kafHey12_Dana.docx         29_kafHey12_Dana.docx   \n",
       "29       ./tmp/30_kafDalet15_Dana.docx       30_kafDalet15_Dana.docx   \n",
       "30         ./tmp/31_kafHey15_Dana.docx         31_kafHey15_Dana.docx   \n",
       "31       ./tmp/32_kafDalet17_Dana.docx       32_kafDalet17_Dana.docx   \n",
       "32         ./tmp/33_kafHey17_Dana.docx         33_kafHey17_Dana.docx   \n",
       "33         ./tmp/34_kafHey18_Dana.docx         34_kafHey18_Dana.docx   \n",
       "34         ./tmp/35_kafHey20_Dana.docx         35_kafHey20_Dana.docx   \n",
       "\n",
       "    doc_idx_from_name  \n",
       "3            1.000000  \n",
       "2            2.000000  \n",
       "1            3.000000  \n",
       "7            4.000000  \n",
       "5            5.000000  \n",
       "6            6.000000  \n",
       "0            7.000000  \n",
       "4            8.000000  \n",
       "35           9.000000  \n",
       "8           10.000000  \n",
       "9           11.000000  \n",
       "10          11.000000  \n",
       "11          12.000000  \n",
       "12          13.000000  \n",
       "13          14.000000  \n",
       "14          15.000000  \n",
       "15          16.000000  \n",
       "16          17.000000  \n",
       "17          18.000000  \n",
       "18          19.000000  \n",
       "19          20.000000  \n",
       "20          21.000000  \n",
       "21          22.000000  \n",
       "22          23.000000  \n",
       "23          24.000000  \n",
       "24          25.000000  \n",
       "25          26.000000  \n",
       "26          27.000000  \n",
       "27          28.000000  \n",
       "28          29.000000  \n",
       "29          30.000000  \n",
       "30          31.000000  \n",
       "31          32.000000  \n",
       "32          33.000000  \n",
       "33          34.000000  \n",
       "34          35.000000  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_db =  pd.read_csv(\"./dataframes/doc_db.csv\")\n",
    "doc_db.sort_values(by='doc_idx_from_name',inplace=True)\n",
    "doc_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4a6e2e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc_idx = [9]\n",
    "new_doc_idx.extend(np.arange(20,36))\n",
    "new_doc_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8a58cf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 sent lemma db saved\n",
      "9 sent count db saved\n",
      "9 sent features db saved\n",
      "9 doc sent saved\n",
      "9 sim_db sent saved\n",
      "20 sent lemma db saved\n",
      "20 sent count db saved\n",
      "20 sent features db saved\n",
      "20 doc sent saved\n",
      "20 sim_db sent saved\n",
      "21 sent lemma db saved\n",
      "21 sent count db saved\n",
      "21 sent features db saved\n",
      "21 doc sent saved\n",
      "21 sim_db sent saved\n",
      "22 sent lemma db saved\n",
      "22 sent count db saved\n",
      "22 sent features db saved\n",
      "22 doc sent saved\n",
      "22 sim_db sent saved\n",
      "23 sent lemma db saved\n",
      "23 sent count db saved\n",
      "23 sent features db saved\n",
      "23 doc sent saved\n",
      "23 sim_db sent saved\n",
      "24 sent lemma db saved\n",
      "24 sent count db saved\n",
      "24 sent features db saved\n",
      "24 doc sent saved\n",
      "24 sim_db sent saved\n",
      "25 sent lemma db saved\n",
      "25 sent count db saved\n",
      "25 sent features db saved\n",
      "25 doc sent saved\n",
      "25 sim_db sent saved\n",
      "26 sent lemma db saved\n",
      "26 sent count db saved\n",
      "26 sent features db saved\n",
      "26 doc sent saved\n",
      "26 sim_db sent saved\n",
      "27 sent lemma db saved\n",
      "27 sent count db saved\n",
      "27 sent features db saved\n",
      "27 doc sent saved\n",
      "27 sim_db sent saved\n",
      "28 sent lemma db saved\n",
      "28 sent count db saved\n",
      "28 sent features db saved\n",
      "28 doc sent saved\n",
      "28 sim_db sent saved\n",
      "29 sent lemma db saved\n",
      "29 sent count db saved\n",
      "29 sent features db saved\n",
      "29 doc sent saved\n",
      "29 sim_db sent saved\n",
      "30 sent lemma db saved\n",
      "30 sent count db saved\n",
      "30 sent features db saved\n",
      "30 doc sent saved\n",
      "30 sim_db sent saved\n",
      "31 sent lemma db saved\n",
      "31 sent count db saved\n",
      "31 sent features db saved\n",
      "31 doc sent saved\n",
      "31 sim_db sent saved\n",
      "32 sent lemma db saved\n",
      "32 sent count db saved\n",
      "32 sent features db saved\n",
      "32 doc sent saved\n",
      "32 sim_db sent saved\n",
      "33 sent lemma db saved\n",
      "33 sent count db saved\n",
      "33 sent features db saved\n",
      "33 doc sent saved\n",
      "33 sim_db sent saved\n",
      "34 sent lemma db saved\n",
      "34 sent count db saved\n",
      "34 sent features db saved\n",
      "34 doc sent saved\n",
      "34 sim_db sent saved\n",
      "35 sent lemma db saved\n",
      "35 sent count db saved\n",
      "35 sent features db saved\n",
      "35 doc sent saved\n",
      "35 sim_db sent saved\n"
     ]
    }
   ],
   "source": [
    "for idx in new_doc_idx:\n",
    "    feature_utils.save_doc_features(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ef9b91f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfIdf 4 saved\n",
      "TfIdf 11 saved\n",
      "TfIdf 30 saved\n",
      "TfIdf 25 saved\n",
      "TfIdf 9 saved\n",
      "TfIdf 23 saved\n",
      "TfIdf 2 saved\n",
      "TfIdf 17 saved\n",
      "TfIdf 28 saved\n",
      "TfIdf 31 saved\n",
      "TfIdf 24 saved\n",
      "TfIdf 5 saved\n",
      "TfIdf 10 saved\n",
      "TfIdf 29 saved\n",
      "TfIdf 3 saved\n",
      "TfIdf 16 saved\n",
      "TfIdf 22 saved\n",
      "TfIdf 8 saved\n",
      "TfIdf 27 saved\n",
      "TfIdf 32 saved\n",
      "TfIdf 18 saved\n",
      "TfIdf 6 saved\n",
      "TfIdf 15 saved\n",
      "TfIdf 21 saved\n",
      "TfIdf 34 saved\n",
      "TfIdf 12 saved\n",
      "TfIdf 7 saved\n",
      "TfIdf 19 saved\n",
      "TfIdf 26 saved\n",
      "TfIdf 33 saved\n",
      "TfIdf 20 saved\n",
      "TfIdf 35 saved\n",
      "TfIdf 14 saved\n",
      "TfIdf 1 saved\n"
     ]
    }
   ],
   "source": [
    "imp.reload(feature_utils)\n",
    "feature_utils.tfidf_build_all_save_per_doc(per_word=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d5144249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfIdf 4 saved\n",
      "TfIdf 11 saved\n",
      "TfIdf 30 saved\n",
      "TfIdf 25 saved\n",
      "TfIdf 9 saved\n",
      "TfIdf 23 saved\n",
      "TfIdf 2 saved\n",
      "TfIdf 17 saved\n",
      "TfIdf 28 saved\n",
      "TfIdf 31 saved\n",
      "TfIdf 24 saved\n",
      "TfIdf 5 saved\n",
      "TfIdf 10 saved\n",
      "TfIdf 29 saved\n",
      "TfIdf 3 saved\n",
      "TfIdf 16 saved\n",
      "TfIdf 22 saved\n",
      "TfIdf 8 saved\n",
      "TfIdf 27 saved\n",
      "TfIdf 32 saved\n",
      "TfIdf 18 saved\n",
      "TfIdf 6 saved\n",
      "TfIdf 15 saved\n",
      "TfIdf 21 saved\n",
      "TfIdf 34 saved\n",
      "TfIdf 12 saved\n",
      "TfIdf 7 saved\n",
      "TfIdf 19 saved\n",
      "TfIdf 26 saved\n",
      "TfIdf 33 saved\n",
      "TfIdf 20 saved\n",
      "TfIdf 35 saved\n",
      "TfIdf 14 saved\n",
      "TfIdf 1 saved\n"
     ]
    }
   ],
   "source": [
    "# imp.reload(feature_utils)#\n",
    "feature_utils.tfidf_build_all_save_per_doc(per_word=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2994e3",
   "metadata": {},
   "source": [
    "### Run cross-validation on 35 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a8207e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "def get_prediction(train_idx,test_idx,X,y):\n",
    "    crf = CRF(\n",
    "    min_freq = 5,\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    )\n",
    "#     print (\"Test idx: {}..., train idx: {}...\".format(train_idx[:10],test_idx[:10]))\n",
    "    X_train = itemgetter(*train_idx)(X)\n",
    "    y_train  = itemgetter(*train_idx)(y)\n",
    "    X_test = itemgetter(*test_idx)(X)\n",
    "    y_test = itemgetter(*test_idx)(y)\n",
    "    crf.fit(X_train, y_train)\n",
    "    y_pred  =  crf.predict(X_test)\n",
    "    labels = list(crf.classes_)\n",
    "    f1 = metrics.flat_f1_score(y_test, y_pred,average='weighted', labels=labels)\n",
    "    recall = metrics.flat_recall_score(y_test, y_pred,average='weighted', labels=labels)\n",
    "    precision = metrics.flat_precision_score(y_test, y_pred,average='weighted', labels=labels)\n",
    "    return [f1,recall,precision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c7fa2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaveOneOut_validate(X,y,groups,seq_len,step):\n",
    "    logo = LeaveOneGroupOut()\n",
    "    score_list = []\n",
    "    for train_idx, test_idx in logo.split(X, y, groups):\n",
    "        score_list.append(get_prediction(train_idx,test_idx,X,y))\n",
    "    return np.array(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "83e3cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import scorers, CRF\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.model_selection import LeaveOneGroupOut,LeavePGroupsOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bd8f771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_db = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "91d81b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_score_to_db(score_db,prefix,score):\n",
    "    mean_values = score.mean(axis=0)\n",
    "    print(\"mean_values {}\".format(mean_values))\n",
    "    score_db.loc[prefix,\"f1\"] = mean_values[0]\n",
    "    score_db.loc[prefix,\"recall\"] = mean_values[1]\n",
    "    score_db.loc[prefix,\"precision\"] = mean_values[2]\n",
    "    return score_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392b917",
   "metadata": {},
   "source": [
    "### Pack features per doc: with TfIDF chars, with only features that hold no zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a812f667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 doc 543 sentences packed\n",
      "Doc 4 sentenced packed\n",
      "11 doc 174 sentences packed\n",
      "Doc 11 sentenced packed\n",
      "30 doc 258 sentences packed\n",
      "Doc 30 sentenced packed\n",
      "25 doc 331 sentences packed\n",
      "Doc 25 sentenced packed\n",
      "9 doc 384 sentences packed\n",
      "Doc 9 sentenced packed\n",
      "23 doc 237 sentences packed\n",
      "Doc 23 sentenced packed\n",
      "2 doc 186 sentences packed\n",
      "Doc 2 sentenced packed\n",
      "17 doc 622 sentences packed\n",
      "Doc 17 sentenced packed\n",
      "28 doc 445 sentences packed\n",
      "Doc 28 sentenced packed\n",
      "31 doc 283 sentences packed\n",
      "Doc 31 sentenced packed\n",
      "24 doc 613 sentences packed\n",
      "Doc 24 sentenced packed\n",
      "5 doc 375 sentences packed\n",
      "Doc 5 sentenced packed\n",
      "10 doc 424 sentences packed\n",
      "Doc 10 sentenced packed\n",
      "29 doc 398 sentences packed\n",
      "Doc 29 sentenced packed\n",
      "3 doc 152 sentences packed\n",
      "Doc 3 sentenced packed\n",
      "16 doc 438 sentences packed\n",
      "Doc 16 sentenced packed\n",
      "22 doc 284 sentences packed\n",
      "Doc 22 sentenced packed\n",
      "8 doc 259 sentences packed\n",
      "Doc 8 sentenced packed\n",
      "27 doc 217 sentences packed\n",
      "Doc 27 sentenced packed\n",
      "32 doc 253 sentences packed\n",
      "Doc 32 sentenced packed\n",
      "18 doc 157 sentences packed\n",
      "Doc 18 sentenced packed\n",
      "6 doc 471 sentences packed\n",
      "Doc 6 sentenced packed\n",
      "15 doc 159 sentences packed\n",
      "Doc 15 sentenced packed\n",
      "21 doc 672 sentences packed\n",
      "Doc 21 sentenced packed\n",
      "34 doc 312 sentences packed\n",
      "Doc 34 sentenced packed\n",
      "12 doc 198 sentences packed\n",
      "Doc 12 sentenced packed\n",
      "7 doc 448 sentences packed\n",
      "Doc 7 sentenced packed\n",
      "19 doc 374 sentences packed\n",
      "Doc 19 sentenced packed\n",
      "26 doc 380 sentences packed\n",
      "Doc 26 sentenced packed\n",
      "33 doc 328 sentences packed\n",
      "Doc 33 sentenced packed\n",
      "20 doc 230 sentences packed\n",
      "Doc 20 sentenced packed\n",
      "35 doc 334 sentences packed\n",
      "Doc 35 sentenced packed\n",
      "14 doc 176 sentences packed\n",
      "Doc 14 sentenced packed\n",
      "1 doc 618 sentences packed\n",
      "Doc 1 sentenced packed\n",
      "11733 sentenced packed for 34 docs\n"
     ]
    }
   ],
   "source": [
    "imp.reload(feature_utils)\n",
    "X,y,group =  feature_utils.pack_all_doc_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d083a",
   "metadata": {},
   "source": [
    "### Check feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "738c0bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_client': 1,\n",
       " 'sent_len': 76.0,\n",
       " 'TOKEN': 17,\n",
       " 'POSTAG_AT': 1,\n",
       " 'POSTAG_CONJ': 5,\n",
       " 'POSTAG_COP': 1,\n",
       " 'POSTAG_DEF': 4,\n",
       " 'POSTAG_DTT': 1,\n",
       " 'POSTAG_IN': 2,\n",
       " 'POSTAG_INTJ': 2,\n",
       " 'POSTAG_NN': 6,\n",
       " 'POSTAG_NNP': 2,\n",
       " 'POSTAG_PREPOSITION': 2,\n",
       " 'POSTAG_VB': 2,\n",
       " 'f_gen_M': 11,\n",
       " 'f_num_P': 2,\n",
       " 'f_num_S': 9,\n",
       " 'f_per_1': 2,\n",
       " 'f_per_3': 1,\n",
       " 'f_tense_PAST': 2,\n",
       " '-1:is_client': 1,\n",
       " '-1:sent_len': 52.0,\n",
       " '-1:TOKEN': 11,\n",
       " '-1:POSTAG_IN': 1,\n",
       " '-1:POSTAG_INTJ': 1,\n",
       " '-1:POSTAG_NN': 2,\n",
       " '-1:POSTAG_NNP': 1,\n",
       " '-1:POSTAG_PREPOSITION': 1,\n",
       " '-1:POSTAG_PRP': 1,\n",
       " '-1:POSTAG_RB': 2,\n",
       " '-1:POSTAG_S_PRN': 1,\n",
       " '-1:POSTAG_TEMP': 1,\n",
       " '-1:POSTAG_VB': 3,\n",
       " '-1:f_gen_M': 7,\n",
       " '-1:f_num_P': 1,\n",
       " '-1:f_num_S': 6,\n",
       " '-1:f_per_1': 2,\n",
       " '-1:f_per_3': 2,\n",
       " '-1:f_tense_PAST': 2,\n",
       " '-2:is_client': 1,\n",
       " '-2:sent_len': 48.0,\n",
       " '-2:TOKEN': 10,\n",
       " '-2:POSTAG_BN': 1,\n",
       " '-2:POSTAG_IN': 1,\n",
       " '-2:POSTAG_INTJ': 2,\n",
       " '-2:POSTAG_JJ': 2,\n",
       " '-2:POSTAG_NN': 2,\n",
       " '-2:POSTAG_PRP': 1,\n",
       " '-2:POSTAG_VB': 1,\n",
       " '-2:f_gen_M': 7,\n",
       " '-2:f_num_P': 2,\n",
       " '-2:f_num_S': 5,\n",
       " '-2:f_per_1': 2,\n",
       " '-2:f_per_A': 1,\n",
       " '-2:f_tense_PAST': 1,\n",
       " '-1.sim': 0.7377620676112061,\n",
       " '+1.sim': 0.6920749637681133,\n",
       " '-2.sim': 0.7100888572732766,\n",
       " '+2.sim': 0.6377150130742303,\n",
       " 'tf_word_6686': 0.27995403178537726,\n",
       " 'tf_word_6650': 0.34895707328720293,\n",
       " 'tf_word_5146': 0.2882079578522682,\n",
       " 'tf_word_5098': 0.2830043205889614,\n",
       " 'tf_word_4817': 0.24311663334586195,\n",
       " 'tf_word_4374': 0.3606815950939775,\n",
       " 'tf_word_4282': 0.36413893951115106,\n",
       " 'tf_word_3264': 0.13897079133067475,\n",
       " 'tf_word_2857': 0.35162367334710104,\n",
       " 'tf_word_1674': 0.11995893198791842,\n",
       " 'tf_word_1279': 0.1606567950624682,\n",
       " 'tf_word_682': 0.09514289383992759,\n",
       " 'tf_word_156': 0.35028608453439153,\n",
       " 'tf_char_2810': 1.0,\n",
       " '+1:is_client': 1,\n",
       " '+1:sent_len': 42.0,\n",
       " '+1:TOKEN': 4,\n",
       " '+1:POSTAG_CD': 1,\n",
       " '+1:POSTAG_COP': 1,\n",
       " '+1:POSTAG_DEF': 1,\n",
       " '+1:POSTAG_DTT': 1,\n",
       " '+1:POSTAG_IN': 1,\n",
       " '+1:POSTAG_NN': 2,\n",
       " '+1:POSTAG_RB': 1,\n",
       " '+1:POSTAG_TEMP': 1,\n",
       " '+1:POSTAG_VB': 1,\n",
       " '+1:f_gen_M': 5,\n",
       " '+1:f_num_P': 1,\n",
       " '+1:f_num_S': 4,\n",
       " '+1:f_per_1': 1,\n",
       " '+1:f_per_3': 1,\n",
       " '+1:f_tense_PAST': 1}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ae39e7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc sentences reshaped: from 11733 to 1956\n",
      "mean_values [0.76735651 0.76704826 0.79595948]\n",
      "doc sentences reshaped: from 11733 to 3911\n",
      "mean_values [0.75805815 0.75864557 0.78100446]\n",
      "doc sentences reshaped: from 11733 to 11733\n",
      "mean_values [0.76324848 0.76188583 0.78334989]\n"
     ]
    }
   ],
   "source": [
    "imp.reload(feature_utils)\n",
    "shapes = [(6,6),(6,3),(6,1)]\n",
    "score_db = pd.DataFrame()\n",
    "for shape in shapes:\n",
    "    seq_len = shape[0]\n",
    "    step = shape[1]\n",
    "    X_shaped,y_shaped,groups_shaped = feature_utils.reshape_doc_features_to_sequence(X,y,group,seq_len,step)\n",
    "    score = leaveOneOut_validate(X_shaped,y_shaped,groups_shaped,seq_len,step)\n",
    "    add_score_to_db(score_db,\"{}_{}\".format(seq_len,step),score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "259ae6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6_6</th>\n",
       "      <td>0.767357</td>\n",
       "      <td>0.767048</td>\n",
       "      <td>0.795959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_3</th>\n",
       "      <td>0.758058</td>\n",
       "      <td>0.758646</td>\n",
       "      <td>0.781004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_1</th>\n",
       "      <td>0.763248</td>\n",
       "      <td>0.761886</td>\n",
       "      <td>0.783350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1   recall  precision\n",
       "6_6 0.767357 0.767048   0.795959\n",
       "6_3 0.758058 0.758646   0.781004\n",
       "6_1 0.763248 0.761886   0.783350"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac3875e",
   "metadata": {},
   "source": [
    "### Run example on 35 files, with TfIDF chars, with only features that hold no zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c3acb3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc sentences reshaped: from 11733 to 1956\n",
      "34 {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35}\n",
      "[ 91  92  93  94  95  96  97  98  99 100] [0 1 2 3 4 5 6 7 8 9]\n",
      "1662 294\n",
      "f1 0.7697249894898157\n",
      "recall 0.771152754116979\n",
      "precition 0.7685780508296276\n"
     ]
    }
   ],
   "source": [
    "crf = CRF(\n",
    "    min_freq = 5,\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    )\n",
    "X_shaped,y_shaped,groups_shaped = feature_utils.reshape_doc_features_to_sequence(X,y,group,6,6)\n",
    "group_indices = set(group)\n",
    "print(len(group_indices),group_indices)\n",
    "num_test_doc = int(len(group_indices)*0.15)\n",
    "logo =  LeavePGroupsOut(n_groups=num_test_doc)\n",
    "split = logo.split(X_shaped, y_shaped, groups_shaped)\n",
    "itteration = 0\n",
    "for train_idx, test_idx in split:\n",
    "    if(itteration > 2):\n",
    "        break\n",
    "    itteration +=1\n",
    "print(train_idx[:10],test_idx[:10])\n",
    "print(len(train_idx),len(test_idx))\n",
    "X_train = itemgetter(*train_idx)(X_shaped)\n",
    "y_train  = itemgetter(*train_idx)(y_shaped)\n",
    "X_test = itemgetter(*test_idx)(X_shaped)\n",
    "y_test = itemgetter(*test_idx)(y_shaped)\n",
    "crf.fit(X_train, y_train)\n",
    "y_pred  =  crf.predict(X_test)\n",
    "labels = list(crf.classes_)\n",
    "f1 = metrics.flat_f1_score(y_test, y_pred,average='weighted', labels=labels)\n",
    "recall = metrics.flat_recall_score(y_test, y_pred,average='weighted', labels=labels)\n",
    "precision = metrics.flat_precision_score(y_test, y_pred,average='weighted', labels=labels)\n",
    "print (\"f1 {}\\nrecall {}\\nprecition {}\".format(f1,recall,precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb462f45",
   "metadata": {},
   "source": [
    "## Run on docs [1-19] ( except 13 and 9 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "76e12304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 doc 543 sentences packed\n",
      "11 doc 174 sentences packed\n",
      "2 doc 186 sentences packed\n",
      "17 doc 622 sentences packed\n",
      "5 doc 375 sentences packed\n",
      "10 doc 424 sentences packed\n",
      "3 doc 152 sentences packed\n",
      "16 doc 438 sentences packed\n",
      "8 doc 259 sentences packed\n",
      "18 doc 157 sentences packed\n",
      "6 doc 471 sentences packed\n",
      "15 doc 159 sentences packed\n",
      "12 doc 198 sentences packed\n",
      "7 doc 448 sentences packed\n",
      "19 doc 374 sentences packed\n",
      "14 doc 176 sentences packed\n",
      "1 doc 618 sentences packed\n",
      "5774 sentenced packed for 17 docs\n"
     ]
    }
   ],
   "source": [
    "imp.reload(feature_utils)\n",
    "X,y,group =  feature_utils.pack_all_doc_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "b12870ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 doc sentences reshaped: from 5774 to 963\n",
      "score [[0.74161977 0.74267101 0.7415918 ]\n",
      " [0.8380273  0.77419355 0.9601518 ]\n",
      " [0.83343915 0.78666667 0.91792075]\n",
      " [0.74374676 0.73809524 0.76641828]\n",
      " [0.74424675 0.74462366 0.74439964]\n",
      " [0.83397289 0.84177215 0.83160188]\n",
      " [0.68858844 0.69333333 0.69302711]\n",
      " [0.81784944 0.81395349 0.82345741]\n",
      " [0.82830498 0.83333333 0.8265873 ]\n",
      " [0.77465006 0.77011494 0.78001768]\n",
      " [0.7290769  0.74747475 0.71851852]\n",
      " [0.82272137 0.85555556 0.84836601]\n",
      " [0.88601975 0.81410256 0.97187438]\n",
      " [0.78109981 0.78310502 0.7815498 ]\n",
      " [0.43676666 0.41666667 0.73547893]\n",
      " [0.64686754 0.68589744 0.67503621]\n",
      " [0.90586739 0.89247312 0.93068141]]\n"
     ]
    }
   ],
   "source": [
    "imp.reload(feature_utils)\n",
    "\n",
    "seq_len = 6\n",
    "step = 6\n",
    "X_shaped,y_shaped,groups_shaped = feature_utils.reshape_doc_features_to_sequence(X,y,group,seq_len,step)\n",
    "score = leaveOneOut_validate(X_shaped,y_shaped,groups_shaped,seq_len,step)\n",
    "add_score_to_db(\"{}_{}\".format(seq_len,step),score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "15fac216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6_6</th>\n",
       "      <td>0.767816</td>\n",
       "      <td>0.760825</td>\n",
       "      <td>0.808628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_3</th>\n",
       "      <td>0.754951</td>\n",
       "      <td>0.747010</td>\n",
       "      <td>0.790973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_1</th>\n",
       "      <td>0.760612</td>\n",
       "      <td>0.755090</td>\n",
       "      <td>0.786439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1   recall  precision\n",
       "6_6 0.767816 0.760825   0.808628\n",
       "6_3 0.754951 0.747010   0.790973\n",
       "6_1 0.760612 0.755090   0.786439"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e5357",
   "metadata": {},
   "source": [
    "## Run on docs [1-19] with TfIdf per char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9427e282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 doc 543 sentences packed\n",
      "11 doc 174 sentences packed\n",
      "2 doc 186 sentences packed\n",
      "17 doc 622 sentences packed\n",
      "5 doc 375 sentences packed\n",
      "10 doc 424 sentences packed\n",
      "3 doc 152 sentences packed\n",
      "16 doc 438 sentences packed\n",
      "8 doc 259 sentences packed\n",
      "18 doc 157 sentences packed\n",
      "6 doc 471 sentences packed\n",
      "15 doc 159 sentences packed\n",
      "12 doc 198 sentences packed\n",
      "7 doc 448 sentences packed\n",
      "19 doc 374 sentences packed\n",
      "14 doc 176 sentences packed\n",
      "1 doc 618 sentences packed\n",
      "5774 sentenced packed for 17 docs\n"
     ]
    }
   ],
   "source": [
    "imp.reload(feature_utils)\n",
    "X,y,group =  feature_utils.pack_all_doc_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75593c14",
   "metadata": {},
   "source": [
    "### Cross - validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0ed7166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 doc sentences reshaped: from 5774 to 963\n",
      "mean_values [0.7742163  0.76612805 0.81464162]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6_6</th>\n",
       "      <td>0.774216</td>\n",
       "      <td>0.766128</td>\n",
       "      <td>0.814642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1   recall  precision\n",
       "6_6 0.774216 0.766128   0.814642"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_char_score_db = pd.DataFrame()\n",
    "seq_len = 6\n",
    "step = 6\n",
    "X_shaped,y_shaped,groups_shaped = feature_utils.reshape_doc_features_to_sequence(X,y,group,seq_len,step)\n",
    "score = leaveOneOut_validate(X_shaped,y_shaped,groups_shaped,seq_len,step)\n",
    "add_score_to_db(tf_char_score_db,\"{}_{}\".format(seq_len,step),score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2b98c",
   "metadata": {},
   "source": [
    "### Run example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8d7aa487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] [388 389 390 391 392 393 394 395 396 397]\n",
      "938 25\n",
      "f1 0.8896014709997201, recall 0.8733333333333333, precition 0.9130256410256411\n"
     ]
    }
   ],
   "source": [
    "crf = CRF(\n",
    "    min_freq = 5,\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    )\n",
    "logo = LeaveOneGroupOut()\n",
    "split = logo.split(X_shaped, y_shaped, groups_shaped)\n",
    "itteration = 0\n",
    "for train_idx, test_idx in split:\n",
    "    if(itteration > 1):\n",
    "        break\n",
    "    itteration +=1\n",
    "print(train_idx[:10],test_idx[:10])\n",
    "print(len(train_idx),len(test_idx))\n",
    "X_train = itemgetter(*train_idx)(X_shaped)\n",
    "y_train  = itemgetter(*train_idx)(y_shaped)\n",
    "X_test = itemgetter(*test_idx)(X_shaped)\n",
    "y_test = itemgetter(*test_idx)(y_shaped)\n",
    "crf.fit(X_train, y_train)\n",
    "y_pred  =  crf.predict(X_test)\n",
    "labels = list(crf.classes_)\n",
    "f1 = metrics.flat_f1_score(y_test, y_pred,average='weighted', labels=labels)\n",
    "recall = metrics.flat_recall_score(y_test, y_pred,average='weighted', labels=labels)\n",
    "precision = metrics.flat_precision_score(y_test, y_pred,average='weighted', labels=labels)\n",
    "print (\"f1 {}\\nrecall {}\\nprecition {}\".format(f1,recall,precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b9cd606",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'ConfusionMatrixDisplay' has no attribute 'from_predictions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-0b8dc5fbb903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_utils\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeature_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prediction_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/sim_reg6/users/zsofya/classroom/MSc/personal_study/thesis/src/feature_utils.py\u001b[0m in \u001b[0;36mget_prediction_report\u001b[0;34m(y_test, y_pred)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_prediction_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m     \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray_r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'ConfusionMatrixDisplay' has no attribute 'from_predictions'"
     ]
    }
   ],
   "source": [
    "imp.reload(feature_utils)\n",
    "feature_utils.get_prediction_report(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fbfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
