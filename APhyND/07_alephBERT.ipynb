{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os, glob\n",
    "import imp\n",
    "import seaborn as sns\n",
    "sys.path.append('./src/')\n",
    "import doc_utils\n",
    "import defines\n",
    "import my_bert\n",
    "import feature_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give a try to AlephBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alephbert_tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\n",
    "alephbert = BertModel.from_pretrained('onlplab/alephbert-base', return_dict=False)\n",
    "\n",
    "# # if not finetuning - disable dropout\n",
    "# alephbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_sent_dbs():\n",
    "    sent_db = pd.concat([pd.read_csv(i,usecols=['text','is_nar','nar_idx','sent_len','is_client']) for i in glob.glob(os.path.join(os.path.join(os.getcwd(),defines.PATH_TO_DFS,\"*_sent_db.csv\")))])\n",
    "    sent_db.reset_index(inplace=True,drop=True)\n",
    "    return sent_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=10\n",
    "# sent_db = pd.read_csv(os.path.join(os.getcwd(),defines.PATH_TO_DFS,\"{:02d}_sent_db.csv\".format(i)),usecols=['text','is_nar'])\n",
    "sent_db = concat_sent_dbs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.72247065 1.62374372]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsofya/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0. 1.], y=30342   0.000000\n",
      "30855   1.000000\n",
      "9518    0.000000\n",
      "6348    0.000000\n",
      "17553   0.000000\n",
      "          ...   \n",
      "28421   0.000000\n",
      "13379   0.000000\n",
      "32525   1.000000\n",
      "18011   1.000000\n",
      "16747   0.000000\n",
      "Name: is_nar, Length: 23265, dtype: float64 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "imp.reload(my_bert)\n",
    "# train_text, train_labels, val_text, test_text, val_labels, test_labels = my_bert.split_train_val_test(sent_db)\n",
    "# tokens_train,tokens_val,tokens_test = my_bert.get_train_val_test_tokens(alephbert_tokenizer,train_text,val_text,test_text)\n",
    "# tensor_map = my_bert.covert_token2tensor(tokens_train,train_labels,tokens_val,val_labels,tokens_test,test_labels)\n",
    "# train_dataloader,val_dataloader = my_bert.get_data_loader(tensor_map)\n",
    "# wrapped_model = my_bert.wrap_pretained_model(alephbert)\n",
    "# optimizer = my_bert.get_optimizer(wrapped_model)\n",
    "# cross_entropy = my_bert.get_cross_entropy(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "Saving best model alephbert\n",
      "\n",
      "Training Loss: 0.633\n",
      "Validation Loss: 0.624\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "Saving best model alephbert\n",
      "\n",
      "Training Loss: 0.621\n",
      "Validation Loss: 0.617\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "Saving best model alephbert\n",
      "\n",
      "Training Loss: 0.616\n",
      "Validation Loss: 0.615\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "Saving best model alephbert\n",
      "\n",
      "Training Loss: 0.614\n",
      "Validation Loss: 0.612\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "Saving best model alephbert\n",
      "\n",
      "Training Loss: 0.610\n",
      "Validation Loss: 0.612\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "Saving best model alephbert\n",
      "\n",
      "Training Loss: 0.608\n",
      "Validation Loss: 0.611\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "Saving best model alephbert\n",
      "\n",
      "Training Loss: 0.607\n",
      "Validation Loss: 0.610\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.606\n",
      "Validation Loss: 0.611\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "Saving best model alephbert\n",
      "\n",
      "Training Loss: 0.604\n",
      "Validation Loss: 0.608\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.604\n",
      "Validation Loss: 0.608\n"
     ]
    }
   ],
   "source": [
    "imp.reload(my_bert)\n",
    "model_name = \"alephbert\"\n",
    "my_bert.train_validate(model_name,wrapped_model,optimizer,train_dataloader,val_dataloader,cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Saved Model & get prediction for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.74      0.77      3451\n",
      "           1       0.51      0.61      0.56      1535\n",
      "\n",
      "    accuracy                           0.70      4986\n",
      "   macro avg       0.66      0.67      0.66      4986\n",
      "weighted avg       0.72      0.70      0.71      4986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imp.reload(my_bert)\n",
    "imp.reload(feature_utils)\n",
    "saved_model = my_bert.load_saved_bert_model(wrapped_model,model_name)\n",
    "preds = my_bert.get_prediction(saved_model,tensor_map['test'])\n",
    "feature_utils.get_prediction_report(tensor_map['test']['y'],preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "he_tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n",
    "he_model = AutoModel.from_pretrained(\"avichr/heBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsofya/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.72247065 1.62374372]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsofya/jupyter_git/jup-nb-generic/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0. 1.], y=30342   0.000000\n",
      "30855   1.000000\n",
      "9518    0.000000\n",
      "6348    0.000000\n",
      "17553   0.000000\n",
      "          ...   \n",
      "28421   0.000000\n",
      "13379   0.000000\n",
      "32525   1.000000\n",
      "18011   1.000000\n",
      "16747   0.000000\n",
      "Name: is_nar, Length: 23265, dtype: float64 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "tokens_train,tokens_val,tokens_test = my_bert.get_train_val_test_tokens(he_tokenizer,train_text,val_text,test_text)\n",
    "tensor_map = my_bert.covert_token2tensor(tokens_train,train_labels,tokens_val,val_labels,tokens_test,test_labels)\n",
    "train_dataloader,val_dataloader = my_bert.get_data_loader(tensor_map)\n",
    "wrapped_model = my_bert.wrap_pretained_model(he_model)\n",
    "cross_entropy = my_bert.get_cross_entropy(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "Saving best model heBERT\n",
      "\n",
      "Training Loss: 0.691\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.691\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.691\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.691\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.691\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.691\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.692\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.691\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.691\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch    50  of    728.\n",
      "  Batch   100  of    728.\n",
      "  Batch   150  of    728.\n",
      "  Batch   200  of    728.\n",
      "  Batch   250  of    728.\n",
      "  Batch   300  of    728.\n",
      "  Batch   350  of    728.\n",
      "  Batch   400  of    728.\n",
      "  Batch   450  of    728.\n",
      "  Batch   500  of    728.\n",
      "  Batch   550  of    728.\n",
      "  Batch   600  of    728.\n",
      "  Batch   650  of    728.\n",
      "  Batch   700  of    728.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.691\n",
      "Validation Loss: 0.691\n"
     ]
    }
   ],
   "source": [
    "imp.reload(my_bert)\n",
    "model_name = \"heBERT\"\n",
    "my_bert.train_validate(model_name,wrapped_model,optimizer,train_dataloader,val_dataloader,cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71      3451\n",
      "           1       0.36      0.36      0.36      1535\n",
      "\n",
      "    accuracy                           0.60      4986\n",
      "   macro avg       0.53      0.53      0.53      4986\n",
      "weighted avg       0.60      0.60      0.60      4986\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEGCAYAAAAT05LOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAegElEQVR4nO3de7hWc/7/8eervUOSCZG0Y2fE/DY/QkwyM5mJBoP4mp+JLtQwNMw0JMb4mjE/flyMQ+OYX9SFmSY0JfpOqHEofteEzjqYr3RQ6aAkJMzm/fvjXru5q324723f7b3v9Xpc17rc9+deh8+qq5fP+nzWWh9FBGZmadOisStgZtYYHH5mlkoOPzNLJYefmaWSw8/MUqm0sSuQTZKHnpuZQw45pLGrYHlYvXo1H374ob7OPvL8d/p8RJz8dY5XKE0q/Kz5efjhhxu7CpaHiy++uEH2I+WWnxHRrkEOWAAOPzPLWx7hV+Ca1J/Dz8zylmv4NWUOPzPLm8PPzFJHEi1aNP8bRRx+ZpY3t/zMLJUcfmaWSg4/M0sdSQ4/M0snh5+ZpZJHe80sldzyM7PUcZ+fmaWWw8/MUsnhZ2ap5AEPM0sd9/mZWWo5/MwslRx+ZpZKxRB+zb/X0sx2uKp+v7qWOvbRSdJLkhZImi/pV0n57ZLekjRX0lOS2ibl5ZI2S5qdLA9m7etoSW9KWiTpHuWQzg4/M8tL1ctMc1nqUAlcFREVQHfgckkVwGTgsIg4HPhv4DdZ27wTEV2TZWBW+TDgZ0CXZKlzxjiHn5nlrSFafhGxKiJmJp8/BhYCHSNiUkRUJqtNA8rqqEsHYPeImBaZGZMeA86s6xwcfmaWtzzCr52k6VnLJTXsrxw4Enhtm59+Cjyb9b2zpFmSpkj6blLWEViRtc6KpKxWHvAws7zlMeCxLiK61bGv3YCxwBUR8VFW+X+SuTQelRStAvaPiPWSjgbGSzo078onHH5mlpeGvMlZUksywTcqIsZllfcHTgN6JZeyRMTnwOfJ5xmS3gEOBlay9aVxWVJWK1/2mlneGmi0V8AIYGFE3JVVfjJwDXBGRHyaVb63pJLk84FkBjYWR8Qq4CNJ3ZN9XgA8Xdc5uOVnZnlroGd7jwfOB96UNDspuw64B9gZmJwE6LRkZPd7wI2S/gV8BQyMiA+S7S4DHgFakekjzO4nrJbDz8zy1hCXvRHxKlDdjibWsP5YMpfI1f02HTgsn+M7/MwsL36xgZmllsPPzFLJ4WdmqeSXmZpZ6rjPz8xSy+FnZqnk8DOzVHL4mVkqOfzMLHWqXmba3Dn8zCxvbvmZWSo5/MwslRx+ZpY6vsnZzFLL4WdmqeTRXjNLJbf8zCx13OdnZqlVDOHX/C/czWyHa6DZ2zpJeknSAknzJf0qKd9T0mRJbyf/3SMpl6R7JC2SNFfSUVn7ujBZ/21JF+ZyDg4/M8tbixYtclrqUAlcFREVQHfgckkVwLXACxHRBXgh+Q5wCpnpKrsAlwDDIBOWwA3At4FjgRuqArPWc8j3pA3Kysp48cUXmT9/PvPmzWPQoEFb/T548GAigr322guAnj178uGHHzJr1ixmzZrFb3/72y3rXnHFFcybN48333yTv/zlL+y888479FzSasyYMVxwwQWcf/75PPnkkwAsWrSIgQMHcuGFF/LrX/+aTZs2AbBgwQIGDBjAgAED6N+/P1OnTm3Mqje6XFt9dbX8ImJVRMxMPn8MLAQ6An2AR5PVHgXOTD73AR6LjGlAW0kdgB8CkyPig4jYAEwGTq7rPAra55dMPnw3UAI8HBG3FvJ4O0plZSVXXXUVs2bNYrfddmPGjBlMnjyZhQsXUlZWRu/evVm2bNlW27zyyiucfvrpW5Xtt99+DBo0iIqKCj777DOeeOIJ+vbty6OPPooVzuLFi5kwYQLDhw+ntLSUIUOG0KNHD2677TYuu+wyjjzySP72t78xevRoLr74Yg488EAeeughSktLWbduHQMGDKBHjx6Ulqa3yzyPPr92kqZnfR8eEcOr2V85cCTwGtA+mYgcYDXQPvncEVietdmKpKym8loVrOWXzKx+P5mmagVwbtKkbfZWr17NrFmzAPjkk09YuHAhHTtm/qyHDh3KNddcQ0TktK/S0lJatWpFSUkJu+66K++9917B6m0Zy5Yto6Kigl122YXS0lK6du3KlClTWL58OV27dgWgW7duvPzyywBb1gP44osviqKz/+vKo+W3LiK6ZS3VBd9uZObjvSIiPsr+LTL/kHL7x5SnQl72HgssiojFEfEF8DiZZmtROeCAAzjyyCN57bXXOOOMM1i5ciVz587dbr3jjjuO2bNnM3HiRCoqMv8PeO+997jjjjt49913WbVqFRs3bmTy5Mk7+hRSp3PnzsyZM4eNGzfy2WefMW3aNNauXUvnzp155ZVXAHjppZdYu3btlm3mz5/P+eefT//+/RkyZEiqW33QMAMeyX5akgm+URExLilek1zOkvy36i9iJdApa/OypKym8loVMvxyaopKukTS9G2axs1C69atGTt2LFdccQWVlZVcd911/O53v9tuvZkzZ3LAAQfQtWtX7r33XsaPHw9A27Zt6dOnD507d2a//fajdevW9OvXbwefRfqUl5fTr18/Bg8ezJAhQzjooIMoKSnh2muvZfz48Vx00UVs3ryZli1bbtnm0EMP5U9/+hPDhw/nz3/+M59//nkjnkHja6DRXgEjgIURcVfWT88AVSO2FwJPZ5VfkIz6dgc2JpfHzwO9Je2RDHT0Tspq1egDHhExvKpJ3Nh1yUdpaSljx45l1KhRPPXUU3zzm9/c0qJYsmQJZWVlzJw5k/bt2/Pxxx9v6Tx/9tlnadmyJXvttRcnnngiS5YsYd26dVRWVjJu3Dh69OjRyGeWDqeddhojRozgvvvuo02bNnTq1IkDDjiAu+66ixEjRtCrV68tXRnZysvLadWqFUuWLGmEWjcNVS8zbYDR3uOB84EfSJqdLKcCtwInSXobODH5DjARWAwsAh4CLgOIiA+Am4A3kuXGpKxWhWy716sp2lyMGDGChQsXMnToUADmzZtH+/btt/y+ZMkSunXrxvr162nfvj1r1qwB4JhjjqFFixasX7+ed999l+7du9OqVSs2b95Mr169mD692TWAm6UNGzawxx57sGbNGqZOncqDDz64peyrr77iscceo0+fTC/Ne++9xz777ENpaSmrV69m2bJl7Lvvvo18Bo2rIfo9I+JVoKYd9apm/QAur2FfI4GR+Ry/kOH3BtBFUmcyodcXOK+Ax9thjj/+eC644ALmzp27ZeDjuuuu49lnn612/R//+Mf8/Oc/p7Kyks2bN9O3b18AXn/9df76178yc+ZMKisrmTVrFsOHb9cfbAVw/fXXs3HjRkpLS7nyyitp06YNY8aMYdy4TLdTz549OfXUUwGYO3cuo0aNorS0FEkMHjyYtm3bNmLtG18xDPoo11HJeu0804T9I5lbXUZGxM11rF+4ylhBVA0QWPNw8cUX89Zbb32t5Np9992je/fuOa07efLkGU21S6ugQ1YRMZHMdbqZFQm/2MDMUsvhZ2ap5JeZmlkqueVnZqnjPj8zSy2Hn5mlksPPzFLJAx5mljru8zOz1HL4mVkqOfzMLJUcfmaWOu7zM7PU8mivmaWSW35mlkoOPzNLHff5mVlqFUP4Nf9eSzPb4Rpw3t6RktZKmpdV9kTWbG5LJc1Oysslbc767cGsbY6W9KakRZLuUQ4Hd8vPzPLWgKO9jwD3AY9VFUTET6o+S7oT2Ji1/jsR0bWa/QwDfga8RmbqjJOB6mcUS7jlZ2Z5ybXVl0vLLyKmAtXOsZu03s4BRtdRnw7A7hExLZne8jHgzLqO7fAzs7zlEX7tJE3PWi7J4zDfBdZExNtZZZ0lzZI0RdJ3k7KOwIqsdVYkZbXyZa+Z5S2PAY91X2PqynPZutW3Ctg/ItZLOhoYL+nQeu7b4Wdm+Sv0aK+kUuA/gKOryiLic+Dz5PMMSe8ABwMrgbKszcuSslr5stfM8iKJFi1a5LR8DScCb0XElstZSXtLKkk+Hwh0ARZHxCrgI0ndk37CC4Cn6zqAw8/M8taAt7qMBv4BHCJphaSLkp/6sv1Ax/eAucmtL38FBkZE1WDJZcDDwCLgHeoY6QVf9ppZPTTUZW9EnFtDef9qysYCY2tYfzpwWD7HrjH8JN0LRE2/R8SgfA5kZsWjGJ7wqK3lN32H1cLMmpWiDr+IeDT7u6RdI+LTwlfJzJqyYnmxQZ0DHpKOk7QAeCv5foSkBwpeMzNrsnbAaG/B5VK7PwI/BNYDRMQcMqMuZpZSDTXa25hyGu2NiOXbnMiXhamOmTUHTT3YcpFL+C2X1AMISS2BXwELC1stM2uqmkOrLhe5XPYOBC4n86Dwe0DX5LuZpVQqLnsjYh3QbwfUxcyaiaY+mJGLXEZ7D5Q0QdL7yRtXn06eqzOzlCqGll8u8f0X4EmgA7AfMIY6Xi5oZsWrIV9m2phyCb9dI+JPEVGZLH8Gdil0xcys6SqG8Kvt2d49k4/PSroWeJzMs74/IfOOfDNLqaYebLmobcBjBpmwqzrLS7N+C+A3haqUmTVtRR1+EdF5R1bEzJqHqpeZNnc5PeEh6TCggqy+voh4rOYtzKyYFXXLr4qkG4ATyITfROAU4FWy5tk0s3QphvDLpe36Y6AXsDoiBgBHAN8oaK3MrEkr6tHeLJsj4itJlZJ2B9YCnQpcLzNrwpp6sOUil5bfdEltgYfIjADPJDPhiJmlUEPe5CxpZPLk2Lysst9LWilpdrKcmvXbbyQtkvRPST/MKj85KVuU3JpXp1ye7b0s+figpOeA3SNibi47N7Pi1ICjvY8A97H9GMLQiLgju0BSBZlZ3Q4l87TZ3yUdnPx8P3ASsAJ4Q9IzEbGgtgPXdpPzUbX9FhEza9uxmRWvhrrsjYipkspzXL0P8HgyefkSSYuAY5PfFkXE4qRujyfr1i/8gDtrqzPwgxwrnLNDDjmEhx9+uKF3awX0ne98p7GrYHnYbbfdGmQ/eYRfO0nZk6ENj4jhOWz3C0kXkJlI7aqI2EDmtXrTstZZkZQBLN+m/Nt1HaC2m5y/n0MFzSxl8hzJXRcR3fI8xDDgJjKNrJvINMR+muc+6uRJy80sb4Uc7Y2INVnHeQj4r+TrSra+06QsKaOW8ho1/2dUzGyHK+TsbZI6ZH09C6gaCX4G6CtpZ0mdgS7A68AbQBdJnSXtRGZQ5Jm6juOWn5nlraFafpJGk3mCrJ2kFcANwAmSupK57F1K8lKViJgv6UkyAxmVwOUR8WWyn18AzwMlwMiImF/XsXN5vE1kXmN/YETcKGl/YN+IeD3P8zSzItCQT29ExLnVFI+oZf2bgZurKZ9Inq/ay6Vd+gBwHFBVyY/J3FNjZimVlsfbvh0RR0maBRARG5LrajNLqaYebLnIJfz+JamEzPU3kvYGviporcysSUtL+N0DPAXsI+lmMm95ub6gtTKzJis1LzONiFGSZpB5rZWAMyNiYcFrZmZNVipafsno7qfAhOyyiHi3kBUzs6YrFeEH/I1/T2S0C9AZ+CeZNyuYWQqlIvwi4n9mf0/e9nJZDaubWQqkIvy2FREzJdX5xgQzK07N4R6+XOTS5zc462sL4CjgvYLVyMyavFSM9gJtsj5XkukDHFuY6phZc1D0Lb/k5uY2ETFkB9XHzJqBog4/SaURUSnp+B1ZITNr2tLQ5/c6mf692ZKeAcYAm6p+jIhxBa6bmTVRxR5+VXYB1pOZs6Pqfr8AHH5mKVXsAx77JCO98/h36FWJgtbKzJq0Ym/5lQC7sXXoVXH4maVUGvr8VkXEjTusJmbWbBR7+DX/szOzgij28Ou1w2phZs1KMYRfjUM2EfHBjqyImTUPVS8zbYipKyWNlLRW0rysstslvSVprqSnJLVNysslbZY0O1kezNrmaElvSlok6R7lkM7Nf7zazHa4BpzA6BHg5G3KJgOHRcThwH8Dv8n67Z2I6JosA7PKhwE/IzOXb5dq9rkdh5+Z5a2hwi8ipgIfbFM2KSIqk6/TgLI66tIB2D0ipkVEAI8BZ9Z1bIefmeUtj/BrJ2l61nJJnof6KfBs1vfOkmZJmiLpu0lZR2BF1jorkrJa5f0+PzNLtzzv81sXEd3qeZz/JPMmqVFJ0Spg/4hYL+loYLyker9R3uFnZnkr9GivpP7AaUCv5FKWiPgc+Dz5PEPSO8DBwEq2vjQuS8pq5cteM8tbQ432VkfSycA1wBkR8WlW+d7Ja/aQdCCZgY3FEbEK+EhS92SU9wLg6bqO45afmeWtoVp+kkYDJ5DpG1wB3EBmdHdnYHJynGnJyO73gBsl/Qv4ChiYdUveZWRGjluR6SPM7ieslsPPzPLSkM/2RsS51RSPqGHdsdTwFvmImA4cls+xHX5mlrdieMLD4WdmeXP4mVkqFfvLTM3MtpOG9/mZmVXL4WdmqeTwM7NUcviZWSo5/MwsdapeZtrcOfzMLG9u+ZlZKjn8zCyVHH5mljq+ydnMUssDHmaWSm75GQBjxoxhwoQJRASnn34655xzDm+//TZ33HEHX3zxBSUlJQwePJiKigo++eQTbrrpJtasWcOXX35J3759+dGPftTYp5A65eXltGnThpKSEkpLS5k+ffqW3+68806GDBnC+++/T7t27bj99tsZNSozjURlZSULFy7k/fffZ88992ys6jc6h18tJI0k8w7+tRGR10sGm5PFixczYcIEhg8fTmlpKUOGDKFHjx4MGzaMAQMG0L17d/7xj38wbNgw7r33XsaNG0d5eTm33XYbGzZsoF+/fvTu3ZuWLVs29qmkzksvvUS7du22Klu+fDmTJk1i//3331J29dVXc/XVVwMwYcIEhg4dmvrgK4bwK+SF+yPkMHFwc7ds2TIqKirYZZddKC0tpWvXrkyZMgWATZs2bflv1T8ySXz66adEBJs3b2b33XenpKSk0epvW7vyyiv5wx/+UOM/7tGjR3PuudW9fDhdGnDS8kZTsPCrbjLiYtS5c2fmzJnDxo0b+eyzz5g2bRpr165l0KBBPPDAA5x99tncf//9XHrppQCcffbZLFu2jDPPPJP+/fszaNCgoug8bm4k0bt3b44++miGDx8OwNNPP03Hjh054ogjqt3m008/5bnnnuPss8/ekVVtkooh/Bq9zy+ZxPgSgPbt2zdybfJXXl5Ov379GDx4MK1ateKggw6ipKSE8ePH88tf/pITTjiBF198kVtvvZU//vGPvPbaaxx00EHcfffdrFy5ksGDB3PEEUfQunXrxj6VVHn11Vfp2LEja9eu5aSTTuJb3/oWt9xyC5MmTapxmwkTJnD88cen+pK3SkP9D7u67jFJewJPAOXAUuCciNiQzMx2N3Aq8CnQPyJmJttcCFyf7Pb/RMSjdZ5Dg5zB1xARwyOiW0R0a9u2bWNXp15OO+00RowYwX333UebNm3o1KkTzz33HD179gTg+9//PgsXLgRg4sSJ9OzZE0mUlZXRoUMHli1b1pjVT6WOHTsCsM8++3DWWWcxZcoUlixZwhFHHEF5eTkrVqzgqKOOYvXq1Vu2efzxx33JS+6tvhxbfo+wfffYtcALEdEFeCH5DnAKmekqu5BpMA1L6rMnmVnfvg0cC9wgaY+6Dtzo4VcMNmzYAMCaNWuYOnUqJ554Iu3atWP27NkAzJgxg7KyzJzK7du3Z8aMGQB88MEHvPvuu+y3336NUu+02rRpEx9//PGWz5MmTeKYY45h7dq1LF26lKVLl1JWVsbMmTPZd999Adi4cSNTpkyhT58+jVn1JqOhwq+G7rE+QFXL7VHgzKzyxyJjGtBWUgfgh8DkiPggIjYAk8lhvKHRL3uLwfXXX8/GjRspLS3lyiuvpE2bNlxzzTXcfffdfPnll+y0005cc801APTv359bbrmFCy+8kIhg4MCBNNcWb3O1Zs0azjrrLCBz68p5553HySfX/m/lqaeeonfv3u6eSOTRn9dO0vSs78MjYngd27RPJiIHWA1U9Yd1BJZnrbciKaupvFaFvNVlu8mII6La+Tibu/vvv3+7ssMPP5wRI7Y/3Xbt2nHXXXftiGpZDQ488EDmzJlT6zpLly7d6nv//v3p379/4SrVzOQRfusiolt9jxMRISnqu31tChZ+NUxGbGZFoMAjuWskdYiIVcll7dqkfCXQKWu9sqRsJZmGVnb5y3UdxH1+ZpaXqpeZ5rLU0zPAhcnnC4Gns8ovUEZ3YGNyefw80FvSHslAR++krFbu8zOzvDVUy6+67jHgVuBJSRcBy4BzktUnkrnNZRGZW10GAETEB5JuAt5I1rsxIuq8x9jhZ2Z5a6jwq6V7rFc16wZweQ37GQmMzOfYDj8zy1tTf3ojFw4/M8tLc3h0LRcOPzPLWzE8j+7wM7O8ueVnZqnk8DOz1HGfn5mllsPPzFLJ4WdmqeTRXjNLHff5mVlqOfzMLJUcfmaWSg4/M0slh5+ZpU7Vy0ybO4efmeXNLT8zSyWHn5mlksPPzFLHNzmbWWoVw4BH8z8DM9vhqlp/dS117OMQSbOzlo8kXSHp95JWZpWfmrXNbyQtkvRPST/8Oufglp+Z5a0hLnsj4p9A12R/JWQmH3+KzJSUQyPijm2OWQH0BQ4F9gP+LungiPiyPsd3y8/M8pJrqy/PgOwFvBMRy2pZpw/weER8HhFLyMzfe2x9z8PhZ2Z5yyP82kmanrVcUsMu+wKjs77/QtJcSSMl7ZGUdQSWZ62zIimrF4efmeUtj/BbFxHdspbh1exrJ+AMYExSNAz4JplL4lXAnYU4B/f5mVneGni09xRgZkSsAaj6L4Ckh4D/Sr6uBDplbVeWlNWLW35mlpcC9PmdS9Ylr6QOWb+dBcxLPj8D9JW0s6TOQBfg9fqeh1t+Zpa3hrrJWVJr4CTg0qziP0jqCgSwtOq3iJgv6UlgAVAJXF7fkV5w+JlZPTRU+EXEJmCvbcrOr2X9m4GbG+LYDj8zy5sfbzOzVHL4mVnq+GWmZpZabvmZWSo5/Mwsdfw+PzNLLYefmaWSBzzMLJXc8jOz1HGfn5mllsPPzFLJ4WdmqeTwM7PU8eNtZpZabvmZWSo5/MwslYoh/BQRjV2HLSS9D9Q2b2dz1Q5Y19iVsLwU69/ZARGx99fZgaTnyPz55GJdRJz8dY5XKE0q/IqVpOkR0a2x62G5899Z8Wv+QzZmZvXg8DOzVHL47RjbzVJvTZ7/zoqc+/zMLJXc8jOzVHL4mVkqOfwKSNLJkv4paZGkaxu7PlY3SSMlrZU0r7HrYoXl8CsQSSXA/cApQAVwrqSKxq2V5eARoEnelGsNy+FXOMcCiyJicUR8ATwO9GnkOlkdImIq8EFj18MKz+FXOB2B5VnfVyRlZtYEOPzMLJUcfoWzEuiU9b0sKTOzJsDhVzhvAF0kdZa0E9AXeKaR62RmCYdfgUREJfAL4HlgIfBkRMxv3FpZXSSNBv4BHCJphaSLGrtOVhh+vM3MUsktPzNLJYefmaWSw8/MUsnhZ2ap5PAzs1Ry+DUjkr6UNFvSPEljJO36Nfb1iKQfJ58fru2lC5JOkNSjHsdYKmm7Wb5qKt9mnU/yPNbvJQ3Jt46WXg6/5mVzRHSNiMOAL4CB2T9Kqtc8zBFxcUQsqGWVE4C8w8+sKXP4NV+vAAclrbJXJD0DLJBUIul2SW9ImivpUgBl3Je8X/DvwD5VO5L0sqRuyeeTJc2UNEfSC5LKyYTslUmr87uS9pY0NjnGG5KOT7bdS9IkSfMlPQzUObO1pPGSZiTbXLLNb0OT8hck7Z2UfVPSc8k2r0j6VoP8aVrq1KulYI0raeGdAjyXFB0FHBYRS5IA2RgRx0jaGfh/kiYBRwKHkHm3YHtgATBym/3uDTwEfC/Z154R8YGkB4FPIuKOZL2/AEMj4lVJ+5N5iuV/ADcAr0bEjZJ+BOTydMRPk2O0At6QNDYi1gOtgekRcaWk3yX7/gWZiYUGRsTbkr4NPAD8oB5/jJZyDr/mpZWk2cnnV4ARZC5HX4+IJUl5b+Dwqv484BtAF+B7wOiI+BJ4T9KL1ey/OzC1al8RUdN77U4EKqQtDbvdJe2WHOM/km3/JmlDDuc0SNJZyedOSV3XA18BTyTlfwbGJcfoAYzJOvbOORzDbDsOv+Zlc0R0zS5IQmBTdhHwy4h4fpv1Tm3AerQAukfEZ9XUJWeSTiATpMdFxKeSXgZ2qWH1SI774bZ/Bmb14T6/4vM88HNJLQEkHSypNTAV+EnSJ9gB+H41204Dviepc7Ltnkn5x0CbrPUmAb+s+iKpa/JxKnBeUnYKsEcddf0GsCEJvm+RaXlWaQFUtV7PI3M5/RGwRNL/So4hSUfUcQyzajn8is/DZPrzZiaT8PxfMi38p4C3k98eI/Pmkq1ExPvAJWQuMefw78vOCcBZVQMewCCgWzKgsoB/jzr/bzLhOZ/M5e+7ddT1OaBU0kLgVjLhW2UTcGxyDj8AbkzK+wEXJfWbj6cGsHryW13MLJXc8jOzVHL4mVkqOfzMLJUcfmaWSg4/M0slh5+ZpZLDz8xS6f8DJ5ng4i8XEi4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imp.reload(my_bert)\n",
    "imp.reload(feature_utils)\n",
    "saved_model = my_bert.load_saved_bert_model(wrapped_model,model_name)\n",
    "preds = my_bert.get_prediction(saved_model,tensor_map['test'])\n",
    "feature_utils.get_prediction_report(tensor_map['test']['y'],preds,labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
